<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
		integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css">


	<title>AWS Certified Solutions Architect - Associate (SAA-C02)</title>
</head>

<body>
	<nav class="navbar navbar-expand-lg navbar-light bg-light">
		<a class="navbar-brand" href="#">AWS Exam Questions</a>
		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
			aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
			<span class="navbar-toggler-icon"></span>
		</button>
		<div class="collapse navbar-collapse" id="navbarNav">
			<ul class="navbar-nav">
				<li class="nav-item"><a class="nav-link" href="#All">All(200)</a></li>
				<li class="nav-item"><a class="nav-link" href="#S3">S3(44)</a></li>
				<li class="nav-item"><a class="nav-link" href="#IAM">IAM(5)</a></li>
				<li class="nav-item"><a class="nav-link" href="#EC2">EC2(29)</a></li>
				<li class="nav-item"><a class="nav-link" href="#VPC">VPC(8)</a></li>
				<li class="nav-item"><a class="nav-link" href="#CloudFront">CloudFront(15)</a></li>
				<li class="nav-item"><a class="nav-link" href="#ECS">ECS(1)</a></li>
			</ul>
		</div>
	</nav>



	<hr><a id=All>
		<h2>All</h2>
	</a> - 200 Questions <br><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 1<br />A company is migrating from an
		on&#8211;premises infrastructure to the AWS Cloud. One of the company&aposs applications stores files on a
		Windows file server farm that uses Distributed File System Replication (DFSR) to keep data in sync. A solutions
		architect needs to replace the file server farm.<br /><br />Which service should the solutions architect
		use?<br /><br />A. Amazon EFS<br />B. Amazon FSx<br />C. Amazon <a href="#S3">S3</a><br />D. AWS Storage
		Gateway<br /><br /><b>Correct Answer:</b><br />B. Amazon FSx<br /><br />Answer Description:<br />Migrating
		Existing Files to Amazon FSx for Windows File Server Using AWS DataSync<br /><br />We recommend using AWS
		DataSync to transfer data between Amazon FSx for Windows File Server file systems. DataSync is a data transfer
		service that simplifies, automates, and accelerates moving and replicating data between on&#8211;premises
		storage systems and other AWS storage services over the internet or AWS Direct Connect. DataSync can transfer
		your file system data and metadata, such as ownership, time stamps, and access permissions.<br /><br />Amazon
		FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the
		industry&#8211;standard Server Message Block (SMB) protocol.<br /><br />Amazon FSx is built on Windows Server
		and provides a rich set of administrative features that include end&#8211;user file restore, user quotas, and
		Access Control Lists (ACLs).<br /><br />Additionally, Amazon FSX for Windows File Server supports Distributed
		File System Replication (DFSR) in both Single&#8211;AZ and Multi&#8211;AZ deployments as can be seen in the
		feature comparison table below.<br /><br />CORRECT: &quot;Amazon FSx&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Amazon EFS&quot; is incorrect as EFS only supports Linux systems. INCORRECT:
		&quot;Amazon <a href="#S3">S3</a>&quot; is incorrect as this is not a suitable replacement for a Microsoft
		filesystem.<br /><br />INCORRECT: &quot;AWS Storage Gateway&quot; is incorrect as this service is primarily used
		for connecting on&#8211;premises storage to cloud storage. It consists of a software device installed
		on&#8211;premises and can be used with SMB shares but it actually stores the data on <a href="#S3">S3</a>. It is
		also used for migration. However, in this case the company need to replace the file server farm and Amazon FSx
		is the best choice for this job.<br /><br />References:<br /><br />Amazon FSx for Windows File Server > Windows
		User Guide > Availability and durability: Single&#8211;AZ and Multi&#8211;AZ file systems</div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 2<br />A company has a legacy application that
		processes data in two parts. The second part of the process takes longer than the first, so the company has
		decided to rewrite the application as two microservices running on Amazon <a href="#ECS">ECS</a> that can scale
		independently.<br /><br />How should a solutions architect integrate the microservices?<br /><br />A. Implement
		code in microservice 1 to send data to an Amazon <a href="#S3">S3</a> bucket. Use <a href="#S3">S3</a> event
		notifications to invoke microservice 2.<br />B. Implement code in microservice 1 to publish data to an Amazon
		SNS topic. Implement code in microservice 2 to subscribe to this topic.<br />C. Implement code in microservice 1
		to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data
		Firehose.<br />D. Implement code in microservice 1 to send data to an Amazon SQS queue. Implement code in
		microservice 2 to process messages from the queue.<br /><br /><b>Correct Answer:</b><br />C. Implement code in
		microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from
		Kinesis Data Firehose.<br /><br />Answer Description:<br />This is a good use case for Amazon SQS. The
		microservices must be decoupled so they can scale independently. An Amazon SQS queue will enable microservice 1
		to add messages to the queue. Microservice 2 can then pick up the messages and process them. This ensures that
		if there&aposs a spike in traffic on the frontend, messages do not get lost due to the backend process not being
		ready to process them.<br /><br />CORRECT: &quot;Implement code in microservice 1 to send data to an Amazon SQS
		queue. Implement code in microservice 2 to process messages from the queue&quot; is the correct answer.
		INCORRECT: &quot;Implement code in microservice 1 to send data to an Amazon <a href="#S3">S3</a> bucket. Use <a
			href="#S3">S3</a> event notifications to invoke microservice 2&quot; is incorrect as a message queue would
		be preferable to an <a href="#S3">S3</a> bucket.<br /><br />INCORRECT: &quot;Implement code in microservice 1 to
		publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic&quot; is
		incorrect as notifications to topics are pushed to subscribers. In this case we want the second microservice to
		pickup the messages when ready (pull them).<br /><br />INCORRECT: &quot;Implement code in microservice 1 to send
		data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose&quot;
		is incorrect as this is not how Firehose works. Firehose sends data directly to destinations, it is not a
		message queue.<br /><br />References:<br /><br />Amazon Simple Queue Service > Developer Guide > What is Amazon
		Simple Queue Service?</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 3<br />A company captures clickstream data from
		multiple websites and analyzes it using batch processing. The data is loaded nightly into Amazon Redshift and is
		consumed by business analysts. The company wants to move towards near&#8211;real&#8211;time data processing for
		timely insights. The solution should process the streaming data with minimal effort and operational
		overhead.<br /><br />Which combination of AWS services are MOST cost&#8211;effective for this solution? (Choose
		two.)<br /><br />A. Amazon <a href="#EC2">EC2</a><br />B. AWS Lambda<br />C. Amazon Kinesis Data Streams<br />D.
		Amazon Kinesis Data Firehose<br />E. Amazon Kinesis Data Analytics<br /><br /><b>Correct Answer:</b><br />A.
		Amazon <a href="#EC2">EC2</a><br />D. Amazon Kinesis Data Firehose<br /><br />Answer Description:<br />Kinesis
		Data Streams and Kinesis Client Library (KCL) &#8212; Data from the data source can be continuously captured and
		streamed in near real&#8211;time using Kinesis Data Streams. With the Kinesis Client Library (KCL), you can
		build your own application that can preprocess the streaming data as they arrive and emit the data for
		generating incremental views and downstream analysis. Kinesis Data Analytics &#8212; This service provides the
		easiest way to process the data that is streaming through Kinesis Data Stream or Kinesis Data Firehose using
		SQL. This enables customers to gain actionable insight in near real&#8211;time from the incremental stream
		before storing it in Amazon <a href="#S3">S3</a>.<br /><br />Lambda architecture building blocks on
		AWS<br /><br />References:<br /><br />Evolve from batch to real&#8211;time analytics</div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 4<br />A company&aposs application runs on Amazon
		<a href="#EC2">EC2</a> instances behind an Application Load Balancer (ALB). The instances run in an Amazon <a
			href="#EC2">EC2</a> Auto Scaling group across multiple Availability Zones. On the first day of every month
		at midnight, the application becomes much slower when the month&#8211;end financial calculation batch executes.
		This causes the CPU utilization of the <a href="#EC2">EC2</a> instances to immediately peak to 100%, which
		disrupts the application.<br /><br />What should a solutions architect recommend to ensure the application is
		able to handle the workload and avoid downtime?<br /><br />A. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution in front of the ALB.<br />B. Configure an <a
			href="#EC2">EC2</a> Auto Scaling simple scaling policy based on CPU utilization.<br />C. Configure an <a
			href="#EC2">EC2</a> Auto Scaling scheduled scaling policy based on the monthly schedule.<br />D. Configure
		Amazon ElastiCache to remove some of the workload from the <a href="#EC2">EC2</a>
		instances.<br /><br /><b>Correct Answer:</b><br />C. Configure an <a href="#EC2">EC2</a> Auto Scaling scheduled
		scaling policy based on the monthly schedule.<br /><br />Answer Description:<br />Scheduled Scaling for Amazon
		<a href="#EC2">EC2</a> Auto Scaling<br />Scheduled scaling allows you to set your own scaling schedule. For
		example, let&aposs say that every week the traffic to your web application starts to increase on Wednesday,
		remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the
		predictable traffic patterns of your web application. Scaling actions are performed automatically as a function
		of time and date.<br /><br />Scheduled scaling allows you to set your own scaling schedule. In this case the
		scaling action can be scheduled to occur just prior to the time that the reports will be run each month. Scaling
		actions are performed automatically as a function of time and date. This will ensure that there are enough <a
			href="#EC2">EC2</a> instances to serve the demand and prevent the application from slowing
		down.<br /><br />CORRECT: &quot;Configure an <a href="#EC2">EC2</a> Auto Scaling scheduled scaling policy based
		on the monthly schedule&quot; is the correct answer.<br /><br />INCORRECT: &quot;Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution in front of the ALB&quot; is incorrect as this would be more
		suitable for providing access to global users by caching content.<br /><br />INCORRECT: &quot;Configure an <a
			href="#EC2">EC2</a> Auto Scaling simple scaling policy based on CPU utilization&quot; is incorrect as this
		would not prevent the slow&#8211;down from occurring as there would be a delay between when the CPU hits 100%
		and the metric being reported and additional instances being launched.<br /><br />INCORRECT: &quot;Configure
		Amazon ElastiCache to remove some of the workload from the <a href="#EC2">EC2</a> instances&quot; is incorrect
		as ElastiCache is a database cache, it cannot replace the compute functions of an <a href="#EC2">EC2</a>
		instance.<br /><br />References:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling > User Guide > Scheduled
		scaling for Amazon <a href="#EC2">EC2</a> Auto Scaling</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 5<br />A company runs a multi&#8211;tier web
		application that hosts news content. The application runs on Amazon <a href="#EC2">EC2</a> instances behind an
		Application Load Balancer. The instances run in an <a href="#EC2">EC2</a> Auto Scaling group across multiple
		Availability Zones and use an Amazon Aurora database. A solutions architect needs to make the application more
		resilient to periodic increases in request rates.<br /><br />Which architecture should the solutions architect
		implement? (Choose two.)<br /><br />A. Add AWS Shield.<br />B. Add Aurora Replica.<br />C. Add AWS Direct
		Connect.<br />D. Add AWS Global Accelerator.<br />E. Add an Amazon <a href="#CloudFront">CloudFront</a>
		distribution in front of the Application Load Balancer.<br /><br /><b>Correct Answer:</b><br />B. Add Aurora
		Replica.<br />E. Add an Amazon <a href="#CloudFront">CloudFront</a> distribution in front of the Application
		Load Balancer.<br /><br />Answer Description:<br />AWS Global Accelerator: Acceleration for
		latency&#8211;sensitive applications. Many applications, especially in areas such as gaming, media, mobile apps,
		and financials, require very low latency for a great user experience. To improve the user experience, Global
		Accelerator directs user traffic to the application endpoint that is nearest to the client, which reduces
		internet latency and jitter.<br />Global Accelerator routes traffic to the closest edge location by using
		Anycast, and then routes it to the closest regional endpoint over the AWS global network. Global Accelerator
		quickly reacts to changes in network performance to improve your users&apos application
		performance.<br /><br />Amazon <a href="#CloudFront">CloudFront</a>: Amazon <a href="#CloudFront">CloudFront</a>
		is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to
		customers globally with low latency, high transfer speeds, all within a developer&#8211;friendly
		environment.<br /><br />The architecture is already highly resilient but the may be subject to performance
		degradation if there are sudden increases in request rates. To resolve this situation Amazon Aurora Read
		Replicas can be used to serve read traffic which offloads requests from the main database. On the frontend an
		Amazon <a href="#CloudFront">CloudFront</a> distribution can be placed in front of the ALB and this will cache
		content for better performance and also offloads requests from the backend.<br /><br />CORRECT: &quot;Add Amazon
		Aurora Replicas&quot; is the correct answer.<br /><br />CORRECT: &quot;Add an Amazon <a
			href="#CloudFront">CloudFront</a> distribution in front of the ALB&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Add and Amazon WAF in front of the ALB&quot; is incorrect. A web application
		firewall protects applications from malicious attacks. It does not improve performance.<br /><br />INCORRECT:
		&quot;Add an Amazon Transit Gateway to the Availability Zones&quot; is incorrect as this is used to connect
		on&#8211;premises networks to <a href="#VPC">VPC</a>s.<br /><br />INCORRECT: &quot;Add an Amazon Global
		Accelerator endpoint&quot; is incorrect as this service is used for directing users to different instances of
		the application in different regions based on latency.<br /><br />References:<br /><br />Amazon Aurora > User
		Guide for Aurora > Replication with Amazon Aurora<br />Amazon <a href="#CloudFront">CloudFront</a> > Developer
		Guide > What is Amazon <a href="#CloudFront">CloudFront</a>?</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 6<br />A product team is creating a new
		application that will store a large amount of data. The data will be analyzed hourly and modified by multiple
		Amazon <a href="#EC2">EC2</a> Linux instances. The application team believes the amount of space needed will
		continue to grow for the next 6 months.<br /><br />Which set of actions should a solutions architect take to
		support these needs?<br /><br />A. Store the data in an Amazon EBS volume. Mount the EBS volume on the
		application instances.<br />B. Store the data in an Amazon EFS file system. Mount the file system on the
		application instances.<br />C. Store the data in Amazon <a href="#S3">S3</a> Glacier. Update the vault policy to
		allow access to the application instances.<br />D. Store the data in Amazon <a href="#S3">S3</a>
		Standard&#8211;Infrequent Access (<a href="#S3">S3</a> Standard&#8211;IA). Update the bucket policy to allow
		access to the application instances.<br /><br /><b>Correct Answer:</b><br />B. Store the data in an Amazon EFS
		file system. Mount the file system on the application instances.<br /><br />Answer Description:<br />Amazon
		Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with
		AWS Cloud services and on&#8211;premises resources. It is built to scale on demand to petabytes without
		disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need
		to provision and manage capacity to accommodate growth.<br /><br />&quot;The data will be analyzed hourly and
		modified by multiple Amazon <a href="#EC2">EC2</a> Linux instances.&quot;<br /><br />Amazon EFS is designed to
		provide massively parallel shared access to thousands of Amazon <a href="#EC2">EC2</a> instances, enabling your
		applications to achieve high levels of aggregate throughput and IOPS with consistent low
		latencies.<br /><br />Amazon EFS is well suited to support a broad spectrum of use cases from home directories
		to business&#8211;critical applications. Customers can use EFS to lift&#8211;and&#8211;shift existing enterprise
		applications to the AWS Cloud. Other use cases include big data analytics, web serving and content management,
		application development and testing, media and entertainment workflows, database backups, and container
		storage.<br /><br />Amazon EFS is a regional service storing data within and across multiple Availability Zones
		(AZs) for high availability and durability. Amazon <a href="#EC2">EC2</a> instances can access your file system
		across AZs, regions, and <a href="#VPC">VPC</a>s, while on&#8211;premises servers can access using AWS Direct
		Connect or AWS VPN.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 7<br />A company is migrating a three&#8211;tier
		application to AWS. The application requires a MySQL database. In the past, the application users reported poor
		application performance when creating new entries. These performance issues were caused by users generating
		different real&#8211;time reports from the application during working hours.<br /><br />Which solution will
		improve the performance of the application when it is moved to AWS?<br /><br />A. Import the data into an Amazon
		DynamoDB table with provisioned capacity. Refactor the application to use DynamoDB for reports.<br />B. Create
		the database on a compute optimized Amazon <a href="#EC2">EC2</a> instance. Ensure compute resources exceed the
		on&#8211;premises database.<br />C. Create an Amazon Aurora MySQL Multi&#8211;AZ DB cluster with multiple read
		replicas. Configure the application to use the reader endpoint for reports.<br />D. Create an Amazon Aurora
		MySQL Multi&#8211;AZ DB cluster. Configure the application to use the backup instance of the cluster as an
		endpoint for the reports.<br /><br /><b>Correct Answer:</b><br />C. Create an Amazon Aurora MySQL Multi&#8211;AZ
		DB cluster with multiple read replicas. Configure the application to use the reader endpoint for
		reports.<br /><br />Answer Description:<br />Amazon RDS Read Replicas Now Support Multi&#8211;AZ
		Deployments<br /><br />Starting today, Amazon RDS Read Replicas for MySQL and MariaDB now support Multi&#8211;AZ
		deployments. Combining Read Replicas with Multi&#8211;AZ enables you to build a resilient disaster recovery
		strategy and simplify your database engine upgrade process.<br /><br />Amazon RDS Read Replicas enable you to
		create one or more read&#8211;only copies of your database instance within the same AWS Region or in a different
		AWS Region. Updates made to the source database are then asynchronously copied to your Read Replicas. In
		addition to providing scalability for read&#8211;heavy workloads, Read Replicas can be promoted to become a
		standalone database instance when needed.<br /><br />Amazon RDS Multi&#8211;AZ deployments provide enhanced
		availability for database instances within a single AWS Region. With Multi&#8211;AZ, your data is synchronously
		replicated to a standby in a different Availability Zone (AZ). In the event of an infrastructure failure, Amazon
		RDS performs an automatic failover to the standby, minimizing disruption to your applications.<br /><br />You
		can now use Read Replicas with Multi&#8211;AZ as part of a disaster recovery (DR) strategy for your production
		databases. A well&#8211;designed and tested DR plan is critical for maintaining business continuity after a
		disaster. A Read Replica in a different region than the source database can be used as a standby database and
		promoted to become the new production database in case of a regional disruption.<br /><br />You can also combine
		Read Replicas with Multi&#8211;AZ for your database engine upgrade process. You can create a Read Replica of
		your production database instance and upgrade it to a new database engine version. When the upgrade is complete,
		you can stop applications, promote the Read Replica to a standalone database instance, and switch over your
		applications. Since the database instance is already a Multi&#8211;AZ deployment, no additional steps are
		needed.<br /><br />Overview of Amazon RDS Read Replicas<br /><br />Deploying one or more read replicas for a
		given source DB instance might make sense in a variety of scenarios, including the following:<br /><br />Scaling
		beyond the compute or I/O capacity of a single DB instance for read&#8211;heavy database workloads. You can
		direct this excess read traffic to one or more read replicas.<br /><br />Serving read traffic while the source
		DB instance is unavailable. In some cases, your source DB instance might not be able to take I/O requests, for
		example due to I/O suspension for backups or scheduled maintenance. In these cases, you can direct read traffic
		to your read replicas. For this use case, keep in mind that the data on the read replica might be
		&quot;stale&quot; because the source DB instance is unavailable.<br /><br />Business reporting or data
		warehousing scenarios where you might want business reporting queries to run against a read replica, rather than
		your primary, production DB instance.<br /><br />Implementing disaster recovery. You can promote a read replica
		to a standalone instance as a disaster recovery solution if the source DB instance fails.<br /><br />The
		MySQL&#8211;compatible edition of Aurora delivers up to 5X the throughput of standard MySQL running on the same
		hardware, and enables existing MySQL applications and tools to run without requiring
		modification.<br /><br />References:<br /><br />Amazon Aurora Features: MySQL&#8211;Compatible Edition</div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 8<br />A solutions architect is deploying a
		distributed database on multiple Amazon <a href="#EC2">EC2</a> instances. The database stores all data on
		multiple instances so it can withstand the loss of an instance. The database requires block storage with latency
		and throughput to support several million transactions per second per server.<br /><br />Which storage solution
		should the solutions architect use?<br /><br />A. Amazon EBS<br />B. Amazon <a href="#EC2">EC2</a> instance
		store<br />C. Amazon EFS<br />D. Amazon <a href="#S3">S3</a><br /><br /><b>Correct Answer:</b><br />B. Amazon <a
			href="#EC2">EC2</a> instance store<br /><br />Answer Description:<br />It is block storage made for high
		throughput and low latency.<br /><br />References:<br /><br />Amazon Elastic Compute Cloud > User Guide for
		Linux Instances > Amazon <a href="#EC2">EC2</a> instance store</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 9<br />Organizers for a global event want to put
		daily reports online as static HTML pages. The pages are expected to generate millions of views from users
		around the world. The files are stored in an Amazon <a href="#S3">S3</a> bucket. A solutions architect has been
		asked to design an efficient and effective solution.<br /><br />Which action should the solutions architect take
		to accomplish this?<br /><br />A. Generate presigned URLs for the files.<br />B. Use cross&#8211;Region
		replication to all Regions.<br />C. Use the geoproximity feature of Amazon Route 53.<br />D. Use Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as its origin.<br /><br /><b>Correct
			Answer:</b><br />D. Use Amazon <a href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as
		its origin.<br /><br />Answer Description:<br />Using Amazon <a href="#S3">S3</a> Origins, MediaPackage
		Channels, and Custom Origins for Web Distributions<br /><br />Using Amazon <a href="#S3">S3</a> Buckets for Your
		Origin<br />When you use Amazon <a href="#S3">S3</a> as an origin for your distribution, you place any objects
		that you want <a href="#CloudFront">CloudFront</a> to deliver in an Amazon <a href="#S3">S3</a> bucket. You can
		use any method that is supported by Amazon <a href="#S3">S3</a> to get your objects into Amazon <a
			href="#S3">S3</a>, for example, the Amazon <a href="#S3">S3</a> console or API, or a third&#8211;party tool.
		You can create a hierarchy in your bucket to store the objects, just as you would with any other Amazon <a
			href="#S3">S3</a> bucket.<br /><br />Using an existing Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change the bucket in any way; you can still use
		it as you normally would to store and access Amazon <a href="#S3">S3</a> objects at the standard Amazon <a
			href="#S3">S3</a> price. You incur regular Amazon <a href="#S3">S3</a> charges for storing the objects in
		the bucket.<br /><br />Using Amazon <a href="#S3">S3</a> Buckets Configured as Website Endpoints for Your
		Origin<br />You can set up an Amazon <a href="#S3">S3</a> bucket that is configured as a website endpoint as
		custom origin with <a href="#CloudFront">CloudFront</a>.<br /><br />When you configure your <a
			href="#CloudFront">CloudFront</a> distribution, for the origin, enter the Amazon <a href="#S3">S3</a> static
		website hosting endpoint for your bucket. This value appears in the Amazon <a href="#S3">S3</a> console, on the
		Properties tab, in the Static website hosting pane. For example:
		http://bucket&#8211;name.s3&#8211;website&#8211;region.amazonaws.com<br /><br />For more information about
		specifying Amazon <a href="#S3">S3</a> static website endpoints, see Website endpoints in the Amazon Simple
		Storage Service Developer Guide.<br /><br />When you specify the bucket name in this format as your origin, you
		can use Amazon <a href="#S3">S3</a> redirects and Amazon <a href="#S3">S3</a> custom error documents. For more
		information about Amazon <a href="#S3">S3</a> features, see the Amazon <a href="#S3">S3</a>
		documentation.<br /><br />Using an Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change it in any way. You can still use it as
		you normally would and you incur regular Amazon <a href="#S3">S3</a> charges.<br /><br />Amazon <a
			href="#CloudFront">CloudFront</a> can be used to cache the files in edge locations around the world and this
		will improve the performance of the webpages.<br /><br />To serve a static website hosted on Amazon <a
			href="#S3">S3</a>, you can deploy a <a href="#CloudFront">CloudFront</a> distribution using one of these
		configurations:<br /><br />Using a REST API endpoint as the origin with access restricted by an origin access
		identity (OAI) Using a website endpoint as the origin with anonymous (public) access allowed<br /><br />Using a
		website endpoint as the origin with access restricted by a Referer header CORRECT: &quot;Use Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as its origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Generate presigned URLs for the files&quot; is incorrect as this is used to
		restrict access which is not a requirement.<br /><br />INCORRECT: &quot;Use cross&#8211;Region replication to
		all Regions&quot; is incorrect as this does not provide a mechanism for directing users to the closest copy of
		the static webpages.<br /><br />INCORRECT: &quot;Use the geoproximity feature of Amazon Route 53&quot; is
		incorrect as this does not include a solution for having multiple copies of the data in different geographic
		locations.<br /><br />References:<br /><br />How do I use <a href="#CloudFront">CloudFront</a> to serve a static
		website hosted on Amazon <a href="#S3">S3</a>?</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 10<br />A start&#8211;up company has a web
		application based in the us&#8211;east&#8211;1 Region with multiple Amazon <a href="#EC2">EC2</a> instances
		running behind an Application Load Balancer across multiple Availability Zones. As the company&aposs user base
		grows in the us&#8211;west&#8211;1 Region, it needs a solution with low latency and high
		availability.<br /><br />What should a solutions architect do to accomplish this?<br /><br />A. Provision <a
			href="#EC2">EC2</a> instances in us&#8211;west&#8211;1. Switch the Application Load Balancer to a Network
		Load Balancer to achieve cross&#8211;Region load balancing.<br />B. Provision <a href="#EC2">EC2</a> instances
		and an Application Load Balancer in us&#8211;west&#8211;1. Make the load balancer distribute the traffic based
		on the location of the request.<br />C. Provision <a href="#EC2">EC2</a> instances and configure an Application
		Load Balancer in us&#8211;west&#8211;1. Create an accelerator in AWS Global Accelerator that uses an endpoint
		group that includes the load balancer endpoints in both Regions.<br />D. Provision <a href="#EC2">EC2</a>
		instances and configure an Application Load Balancer in us&#8211;west&#8211;1. Configure Amazon Route 53 with a
		weighted routing policy. Create alias records in Route 53 that point to the Application Load
		Balancer.<br /><br /><b>Correct Answer:</b><br />C. Provision <a href="#EC2">EC2</a> instances and configure an
		Application Load Balancer in us&#8211;west&#8211;1. Create an accelerator in AWS Global Accelerator that uses an
		endpoint group that includes the load balancer endpoints in both Regions.<br /><br />Answer
		Description:<br />Register endpoints for endpoint groups: You register one or more regional resources, such as
		Application Load Balancers, Network Load Balancers, <a href="#EC2">EC2</a> Instances, or Elastic IP addresses,
		in each endpoint group. Then you can set weights to choose how much traffic is routed to each
		endpoint.<br /><br />Endpoints in AWS Global Accelerator: Endpoints in AWS Global Accelerator can be Network
		Load Balancers, Application Load Balancers, Amazon <a href="#EC2">EC2</a> instances, or Elastic IP addresses. A
		static IP address serves as a single point of contact for clients, and Global Accelerator then distributes
		incoming traffic across healthy endpoints. Global Accelerator directs traffic to endpoints by using the port (or
		port range) that you specify for the listener that the endpoint group for the endpoint belongs
		to.<br /><br />Each endpoint group can have multiple endpoints. You can add each endpoint to multiple endpoint
		groups, but the endpoint groups must be associated with different listeners.<br /><br />Global Accelerator
		continually monitors the health of all endpoints that are included in an endpoint group. It routes traffic only
		to the active endpoints that are healthy. If Global Accelerator doesn&apost have any healthy endpoints to route
		traffic to, it routes traffic to all endpoints.<br /><br />ELB provides load balancing within one Region, AWS
		Global Accelerator provides traffic management across multiple Regions […] AWS Global Accelerator complements
		ELB by extending these capabilities beyond a single AWS Region, allowing you to provision a global interface for
		your applications in any number of Regions. If you have workloads that cater to a global client base, we
		recommend that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by
		clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to
		manage your resources.<br /><br />References:<br /><br />AWS Global Accelerator FAQs<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 11<br />A solutions architect is designing a
		solution to access a catalog of images and provide users with the ability to submit requests to customize
		images. Image customization parameters will be in any request sent to an AWS API Gateway API. The customized
		image will be generated on demand, and users will receive a link they can click to view or download their
		customized image. The solution must be highly available for viewing and customizing images.<br /><br />What is
		the MOST cost&#8211;effective solution to meet these requirements?<br /><br />A. Use Amazon <a
			href="#EC2">EC2</a> instances to manipulate the original image into the requested customization. Store the
		original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an Elastic Load Balancer in front of
		the <a href="#EC2">EC2</a> instances.<br />B. Use AWS Lambda to manipulate the original image to the requested
		customization. Store the original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the origin.<br />C.
		Use AWS Lambda to manipulate the original image to the requested customization. Store the original images in
		Amazon <a href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in
		front of the Amazon <a href="#EC2">EC2</a> instances.<br />D. Use Amazon <a href="#EC2">EC2</a> instances to
		manipulate the original image into the requested customization. Store the original images in Amazon <a
			href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the
		origin.<br /><br /><b>Correct Answer:</b><br />B. Use AWS Lambda to manipulate the original image to the
		requested customization. Store the original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an
		Amazon <a href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the
		origin.<br /><br />Answer Description:<br />AWS Lambda is a compute service that lets you run code without
		provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from
		a few requests per day to thousands per second. You pay only for the compute time you consume &#8212; there is
		no charge when your code is not running. With AWS Lambda, you can run code for virtually any type of application
		or backend service &#8212; all with zero administration. AWS Lambda runs your code on a high&#8211;availability
		compute infrastructure and performs all of the administration of the compute resources, including server and
		operating system maintenance, capacity provisioning and automatic scaling, code monitoring, and
		logging.<br /><br /><a href="#All">All</a> you need to do is supply your code in one of the languages that AWS
		Lambda supports.<br /><br />Storing your static content with <a href="#S3">S3</a> provides a lot of advantages.
		But to help optimize your application&aposs performance and security while effectively managing cost, we
		recommend that you also set up Amazon <a href="#CloudFront">CloudFront</a> to work with your <a
			href="#S3">S3</a> bucket to serve and protect the content. <a href="#CloudFront">CloudFront</a> is a content
		delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the
		world, securely and at scale. By design, delivering data out of <a href="#CloudFront">CloudFront</a> can be more
		cost effective than delivering it from <a href="#S3">S3</a> directly to your users.<br /><br /><a
			href="#CloudFront">CloudFront</a> serves content through a worldwide network of data centers called Edge
		Locations. Using edge servers to cache and serve content improves performance by providing content closer to
		where viewers are located. <a href="#CloudFront">CloudFront</a> has edge servers in locations all around the
		world.<br /><br /><a href="#All">All</a> solutions presented are highly available. The key requirement that must
		be satisfied is that the solution should be cost&#8211;effective and you must choose the most
		cost&#8211;effective option.<br /><br />Therefore, it&aposs best to eliminate services such as Amazon <a
			href="#EC2">EC2</a> and ELB as these require ongoing costs even when they&aposre not used. Instead, a fully
		serverless solution should be used. AWS Lambda, Amazon <a href="#S3">S3</a> and <a
			href="#CloudFront">CloudFront</a> are the best services to use for these requirements.<br /><br />CORRECT:
		&quot;Use AWS Lambda to manipulate the original images to the requested customization. Store the original and
		manipulated images in Amazon <a href="#S3">S3</a>. Configure an Amazon <a href="#CloudFront">CloudFront</a>
		distribution with the <a href="#S3">S3</a> bucket as the origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Use Amazon <a href="#EC2">EC2</a> instances to manipulate the original
		images into the requested customization. Store the original and manipulated images in Amazon <a
			href="#S3">S3</a>. Configure an Elastic Load Balancer in front of the <a href="#EC2">EC2</a> instances&quot;
		is incorrect. This is not the most cost&#8211;effective option as the ELB and <a href="#EC2">EC2</a> instances
		will incur costs even when not used.<br /><br />INCORRECT: &quot;Use AWS Lambda to manipulate the original
		images to the requested customization. Store the original images in Amazon <a href="#S3">S3</a> and the
		manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in front of the Amazon <a
			href="#EC2">EC2</a> instances&quot; is incorrect. This is not the most cost&#8211;effective option as the
		ELB will incur costs even when not used. Also, Amazon DynamoDB will incur RCU/WCUs when running and is not the
		best choice for storing images.<br /><br />INCORRECT: &quot;Use Amazon <a href="#EC2">EC2</a> instances to
		manipulate the original images into the requested customization. Store the original images in Amazon <a
			href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the origin&quot; is
		incorrect. This is not the most cost&#8211;effective option as the <a href="#EC2">EC2</a> instances will incur
		costs even when not used.<br /><br />References:<br /><br />Serverless on AWS</div><a href="#All">All(200)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 12<br />A company is planning to migrate a
		business&#8211;critical dataset to Amazon <a href="#S3">S3</a>. The current solution design uses a single <a
			href="#S3">S3</a> bucket in the us&#8211;east&#8211;1 Region with versioning enabled to store the dataset.
		The company&aposs disaster recovery policy states that all data multiple AWS Regions.<br /><br />How should a
		solutions architect design the <a href="#S3">S3</a> solution?<br /><br />A. Create an additional <a
			href="#S3">S3</a> bucket in another Region and configure cross&#8211;Region replication.<br />B. Create an
		additional <a href="#S3">S3</a> bucket in another Region and configure cross&#8211;origin resource sharing
		(CORS).<br />C. Create an additional <a href="#S3">S3</a> bucket with versioning in another Region and configure
		cross&#8211;Region replication.<br />D. Create an additional <a href="#S3">S3</a> bucket with versioning in
		another Region and configure cross&#8211;origin resource (CORS).<br /><br /><b>Correct Answer:</b><br />C.
		Create an additional <a href="#S3">S3</a> bucket with versioning in another Region and configure
		cross&#8211;Region replication.<br /><br />Answer Description:<br />Replication enables automatic, asynchronous
		copying of objects across Amazon <a href="#S3">S3</a> buckets. Buckets that are configured for object
		replication can be owned by the same AWS account or by different accounts. You can copy objects between
		different AWS Regions or within the same Region. Both source and destination buckets must have versioning
		enabled.<br /><br />CORRECT: &quot;Create an additional <a href="#S3">S3</a> bucket with versioning in another
		Region and configure cross&#8211;Region replication&quot; is the correct answer.<br /><br />INCORRECT:
		&quot;Create an additional <a href="#S3">S3</a> bucket in another Region and configure cross&#8211;Region
		replication&quot; is incorrect as the destination bucket must also have versioning
		enabled.<br /><br />INCORRECT: &quot;Create an additional <a href="#S3">S3</a> bucket in another Region and
		configure cross&#8211;origin resource sharing (CORS)&quot; is incorrect as CORS is not related to
		replication.<br /><br />INCORRECT: &quot;Create an additional <a href="#S3">S3</a> bucket with versioning in
		another Region and configure cross&#8211;origin resource sharing (CORS)&quot; is incorrect as CORS is not
		related to replication.<br /><br />References:<br />Amazon Simple Storage Service > User Guide > Replicating
		objects<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 13<br />A company has application running on
		Amazon <a href="#EC2">EC2</a> instances in a <a href="#VPC">VPC</a>. One of the applications needs to call an
		Amazon <a href="#S3">S3</a> API to store and read objects. The company&aposs security policies restrict any
		internet&#8211;bound traffic from the applications.<br /><br />Which action will fulfill these requirements and
		maintain security?<br /><br />A. Configure an <a href="#S3">S3</a> interface endpoint.<br />B. Configure an <a
			href="#S3">S3</a> gateway endpoint.<br />C. Create an <a href="#S3">S3</a> bucket in a private
		subnet.<br />D. Create an <a href="#S3">S3</a> bucket in the same Region as the <a href="#EC2">EC2</a>
		instance.<br /><br /><b>Correct Answer:</b><br />B. Configure an <a href="#S3">S3</a> gateway
		endpoint.<br /><br />Answer Description:<br /><a href="#VPC">VPC</a> endpoints: A <a href="#VPC">VPC</a>
		endpoint enables you to privately connect your <a href="#VPC">VPC</a> to supported AWS services and <a
			href="#VPC">VPC</a> endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT
		device, VPN connection, or AWS Direct Connect connection. Instances in your <a href="#VPC">VPC</a> do not
		require public IP addresses to communicate with resources in the service. Traffic between your <a
			href="#VPC">VPC</a> and the other service does not leave the Amazon network.<br /><br />An interface
		endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that
		serves as an entry point for traffic destined to a supported service. Interface endpoints are powered by AWS
		PrivateLink, a technology that enables you to privately access services by using private IP addresses. AWS
		PrivateLink restricts all network traffic between your <a href="#VPC">VPC</a> and services to the Amazon
		network. You do not need an internet gateway, a NAT device, or a virtual private
		gateway.<br /><br />References:<br /><br />Amazon Virtual Private Cloud > AWS PrivateLink > Endpoints for Amazon
		<a href="#S3">S3</a><br />Amazon Virtual Private Cloud > AWS PrivateLink > Gateway <a href="#VPC">VPC</a>
		endpoints</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 14<br />A company&aposs web application uses an
		Amazon RDS PostgreSQL DB instance to store its application data.<br /><br />During the financial closing period
		at the start of every month, Accountants run large queries that impact the database&aposs performance due to
		high usage. The company wants to minimize the impact that the reporting activity has on the web
		application.<br /><br />What should a solutions architect do to reduce the impact on the database with the LEAST
		amount of effort?<br /><br />A. Create a read replica and direct reporting traffic to the replica.<br />B.
		Create a Multi&#8211;AZ database and direct reporting traffic to the standby.<br />C. Create a
		cross&#8211;Region read replica and direct reporting traffic to the replica.<br />D. Create an Amazon Redshift
		database and direct reporting traffic to the Amazon Redshift database.<br /><br /><b>Correct Answer:</b><br />A.
		Create a read replica and direct reporting traffic to the replica.<br /><br />Answer Description:<br />Amazon
		RDS uses the MariaDB, MySQL, Oracle, PostgreSQL, and Microsoft SQL Server DB engines&apos built&#8211;in
		replication functionality to create a special type of DB instance called a read replica from a source DB
		instance. Updates made to the source DB instance are asynchronously copied to the read replica. You can reduce
		the load on your source DB instance by routing read queries from your applications to the read
		replica.<br /><br />When you create a read replica, you first specify an existing DB instance as the source.
		Then Amazon RDS takes a snapshot of the source instance and creates a read&#8211;only instance from the
		snapshot. Amazon RDS then uses the asynchronous replication method for the DB engine to update the read replica
		whenever there is a change to the source DB instance. The read replica operates as a DB instance that allows
		only read&#8211;only connections. Applications connect to a read replica the same way they do to any DB
		instance.<br /><br />Amazon RDS replicates all databases in the source DB
		instance.<br /><br />References:<br /><br />Amazon Relational Database Service > User Guide > Working with read
		replicas</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 15<br />A solutions architect needs to ensure that
		API calls to Amazon DynamoDB from Amazon <a href="#EC2">EC2</a> instances in a <a href="#VPC">VPC</a> do not
		traverse the internet.<br /><br />What should the solutions architect do to accomplish this? (Choose
		two.)<br /><br />A. Create a route table entry for the endpoint.<br />B. Create a gateway endpoint for
		DynamoDB.<br />C. Create a new DynamoDB table that uses the endpoint.<br />D. Create an ENI for the endpoint in
		each of the subnets of the <a href="#VPC">VPC</a>.<br />E. Create a security group entry in the default security
		group to provide access.<br /><br /><b>Correct Answer:</b><br />A. Create a route table entry for the
		endpoint.<br />B. Create a gateway endpoint for DynamoDB.<br /><br />Answer Description:<br />A <a
			href="#VPC">VPC</a> endpoint enables you to privately connect your <a href="#VPC">VPC</a> to supported AWS
		services and <a href="#VPC">VPC</a> endpoint services powered by AWS PrivateLink without requiring an internet
		gateway, NAT device, VPN connection, or AWS Direct Connect connection.<br /><br />Instances in your <a
			href="#VPC">VPC</a> do not require public IP addresses to communicate with resources in the service. Traffic
		between your <a href="#VPC">VPC</a> and the other service does not leave the Amazon network.<br /><br />Gateway
		endpoints: A gateway endpoint is a gateway that you specify as a target for a route in your route table for
		traffic destined to a supported AWS service. The following AWS services are supported:<br />Amazon <a
			href="#S3">S3</a><br />DynamoDB<br /><br />Amazon DynamoDB and Amazon <a href="#S3">S3</a> support gateway
		endpoints, not interface endpoints. With a gateway endpoint you create the endpoint in the <a
			href="#VPC">VPC</a>, attach a policy allowing access to the service, and then specify the route table to
		create a route table entry in.<br /><br />CORRECT: &quot;Create a route table entry for the endpoint&quot; is a
		correct answer. CORRECT: &quot;Create a gateway endpoint for DynamoDB&quot; is also a correct
		answer.<br /><br />INCORRECT: &quot;Create a new DynamoDB table that uses the endpoint&quot; is incorrect as it
		is not necessary to create a new DynamoDB table.<br /><br />INCORRECT: &quot;Create an ENI for the endpoint in
		each of the subnets of the <a href="#VPC">VPC</a>&quot; is incorrect as an ENI is used by an interface endpoint,
		not a gateway endpoint.<br /><br />INCORRECT: &quot;Create a <a href="#VPC">VPC</a> peering connection between
		the <a href="#VPC">VPC</a> and DynamoDB&quot; is incorrect as you cannot create a <a href="#VPC">VPC</a> peering
		connection between a <a href="#VPC">VPC</a> and a public AWS service as public services are outside of <a
			href="#VPC">VPC</a>s.<br /><br />References:<br /><br />Amazon Virtual Private Cloud > AWS PrivateLink >
		Gateway <a href="#VPC">VPC</a> endpoints</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 16<br />A company has been storing analytics data
		in an Amazon RDS instance for the past few years. The company asked a solutions architect to find a solution
		that allows users to access this data using an API.<br /><br />The expectation is that the application will
		experience periods of inactivity but could receive bursts of traffic within seconds.<br /><br />Which solution
		should the solution architect suggest?<br /><br />A. Set up an Amazon API Gateway and use Amazon <a
			href="#ECS">ECS</a>.<br />B. Set up an Amazon API Gateway and use AWS Elastic Beanstalk.<br />C. Set up an
		Amazon API Gateway and use AWS Lambda functions.<br />D. Set up an Amazon API Gateway and use Amazon <a
			href="#EC2">EC2</a> with Auto Scaling.<br /><br /><b>Correct Answer:</b><br />C. Set up an Amazon API
		Gateway and use AWS Lambda functions.<br /><br />Answer Description:<br />AWS Lambda: With Lambda, you can run
		code for virtually any type of application or backend service &#8212; all with zero administration. Just upload
		your code and Lambda takes care of everything required to run and scale your code with high availability. You
		can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile
		app.<br /><br />How it works<br /><br />Amazon API Gateway: Amazon API Gateway is a fully managed service that
		makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as
		the &quot;front door&quot; for applications to access data, business logic, or functionality from your backend
		services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real&#8211;time
		two&#8211;way communication applications. API Gateway supports containerized and serverless workloads, as well
		as web applications.<br /><br />API Gateway handles all the tasks involved in accepting and processing up to
		hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and
		access control, throttling, monitoring, and API version management. API Gateway has no minimum fees or startup
		costs.<br /><br />You pay for the API calls you receive and the amount of data transferred out and, with the API
		Gateway tiered pricing model, you can reduce your cost as your API usage scales.<br /><br />This question is
		simply asking you to work out the best compute service for the stated requirements. The key requirements are
		that the compute service should be suitable for a workload that can range quite broadly in demand from no
		requests to large bursts of traffic. AWS Lambda is an ideal solution as you pay only when requests are made and
		it can easily scale to accommodate the large bursts in traffic. Lambda works well with both API Gateway and
		Amazon RDS.<br /><br />CORRECT: &quot;Set up an Amazon API Gateway and use AWS Lambda functions&quot; is the
		correct answer.<br /><br />INCORRECT: &quot;Set up an Amazon API Gateway and use Amazon <a
			href="#ECS">ECS</a>&quot; is incorrect as Lambda is a better fit for this use case as the traffic patterns
		are highly dynamic.<br /><br />INCORRECT: &quot;Set up an Amazon API Gateway and use AWS Elastic Beanstalk&quot;
		is incorrect as Lambda is a better fit for this use case as the traffic patterns are highly
		dynamic.<br /><br />INCORRECT: &quot;Set up an Amazon API Gateway and use Amazon <a href="#EC2">EC2</a> with
		Auto Scaling&quot; is incorrect as Lambda is a better fit for this use case as the traffic patterns are highly
		dynamic.<br /><br />References:<br /><br />AWS Lambda > Developer Guide > Lambda function scaling<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 17<br />A company must generate sales reports at
		the beginning of every month. The reporting process launches 20 Amazon <a href="#EC2">EC2</a> instances on the
		first of the month. The process runs for 7 days and cannot be interrupted.<br /><br />The company wants to
		minimize costs.<br /><br />Which pricing model should the company choose?<br /><br />A. Reserved
		Instances<br />B. Spot Block Instances<br />C. On&#8211;Demand Instances<br />D. Scheduled Reserved
		Instances<br /><br /><b>Correct Answer:</b><br />D. Scheduled Reserved Instances<br /><br />Answer
		Description:<br />Scheduled Reserved Instances: Scheduled Reserved Instances (Scheduled Instances) enable you to
		purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and
		duration, for a one&#8211;year term. You reserve the capacity in advance, so that you know it is available when
		you need it. You pay for the time that the instances are scheduled, even if you do not use
		them.<br /><br />Scheduled Instances are a good choice for workloads that do not run continuously, but do run on
		a regular schedule. For example, you can use Scheduled Instances for an application that runs during business
		hours or for batch processing that runs at the end of the week.<br /><br />If you require a capacity reservation
		on a continuous basis, Reserved Instances might meet your needs and decrease costs.<br /><br />How Scheduled
		Instances Work<br /><br />Amazon <a href="#EC2">EC2</a> sets aside pools of <a href="#EC2">EC2</a> instances in
		each Availability Zone for use as Scheduled Instances.<br /><br />Each pool supports a specific combination of
		instance type, operating system, and network.<br /><br />To get started, you must search for an available
		schedule. You can search across multiple pools or a single pool. After you locate a suitable schedule, purchase
		it.<br /><br />You must launch your Scheduled Instances during their scheduled time periods, using a launch
		configuration that matches the following attributes of the schedule that you purchased: instance type,
		Availability Zone, network, and platform. When you do so, Amazon <a href="#EC2">EC2</a> launches <a
			href="#EC2">EC2</a> instances on your behalf, based on the specified launch specification. Amazon <a
			href="#EC2">EC2</a> must ensure that the <a href="#EC2">EC2</a> instances have terminated by the end of the
		current scheduled time period so that the capacity is available for any other Scheduled Instances it is reserved
		for. Therefore, Amazon <a href="#EC2">EC2</a> terminates the <a href="#EC2">EC2</a> instances three minutes
		before the end of the current scheduled time period.<br /><br />You can&apost stop or reboot Scheduled
		Instances, but you can terminate them manually as needed. If you terminate a Scheduled Instance before its
		current scheduled time period ends, you can launch it again after a few minutes. Otherwise, you must wait until
		the next scheduled time period.<br /><br />The following diagram illustrates the lifecycle of a Scheduled
		Instance.<br /><br /><br />References:<br /><br />Amazon Elastic Compute Cloud > User Guide for Linux Instances
		> Scheduled Reserved Instances</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 18<br />A gaming company has multiple Amazon <a
			href="#EC2">EC2</a> instances in a single Availability Zone for its multiplayer game that communicates with
		users on Layer 4. The chief technology officer (CTO) wants to make the architecture highly available and
		cost&#8211;effective.<br />What should a solutions architect do to meet these requirements? (Choose
		two.)?<br /><br />A. Increase the number of <a href="#EC2">EC2</a> instances.<br />B. Decrease the number of <a
			href="#EC2">EC2</a> instances.<br />C. Configure a Network Load Balancer in front of the <a
			href="#EC2">EC2</a> instances.<br />D. Configure an Application Load Balancer in front of the <a
			href="#EC2">EC2</a> instances.<br />E. Configure an Auto Scaling group to add or remove instances in
		multiple Availability Zones automatically.<br /><br /><b>Correct Answer:</b><br />C. Configure a Network Load
		Balancer in front of the <a href="#EC2">EC2</a> instances.<br />E. Configure an Auto Scaling group to add or
		remove instances in multiple Availability Zones automatically.<br /><br />Answer Description:<br />Network Load
		Balancer overview: A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection
		(OSI) model. It can handle millions of requests per second. After the load balancer receives a connection
		request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to
		the selected target on the port specified in the listener configuration.<br /><br />When you enable an
		Availability Zone for the load balancer, Elastic Load Balancing creates a load balancer node in the Availability
		Zone. By default, each load balancer node distributes traffic across the registered targets in its Availability
		Zone only. If you enable cross&#8211;zone load balancing, each load balancer node distributes traffic across the
		registered targets in all enabled Availability Zones. For more information, see Availability
		Zones.<br /><br />If you enable multiple Availability Zones for your load balancer and ensure that each target
		group has at least one target in each enabled Availability Zone, this increases the fault tolerance of your
		applications. For example, if one or more target groups does not have a healthy target in an Availability Zone,
		we remove the IP address for the corresponding subnet from DNS, but the load balancer nodes in the other
		Availability Zones are still available to route traffic. If a client doesn&apost honor the
		time&#8211;to&#8211;live (TTL) and sends requests to the IP address after it is removed from DNS, the requests
		fail.<br /><br />For TCP traffic, the load balancer selects a target using a flow hash algorithm based on the
		protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number. The
		TCP connections from a client have different source ports and sequence numbers, and can be routed to different
		targets. Each individual TCP connection is routed to a single target for the life of the
		connection.<br /><br />For UDP traffic, the load balancer selects a target using a flow hash algorithm based on
		the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the
		same source and destination, so it is consistently routed to a single target throughout its lifetime. Different
		UDP flows have different source IP addresses and ports, so they can be routed to different
		targets.<br /><br />An Auto Scaling group contains a collection of Amazon <a href="#EC2">EC2</a> instances that
		are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group
		also enables you to use Amazon <a href="#EC2">EC2</a> Auto Scaling features such as health check replacements
		and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling
		are the core functionality of the Amazon <a href="#EC2">EC2</a> Auto Scaling service.<br /><br />The size of an
		Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its
		size to meet demand, either manually or by using automatic scaling.<br /><br />An Auto Scaling group starts by
		launching enough instances to meet its desired capacity. It maintains this number of instances by performing
		periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed
		number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group
		terminates the unhealthy instance and launches another instance to replace it.<br /><br />The solutions
		architect must enable high availability for the architecture and ensure it is cost&#8211; effective. To enable
		high availability an Amazon <a href="#EC2">EC2</a> Auto Scaling group should be created to add and remove
		instances across multiple availability zones.<br /><br />In order to distribute the traffic to the instances the
		architecture should use a Network Load Balancer which operates at Layer 4. This architecture will also be
		cost&#8211;effective as the Auto Scaling group will ensure the right number of instances are running based on
		demand.<br /><br />CORRECT: &quot;Configure a Network Load Balancer in front of the <a href="#EC2">EC2</a>
		instances&quot; is a correct answer.<br /><br />CORRECT: &quot;Configure an Auto Scaling group to add or remove
		instances in multiple Availability Zones automatically&quot; is also a correct answer.<br /><br />INCORRECT:
		&quot;Increase the number of instances and use smaller <a href="#EC2">EC2</a> instance types&quot; is incorrect
		as this is not the most cost&#8211;effective option. Auto Scaling should be used to maintain the right number of
		active instances.<br /><br />INCORRECT: &quot;Configure an Auto Scaling group to add or remove instances in the
		Availability Zone automatically&quot; is incorrect as this is not highly available as it&aposs a single
		AZ.<br /><br />INCORRECT: &quot;Configure an Application Load Balancer in front of the <a href="#EC2">EC2</a>
		instances&quot; is incorrect as an ALB operates at Layer 7 rather than Layer
		4.<br /><br />References:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling > User Guide > Elastic Load
		Balancing and Amazon <a href="#EC2">EC2</a> Auto Scaling<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 19<br />A solutions architect has created a new
		AWS account and must secure AWS account root user access.<br /><br />Which combination of actions will
		accomplish this? (Choose two.)<br /><br />A. Ensure the root user uses a strong password.<br />B. Enable
		multi&#8211;factor authentication to the root user.<br />C. Store root user access keys in an encrypted Amazon
		<a href="#S3">S3</a> bucket.<br />D. Add the root user to a group containing administrative permissions.<br />E.
		Apply the required permissions to the root user with an inline policy document.<br /><br /><b>Correct
			Answer:</b><br />A. Ensure the root user uses a strong password.<br />B. Enable multi&#8211;factor
		authentication to the root user.<br /><br />Answer Description:<br />&quot;Enable MFA&quot;<br />The AWS Account
		Root User &#8212; https://docs.aws.amazon.com/<a href="#IAM">IAM</a>/latest/UserGuide/id_root&#8211;
		user.html<br />&quot;Choose a strong password&quot;<br />Changing the AWS Account Root User Password
		&#8212;<br /><br />References:<br /><br />AWS Identity and Access Management > User Guide > Changing the AWS
		account root user password</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 20<br />A company&aposs website is used to sell
		products to the public. The site runs on Amazon <a href="#EC2">EC2</a> instances in an Auto Scaling group behind
		an Application Load Balancer (ALB). There is also an Amazon <a href="#CloudFront">CloudFront</a> distribution,
		and AWS WAF is being used to protect against SQL injection attacks. The ALB is the origin for the <a
			href="#CloudFront">CloudFront</a> distribution. A recent review of security logs revealed an external
		malicious IP that needs to be blocked from accessing the website.<br /><br />What should a solutions architect
		do to protect the application?<br /><br />A. Modify the network ACL on the <a href="#CloudFront">CloudFront</a>
		distribution to add a deny rule for the malicious IP address.<br />B. Modify the configuration of AWS WAF to add
		an IP match condition to block the malicious IP address.<br />C. Modify the network ACL for the <a
			href="#EC2">EC2</a> instances in the target groups behind the ALB to deny the malicious IP address.<br />D.
		Modify the security groups for the <a href="#EC2">EC2</a> instances in the target groups behind the ALB to deny
		the malicious IP address.<br /><br /><b>Correct Answer:</b><br />B. Modify the configuration of AWS WAF to add
		an IP match condition to block the malicious IP address.<br /><br />Answer Description:<br />If you want to
		allow or block web requests based on the IP addresses that the requests originate from, create one or more IP
		match conditions. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests
		originate from. Later in the process, when you create a web ACL, you specify whether to allow or block requests
		from those IP addresses.<br /><br />AWS Web Application Firewall (WAF) &#8212; Helps to protect your web
		applications from common application layer exploits that can affect availability or consume excessive resources.
		As you can see in my post (New &#8212; AWS WAF), WAF allows you to use access control lists (ACLs), rules, and
		conditions that define acceptable or unacceptable requests or IP addresses. You can selectively allow or deny
		access to specific parts of your web application and you can also guard against various SQL injection attacks.
		We launched WAF with support for Amazon <a href="#CloudFront">CloudFront</a>.<br /><br />A new version of the
		AWS Web Application Firewall was released in November 2019. With AWS WAF classic you create &quot;IP match
		conditions&quot;, whereas with AWS WAF (new version) you create &quot;IP set match statements&quot;. Look out
		for wording on the exam.<br /><br />The IP match condition / IP set match statement inspects the IP address of a
		web request&aposs origin against a set of IP addresses and address ranges.<br /><br />Use this to allow or block
		web requests based on the IP addresses that the requests originate from.<br /><br />AWS WAF supports all IPv4
		and IPv6 address ranges. An IP set can hold up to 10,000 IP addresses or IP address ranges to
		check.<br /><br />CORRECT: &quot;Modify the configuration of AWS WAF to add an IP match condition to block the
		malicious IP address&quot; is the correct answer.<br /><br />INCORRECT: &quot;Modify the network ACL on the <a
			href="#CloudFront">CloudFront</a> distribution to add a deny rule for the malicious IP address&quot; is
		incorrect as <a href="#CloudFront">CloudFront</a> does not sit within a subnet so network ACLs do not apply to
		it.<br /><br />INCORRECT: &quot;Modify the network ACL for the <a href="#EC2">EC2</a> instances in the target
		groups behind the ALB to deny the malicious IP address&quot; is incorrect as the source IP addresses of the data
		in the <a href="#EC2">EC2</a> instances&apos subnets will be the ELB IP addresses.<br /><br />INCORRECT:
		&quot;Modify the security groups for the <a href="#EC2">EC2</a> instances in the target groups behind the ALB to
		deny the malicious IP address.&quot; is incorrect as you cannot create deny rules with security
		groups.<br /><br />References:<br /><br />AWS WAF, AWS Firewall Manager, and AWS Shield Advanced > Developer
		Guide > What are AWS WAF, AWS Shield, and AWS Firewall Manager?<br /><br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 21<br />A web application is deployed in the AWS
		Cloud. It consists of a two&#8211;tier architecture that includes a web layer and a database layer. The web
		server is vulnerable to cross&#8211;site scripting (XSS) attacks.<br /><br />What should a solutions architect
		do to remediate the vulnerability?<br /><br />A. Create a Classic Load Balancer. Put the web layer behind the
		load balancer and enable AWS WAF.<br />B. Create a Network Load Balancer. Put the web layer behind the load
		balancer and enable AWS WAF.<br />C. Create an Application Load Balancer. Put the web layer behind the load
		balancer and enable AWS WAF.<br />D. Create an Application Load Balancer. Put the web layer behind the load
		balancer and use AWS Shield Standard.<br /><br /><b>Correct Answer:</b><br />C. Create an Application Load
		Balancer. Put the web layer behind the load balancer and enable AWS WAF.<br /><br />Answer
		Description:<br />Working with cross&#8211;site scripting match conditions: Attackers sometimes insert scripts
		into web requests in an effort to exploit vulnerabilities in web applications. You can create one or more
		cross&#8211;site scripting match conditions to identify the parts of web requests, such as the URI or the query
		string, that you want AWS WAF Classic to inspect for possible malicious scripts. Later in the process, when you
		create a web ACL, you specify whether to allow or block requests that appear to contain malicious
		scripts.<br />Web Application Firewall: You can now use AWS WAF to protect your web applications on your
		Application Load Balancers. AWS WAF is a web application firewall that helps protect your web applications from
		common web exploits that could affect application availability, compromise security, or consume excessive
		resources.<br /><br />The AWS Web Application Firewall (WAF) is available on the Application Load Balancer
		(ALB). You can use AWS WAF directly on Application Load Balancers (both internal and external) in a <a
			href="#VPC">VPC</a>, to protect your websites and web services.<br /><br />Attackers sometimes insert
		scripts into web requests in an effort to exploit vulnerabilities in web applications. You can create one or
		more cross&#8211;site scripting match conditions to identify the parts of web requests, such as the URI or the
		query string, that you want AWS WAF to inspect for possible malicious scripts.<br /><br />CORRECT: &quot;Create
		an Application Load Balancer. Put the web layer behind the load balancer and enable AWS WAF&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Create a Classic Load Balancer. Put the web layer behind the load balancer
		and enable AWS WAF&quot; is incorrect as you cannot use AWS WAF with a classic load
		balancer.<br /><br />INCORRECT: &quot;Create a Network Load Balancer. Put the web layer behind the load balancer
		and enable AWS WAF&quot; is incorrect as you cannot use AWS WAF with a network load
		balancer.<br /><br />INCORRECT: &quot;Create an Application Load Balancer. Put the web layer behind the load
		balancer and use AWS Shield Standard&quot; is incorrect as you cannot use AWS Shield to protect against XSS
		attacks. Shield is used to protect against DDoS attacks.<br /><br />References:<br /><br />AWS WAF, AWS Firewall
		Manager, and AWS Shield Advanced > Developer Guide > Working with cross&#8211;site scripting match conditions
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 22<br />A company&aposs website is using an Amazon
		RDS MySQL Multi&#8211;AZ DB instance for its transactional data storage. There are other internal systems that
		query this DB instance to fetch data for internal batch processing. The RDS DB instance slows down significantly
		when the internal systems fetch data. This impacts the website&aposs read and write performance, and the users
		experience slow response times.<br /><br />Which solution will improve the website&aposs
		performance?<br /><br />A. Use an RDS PostgreSQL DB instance instead of a MySQL database.<br />B. Use Amazon
		ElastiCache to cache the query responses for the website.<br />C. Add an additional Availability Zone to the
		current RDS MySQL Multi&#8211;AZ DB instance.<br />D. Add a read replica to the RDS DB instance and configure
		the internal systems to query the read replica.<br /><br /><b>Correct Answer:</b><br />D. Add a read replica to
		the RDS DB instance and configure the internal systems to query the read replica.<br /><br />Answer
		Description:<br />Amazon RDS Read Replicas<br />Enhanced performance<br />You can reduce the load on your source
		DB instance by routing read queries from your applications to the read replica. Read replicas allow you to
		elastically scale out beyond the capacity constraints of a single DB instance for read&#8211;heavy database
		workloads. Because read replicas can be promoted to master status, they are useful as part of a sharding
		implementation.<br /><br />To further maximize read performance, Amazon RDS for MySQL allows you to add table
		indexes directly to Read Replicas, without those indexes being present on the master.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 23<br />A financial services company has a web
		application that serves users in the United States and Europe. The application consists of a database tier and a
		web server tier. The database tier consists of a MySQL database hosted in us&#8211;east&#8211;1. Amazon Route 53
		geoproximity routing is used to direct traffic to instances in the closest Region. A performance review of the
		system reveals that European users are not receiving the same level of query performance as those in the United
		States.<br /><br />Which changes should be made to the database tier to improve performance?<br /><br />A.
		Migrate the database to Amazon RDS for MySQL. Configure Multi&#8211;AZ in one of the European Regions.<br />B.
		Migrate the database to Amazon DynamoDB. Use DynamoDB global tables to enable replication to additional
		Regions.<br />C. Deploy MySQL instances in each Region. Deploy an Application Load Balancer in front of MySQL to
		reduce the load on the primary instance.<br />D. Migrate the database to an Amazon Aurora global database in
		MySQL compatibility mode. Configure read replicas in one of the European Regions.<br /><br /><b>Correct
			Answer:</b><br />D. Migrate the database to an Amazon Aurora global database in MySQL compatibility mode.
		Configure read replicas in one of the European Regions.<br /><br />Answer Description:<br />The issue here is
		latency with read queries being directed from Australia to UK which is great physical distance. A solution is
		required for improving read performance in Australia.<br /><br />An Aurora global database consists of one
		primary AWS Region where your data is mastered, and up to five read&#8211;only, secondary AWS
		Regions.<br /><br />Aurora replicates data to the secondary AWS Regions with typical latency of under a second.
		You issue write operations directly to the primary DB instance in the primary AWS Region.<br /><br />This
		solution will provide better performance for users in the Australia Region for queries. Writes must still take
		place in the UK Region but read performance will be greatly improved.<br /><br />CORRECT: &quot;Migrate the
		database to an Amazon Aurora global database in MySQL compatibility mode. Configure read replicas in
		ap&#8211;southeast&#8211;2&quot; is the correct answer.<br /><br />INCORRECT: &quot;Migrate the database to
		Amazon RDS for MySQL. Configure Multi&#8211;AZ in the Australian Region&quot; is incorrect. The database is
		located in UK. If the database is migrated to Australia then the reverse problem will occur. Multi&#8211;AZ does
		not assist with improving query performance across Regions.<br /><br />INCORRECT: &quot;Migrate the database to
		Amazon DynamoDB. Use DynamoDB global tables to enable replication to additional Regions&quot; is incorrect as a
		relational database running on MySQL is unlikely to be compatible with DynamoDB.<br /><br />INCORRECT:
		&quot;Deploy MySQL instances in each Region. Deploy an Application Load Balancer in front of MySQL to reduce the
		load on the primary instance&quot; is incorrect as you can only put ALBs in front of the web tier, not the DB
		tier.<br /><br />References:<br /><br />Amazon Aurora > User Guide for Aurora > Using Amazon Aurora global
		databases<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 24<br />A company is performing an AWS
		Well&#8211;Architected Framework review of an existing workload deployed on AWS. The review identified a
		public&#8211;facing website running on the same Amazon <a href="#EC2">EC2</a> instance as a Microsoft Active
		Directory domain controller that was installed recently to support other AWS services. A solutions architect
		needs to recommend a new design that would improve the security of the architecture and minimize the
		administrative demand on IT staff.<br /><br />What should the solutions architect recommend?<br /><br />A. Use
		AWS Directory Service to create a managed Active Directory. Uninstall Active Directory on the current <a
			href="#EC2">EC2</a> instance.<br />B. Create another <a href="#EC2">EC2</a> instance in the same subnet and
		reinstall Active Directory on it. Uninstall Active Directory.<br />C. Use AWS Directory Service to create an
		Active Directory connector. Proxy Active Directory requests to the Active domain controller running on the
		current <a href="#EC2">EC2</a> instance.<br />D. Enable AWS Single Sign&#8211;On (AWS SSO) with Security
		Assertion Markup Language (SAML) 2.0 federation with the current Active Directory controller. Modify the <a
			href="#EC2">EC2</a> instance&aposs security group to deny public access to Active
		Directory.<br /><br /><b>Correct Answer:</b><br />A. Use AWS Directory Service to create a managed Active
		Directory. Uninstall Active Directory on the current <a href="#EC2">EC2</a> instance.<br /><br />Answer
		Description:<br />AWS Managed Microsoft AD: AWS Directory Service lets you run Microsoft Active Directory (AD)
		as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed
		Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is
		created as a highly available pair of domain controllers connected to your virtual private cloud (<a
			href="#VPC">VPC</a>). The domain controllers run in different Availability Zones in a region of your choice.
		Host monitoring and recovery, data replication, snapshots, and software updates are automatically configured and
		managed for you.<br /><br />Migrate AD to AWS Managed AD and keep the webserver alone. Reduce risk = remove AD
		from that <a href="#EC2">EC2</a>. Minimize admin = remove AD from any <a href="#EC2">EC2</a><br /><br />&#8211;>
		use AWS Directory Service<br /><br />Active Directory connector is only for ON&#8211;PREM AD. The one they have
		exists in the cloud already.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 25<br />A company hosts a static website within an
		Amazon <a href="#S3">S3</a> bucket. A solutions architect needs to ensure that data can be recovered in case of
		accidental deletion.<br /><br />Which action will accomplish this?<br /><br />A. Enable Amazon <a
			href="#S3">S3</a> versioning.<br />B. Enable Amazon <a href="#S3">S3</a> Intelligent&#8211;Tiering.<br />C.
		Enable an Amazon <a href="#S3">S3</a> lifecycle policy.<br />D. Enable Amazon <a href="#S3">S3</a>
		cross&#8211;Region replication.<br /><br /><b>Correct Answer:</b><br />A. Enable Amazon <a href="#S3">S3</a>
		versioning.<br /><br />Answer Description:<br />Data can be recover if versioning enable, also it provide a
		extra protection like file delete, MFA delete. MFA. Delete only works for CLI or API interaction, not in the AWS
		Management Console. Also, you cannot make version DELETE actions with MFA using <a href="#IAM">IAM</a> user
		credentials. You must use your root AWS account.<br /><br />Object Versioning: Use Amazon <a href="#S3">S3</a>
		Versioning to keep multiple versions of an object in one bucket. For example, you could store my&#8211;image.jpg
		(version 111111) and my&#8211;image.jpg (version 222222) in a single bucket. <a href="#S3">S3</a> Versioning
		protects you from the consequences of unintended overwrites and deletions. You can also use it to archive
		objects so that you have access to previous versions.<br /><br />You must explicitly enable <a href="#S3">S3</a>
		Versioning on your bucket. By default, <a href="#S3">S3</a> Versioning is disabled. Regardless of whether you
		have enabled Versioning, each object in your bucket has a version ID. If you have not enabled Versioning, Amazon
		<a href="#S3">S3</a> sets the value of the version ID to null. If <a href="#S3">S3</a> Versioning is enabled,
		Amazon <a href="#S3">S3</a> assigns a version ID value for the object. This value distinguishes it from other
		versions of the same key.<br /><br />Object versioning is a means of keeping multiple variants of an object in
		the same Amazon <a href="#S3">S3</a> bucket. Versioning provides the ability to recover from both unintended
		user actions and application failures. You can use versioning to preserve, retrieve, and restore every version
		of every object stored in your Amazon <a href="#S3">S3</a> bucket.<br /><br />CORRECT: &quot;Enable Amazon <a
			href="#S3">S3</a> versioning&quot; is the correct answer.<br /><br />INCORRECT: &quot;Enable Amazon <a
			href="#S3">S3</a> Intelligent&#8211;Tiering&quot; is incorrect. This is a storage class that automatically
		moves data between frequent access and infrequent access classes based on usage patterns.<br /><br />INCORRECT:
		&quot;Enable an Amazon <a href="#S3">S3</a> lifecycle policy&quot; is incorrect. An <a href="#S3">S3</a>
		lifecycle policy is a set of rules that define actions that apply to groups of <a href="#S3">S3</a> objects such
		as transitioning objects to another storage class.<br /><br />INCORRECT: &quot;Enable Amazon <a
			href="#S3">S3</a> cross&#8211;Region replication&quot; is incorrect as this is used to copy objects to
		different regions. CRR relies on versioning which is the feature that is required for protecting against
		accidental deletion.<br /><br />References:<br /><br />Protecting Amazon <a href="#S3">S3</a> Against Object
		Deletion</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 26<br />A company&aposs production application
		runs online transaction processing (OLTP) transactions on an Amazon RDS MySQL DB instance. The company is
		launching a new reporting tool that will access the same data.<br /><br />The reporting tool must be highly
		available and not impact the performance of the production application.<br /><br />How can this be
		achieved?<br /><br />A. Create hourly snapshots of the production RDS DB instance.<br />B. Create a
		Multi&#8211;AZ RDS Read Replica of the production RDS DB instance.<br />C. Create multiple RDS Read Replicas of
		the production RDS DB instance. Place the Read Replicas in an Auto Scaling group.<br />D. Create a
		Single&#8211;AZ RDS Read Replica of the production RDS DB instance. Create a second Single&#8211;AZ RDS Read
		Replica from the replica.<br /><br /><b>Correct Answer:</b><br />B. Create a Multi&#8211;AZ RDS Read Replica of
		the production RDS DB instance.<br /><br />Answer Description:<br />Amazon RDS Read Replicas Now Support
		Multi&#8211;AZ Deployments<br /><br />Amazon RDS Read Replicas enable you to create one or more read&#8211;only
		copies of your database instance within the same AWS Region or in a different AWS Region. Updates made to the
		source database are then asynchronously copied to your Read Replicas. In addition to providing scalability for
		read&#8211;heavy workloads, Read Replicas can be promoted to become a standalone database instance when
		needed.<br /><br />Amazon RDS Multi&#8211;AZ deployments provide enhanced availability for database instances
		within a single AWS Region. With Multi&#8211;AZ, your data is synchronously replicated to a standby in a
		different Availability Zone (AZ). In the event of an infrastructure failure, Amazon RDS performs an automatic
		failover to the standby, minimizing disruption to your applications.<br /><br />You can now use Read Replicas
		with Multi&#8211;AZ as part of a disaster recovery (DR) strategy for your production databases. A
		well&#8211;designed and tested DR plan is critical for maintaining business continuity after a disaster. A Read
		Replica in a different region than the source database can be used as a standby database and promoted to become
		the new production database in case of a regional disruption.<br /><br />You can create a read replica as a
		Multi&#8211;AZ DB instance. Amazon RDS creates a standby of your replica in another Availability Zone for
		failover support for the replica. Creating your read replica as a Multi&#8211;AZ DB instance is independent of
		whether the source database is a Multi&#8211;AZ DB instance.<br /><br />CORRECT: &quot;Create a Multi&#8211;AZ
		RDS Read Replica of the production RDS DB instance&quot; is the correct answer.<br /><br />INCORRECT:
		&quot;Create a Single&#8211;AZ RDS Read Replica of the production RDS DB instance. Create a second
		Single&#8211;AZ RDS Read Replica from the replica&quot; is incorrect. Read replicas are primarily used for
		horizontal scaling. The best solution for high availability is to use a Multi&#8211;AZ read
		replica.<br /><br />INCORRECT: &quot;Create a cross&#8211;region Multi&#8211;AZ deployment and create a read
		replica in the second region&quot; is incorrect as you cannot create a cross&#8211;region Multi&#8211;AZ
		deployment with RDS. INCORRECT: &quot;Use Amazon Data Lifecycle Manager to automatically create and manage
		snapshots&quot; is incorrect as using snapshots is not the best solution for high
		availability.<br /><br />References:<br /><br />Amazon Relational Database Service > User Guide > What is Amazon
		Relational Database Service (Amazon RDS)?</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 27<br />A company runs an application in a branch
		office within a small data closet with no virtualized compute resources. The application data is stored on an
		NFS volume. Compliance standards require a daily offsite backup of the NFS volume.<br /><br />Which solution
		meet these requirements?<br /><br />A. Install an AWS Storage Gateway file gateway on premises to replicate the
		data to Amazon <a href="#S3">S3</a>.<br />B. Install an AWS Storage Gateway file gateway hardware appliance on
		premises to replicate the data to Amazon <a href="#S3">S3</a>.<br />C. Install an AWS Storage Gateway volume
		gateway with stored volumes on premises to replicate the data to Amazon <a href="#S3">S3</a>.<br />D. Install an
		AWS Storage Gateway volume gateway with cached volumes on premises to replicate the data to Amazon <a
			href="#S3">S3</a>.<br /><br /><b>Correct Answer:</b><br />B. Install an AWS Storage Gateway file gateway
		hardware appliance on premises to replicate the data to Amazon <a href="#S3">S3</a>.<br /><br />Answer
		Description:<br />AWS Storage Gateway Hardware Appliance<br />Hardware Appliance: Storage Gateway is available
		as a hardware appliance, adding to the existing support for VMware ESXi, Microsoft Hyper&#8211;V, and Amazon <a
			href="#EC2">EC2</a>. This means that you can now make use of Storage Gateway in situations where you do not
		have a virtualized environment, server&#8211;class hardware or IT staff with the specialized skills that are
		needed to manage them. You can order appliances from Amazon.com for delivery to branch offices, warehouses, and
		&quot;outpost&quot; offices that lack dedicated IT resources. Setup (as you will see in a minute) is quick and
		easy, and gives you access to three storage solutions:<br /><br />File Gateway: A file interface to Amazon <a
			href="#S3">S3</a>, accessible via NFS or SMB. The files are stored as <a href="#S3">S3</a> objects, allowing
		you to make use of specialized <a href="#S3">S3</a> features such as lifecycle management and cross region
		replication. You can trigger AWS Lambda functions, run Amazon Athena queries, and use Amazon Macie to discover
		and classify sensitive data.<br /><br />Keyword: NFS + Compliance<br /><br />File gateway provides a virtual
		on&#8211;premises file server, which enables you to store and retrieve files as objects in Amazon <a
			href="#S3">S3</a>. It can be used for on&#8211;premises applications, and for Amazon <a
			href="#EC2">EC2</a>&#8211; resident applications that need file storage in <a href="#S3">S3</a> for object
		based workloads. Used for flat files only, stored directly on <a href="#S3">S3</a>. File gateway offers SMB or
		NFS&#8211;based access to data in Amazon <a href="#S3">S3</a> with local caching.<br /><br />WS Storage Gateway
		&#8212; File Gateway<br /><br />The table below shows the different gateways available and the interfaces and
		use cases:<br /><br />Storage Gateway Overview<br /><br />CORRECT: &quot;Install an AWS Storage Gateway file
		gateway hardware appliance on premises to replicate the data to Amazon <a href="#S3">S3</a>&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Install an AWS Storage Gateway file gateway on premises to replicate the
		data to Amazon <a href="#S3">S3</a>&quot; is incorrect.<br /><br />INCORRECT: &quot;Install an AWS Storage
		Gateway volume gateway with stored volumes on premises to replicate the data to Amazon <a
			href="#S3">S3</a>&quot; is incorrect as unsupported NFS. INCORRECT: &quot;Install an AWS Storage Gateway
		volume gateway with cached volumes on premises to replicate the data to Amazon <a href="#S3">S3</a>&quot; is
		incorrect as unsupported NFS.<br /><br />References:<br /><br />AWS News Blog > File Interface to AWS Storage
		Gateway</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 28<br />A company&aposs web application is using
		multiple Linux Amazon <a href="#EC2">EC2</a> instances and storing data on Amazon EBS volumes. The company is
		looking for a solution to increase the resiliency of the application in case of a failure and to provide storage
		that complies with atomicity, consistency, isolation, and durability (ACID).<br /><br />What should a solutions
		architect do to meet these requirements?<br /><br />A. Launch the application on <a href="#EC2">EC2</a>
		instances in each Availability Zone. Attach EBS volumes to each <a href="#EC2">EC2</a> instance.<br />B. Create
		an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Mount an instance
		store on each <a href="#EC2">EC2</a> instance.<br />C. Create an Application Load Balancer with Auto Scaling
		groups across multiple Availability Zones. Store data on Amazon EFS and mount a target on each instance.<br />D.
		Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data
		using Amazon <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA).<br /><br /><b>Correct Answer:</b><br />C. Create an Application Load Balancer with Auto Scaling
		groups across multiple Availability Zones. Store data on Amazon EFS and mount a target on each
		instance.<br /><br />Answer Description:<br />How Amazon EFS Works with Amazon <a href="#EC2">EC2</a><br />The
		following illustration shows an example <a href="#VPC">VPC</a> accessing an Amazon EFS file system. Here, <a
			href="#EC2">EC2</a> instances in the <a href="#VPC">VPC</a> have file systems mounted.<br /><br />In this
		illustration, the <a href="#VPC">VPC</a> has three Availability Zones, and each has one mount target created in
		it. We recommend that you access the file system from a mount target within the same Availability Zone. One of
		the Availability Zones has two subnets. However, a mount target is created in only one of the
		subnets.<br /><br /><br /><br />Benefits of Auto Scaling<br />Better fault tolerance. Amazon <a
			href="#EC2">EC2</a> Auto Scaling can detect when an instance is unhealthy, terminate it, and launch an
		instance to replace it. You can also configure Amazon <a href="#EC2">EC2</a> Auto Scaling to use multiple
		Availability Zones. If one Availability Zone becomes unavailable, Amazon <a href="#EC2">EC2</a> Auto Scaling can
		launch instances in another one to compensate.<br /><br />Better availability. Amazon <a href="#EC2">EC2</a>
		Auto Scaling helps ensure that your application always has the right amount of capacity to handle the current
		traffic demand.<br /><br />Better cost management. Amazon <a href="#EC2">EC2</a> Auto Scaling can dynamically
		increase and decrease capacity as needed. Because you pay for the <a href="#EC2">EC2</a> instances you use, you
		save money by launching instances when they are needed and terminating them when they aren&apost.<br /><br />To
		increase the resiliency of the application the solutions architect can use Auto Scaling groups to launch and
		terminate instances across multiple availability zones based on demand. An application load balancer (ALB) can
		be used to direct traffic to the web application running on the <a href="#EC2">EC2</a>
		instances.<br /><br />Lastly, the Amazon Elastic File System (EFS) can assist with increasing the resilience of
		the application by providing a shared file system that can be mounted by multiple <a href="#EC2">EC2</a>
		instances from multiple availability zones.<br /><br />CORRECT: &quot;Create an Application Load Balancer with
		Auto Scaling groups across multiple Availability Zones. Store data on Amazon EFS and mount a target on each
		instance&quot; is the correct answer.<br /><br />INCORRECT: &quot;Launch the application on <a
			href="#EC2">EC2</a> instances in each Availability Zone. Attach EBS volumes to each <a href="#EC2">EC2</a>
		instance&quot; is incorrect as the EBS volumes are single points of failure which are not shared with other
		instances.<br /><br />INCORRECT: &quot;Create an Application Load Balancer with Auto Scaling groups across
		multiple Availability Zones. Mount an instance store on each <a href="#EC2">EC2</a> instance&quot; is incorrect
		as instance stores are ephemeral data stores which means data is lost when powered down. Also, instance stores
		cannot be shared between instances.<br /><br />INCORRECT: &quot;Create an Application Load Balancer with Auto
		Scaling groups across multiple Availability Zones. Store data using Amazon <a href="#S3">S3</a> One
		Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One Zone&#8211;IA)&quot; is incorrect as there are data
		retrieval charges associated with this <a href="#S3">S3</a> tier. It is not a suitable storage tier for
		application files.<br /><br />References:<br /><br />Amazon Elastic File System Documentation<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 29<br />A data science team requires storage for
		nightly log processing. The size and number of logs is unknown and will persist for 24 hours
		only.<br /><br />What is the MOST cost&#8211;effective solution?<br /><br />A. Amazon <a href="#S3">S3</a>
		Glacier<br />B. Amazon <a href="#S3">S3</a> Standard<br />C. Amazon <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br />D. Amazon <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a
			href="#S3">S3</a> One Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />B. Amazon <a href="#S3">S3</a>
		Standard<br /><br />Answer Description:<br />The <a href="#S3">S3</a> Intelligent&#8211;Tiering storage class is
		designed to optimize costs by automatically moving data to the most cost&#8211;effective access tier, without
		performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is
		optimized for frequent access and another lower&#8211;cost tier that is optimized for infrequent access. This is
		an ideal use case for intelligent&#8211;tiering as the access patterns for the log files are not
		known.<br /><br />CORRECT: &quot;<a href="#S3">S3</a> Intelligent&#8211;Tiering&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;<a href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a>
		Standard&#8211;IA)&quot; is incorrect as if the data is accessed often retrieval fees could become
		expensive.<br /><br />INCORRECT: &quot;<a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a
			href="#S3">S3</a> One Zone&#8211;IA)&quot; is incorrect as if the data is accessed often retrieval fees
		could become expensive.<br /><br />INCORRECT: &quot;<a href="#S3">S3</a> Glacier&quot; is incorrect as if the
		data is accessed often retrieval fees could become expensive. Glacier also requires more work in retrieving the
		data from the archive and quick access requirements can add further
		costs.<br /><br />References:<br /><br />Unknown or changing access</div><a href="#All">All(200)</a> <a href="#"
		<i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 30<br />A company is hosting a web application on
		AWS using a single Amazon <a href="#EC2">EC2</a> instance that stores user uploaded documents in an Amazon EBS
		volume. For better scalability and availability, the company duplicated the architecture and created a second <a
			href="#EC2">EC2</a> instance and EBS volume in another Availability Zone, placing both behind an Application
		Load Balancer. After completing this change, users reported that each time they refreshed the website, they
		could see one subset of their documents or the other, but never all of the documents at the same
		time.<br /><br />What should a solutions architect propose to ensure users see all of their documents at
		once?<br /><br />A. Copy the data so both EBS volumes contain all the documents.<br />B. Configure the
		Application Load Balancer to direct a user to the server with the documents.<br />C. Copy the data from both EBS
		volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS.<br />D. Configure the
		Application Load Balancer to send the request to both servers. Return each document from the correct
		server.<br /><br /><b>Correct Answer:</b><br />C. Copy the data from both EBS volumes to Amazon EFS. Modify the
		application to save new documents to Amazon EFS.<br /><br />Answer Description:<br />Amazon EFS provides file
		storage in the AWS Cloud. With Amazon EFS, you can create a file system, mount the file system on an Amazon <a
			href="#EC2">EC2</a> instance, and then read and write data to and from your file system. You can mount an
		Amazon EFS file system in your <a href="#VPC">VPC</a>, through the Network File System versions 4.0 and 4.1
		(NFSv4) protocol. We recommend using a current generation Linux NFSv4.1 client, such as those found in the
		latest Amazon Linux, Redhat, and Ubuntu AMIs, in conjunction with the Amazon EFS Mount Helper. For instructions,
		see Using the amazon&#8211;efs&#8211;utils Tools.<br /><br />For a list of Amazon <a href="#EC2">EC2</a> Linux
		Amazon Machine Images (AMIs) that support this protocol, see NFS Support. For some AMIs, you&aposll need to
		install an NFS client to mount your file system on your Amazon <a href="#EC2">EC2</a> instance. For
		instructions, see Installing the NFS Client.<br /><br />You can access your Amazon EFS file system concurrently
		from multiple NFS clients, so applications that scale beyond a single connection can access a file system.
		Amazon <a href="#EC2">EC2</a> instances running in multiple Availability Zones within the same AWS Region can
		access the file system, so that many users can access and share a common data source.<br /><br />How Amazon EFS
		Works with Amazon <a href="#EC2">EC2</a></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 31<br />A company is planning to use Amazon <a
			href="#S3">S3</a> to store images uploaded by its users. The images must be encrypted at rest in Amazon <a
			href="#S3">S3</a>. The company does not want to spend time managing and rotating the keys, but it does want
		to control who can access those keys.<br /><br />What should a solutions architect use to accomplish
		this?<br /><br />A. Server&#8211;Side Encryption with keys stored in an <a href="#S3">S3</a> bucket<br />B.
		Server&#8211;Side Encryption with Customer&#8211;Provided Keys (SSE&#8211;C)<br />C. Server&#8211;Side
		Encryption with Amazon <a href="#S3">S3</a>&#8211;Managed Keys (SSE&#8211;<a href="#S3">S3</a>)<br />D.
		Server&#8211;Side Encryption with AWS KMS&#8211;Managed Keys (SSE&#8211;KMS)<br /><br /><b>Correct
			Answer:</b><br />D. Server&#8211;Side Encryption with AWS KMS&#8211;Managed Keys
		(SSE&#8211;KMS)<br /><br />Answer Description:<br />&quot;Server&#8211;Side Encryption with Customer Master Keys
		(CMKs) Stored in AWS Key Management Service (SSE&#8211;KMS) is similar to SSE&#8211;<a href="#S3">S3</a>, but
		with some additional benefits and charges for using this service.<br /><br />There are separate permissions for
		the use of a CMK that provides added protection against unauthorized access of your objects in Amazon <a
			href="#S3">S3</a>. SSE&#8211;KMS also provides you with an audit trail that shows when your CMK was used and
		by whom.&quot;<br /><br />Server&#8211;Side Encryption: Using SSE&#8211;KMS<br />You can protect data at rest in
		Amazon <a href="#S3">S3</a> by using three different modes of server&#8211;side encryption: SSE<a
			href="#S3">S3</a>, SSE&#8211;C, or SSE&#8211;KMS.<br />SSE&#8211;<a href="#S3">S3</a> requires that Amazon
		<a href="#S3">S3</a> manage the data and master encryption keys. For more information about SSE&#8211;<a
			href="#S3">S3</a>, see Protecting Data Using Server&#8211;Side Encryption with Amazon <a
			href="#S3">S3</a>&#8211;Managed Encryption Keys (SSE&#8211;<a href="#S3">S3</a>).<br /><br />SSE&#8211;C
		requires that you manage the encryption key. For more information about SSE&#8211;C, see Protecting Data Using
		Server&#8211;Side Encryption with Customer&#8211;Provided Encryption Keys
		(SSE&#8211;C).<br /><br />SSE&#8211;KMS requires that AWS manage the data key but you manage the customer master
		key (CMK) in AWS KMS.<br /><br />The remainder of this topic discusses how to protect data by using
		server&#8211;side encryption with AWS KMS&#8211;managed keys (SSE&#8211;KMS).<br /><br />You can request
		encryption and select a CMK by using the Amazon <a href="#S3">S3</a> console or API. In the console, check the
		appropriate box to perform encryption and select your CMK from the list. For the Amazon <a href="#S3">S3</a>
		API, specify encryption and choose your CMK by setting the appropriate headers in a GET or PUT
		request.<br /><br />SSE&#8211;KMS requires that AWS manage the data key but you manage the customer master key
		(CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon <a href="#S3">S3</a>
		in your account.<br /><br />Customer managed CMKs are CMKs in your AWS account that you create, own, and manage.
		You have full control over these CMKs, including establishing and maintaining their key policies, <a
			href="#IAM">IAM</a> policies, and grants, enabling and disabling them, rotating their cryptographic
		material, adding tags, creating aliases that refer to the CMK, and scheduling the CMKs for
		deletion.<br /><br />For this scenario, the solutions architect should use SSE&#8211;KMS with a customer managed
		CMK. That way KMS will manage the data key but the company can configure key policies defining who can access
		the keys.<br /><br />CORRECT: &quot;Server&#8211;Side Encryption with AWS KMS&#8211;Managed Keys
		(SSE&#8211;KMS)&quot; is the correct answer.<br /><br />INCORRECT: &quot;Server&#8211;Side Encryption with keys
		stored in an <a href="#S3">S3</a> bucket&quot; is incorrect as you cannot store your keys in a bucket with
		server&#8211;side encryption<br /><br />INCORRECT: &quot;Server&#8211;Side Encryption with
		Customer&#8211;Provided Keys (SSE&#8211;C)&quot; is incorrect as the company does not want to manage the
		keys.<br /><br />INCORRECT: &quot;Server&#8211;Side Encryption with Amazon <a href="#S3">S3</a>&#8211;Managed
		Keys (SSE&#8211;<a href="#S3">S3</a>)&quot; is incorrect as the company needs to manage access control for the
		keys which is not possible when they&aposre managed by Amazon.<br /><br />References:<br /><br />AWS Key
		Management Service > Developer Guide > Server&#8211;Side Encryption: Using SSE&#8211;KMS<br />AWS Key Management
		Service > Developer Guide > AWS KMS keys concepts<br /><br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 32<br />A solutions architect is tasked with
		transferring 750 TB of data from a network&#8211;attached file system located at a branch office Amazon <a
			href="#S3">S3</a> Glacier. The solution must avoid saturating the branch office&aposs low&#8211;bandwidth
		internet connection.<br /><br />What is the MOST cost&#8211;effective solution?<br /><br />A. Create a
		site&#8211;to&#8211;site VPN tunnel to an Amazon <a href="#S3">S3</a> bucket and transfer the files directly.
		Create a bucket policy to enforce a <a href="#VPC">VPC</a> endpoint.<br />B. Order 10 AWS Snowball appliances
		and select an <a href="#S3">S3</a> Glacier vault as the destination. Create a bucket policy to enforce a <a
			href="#VPC">VPC</a> endpoint.<br />C. Mount the network&#8211;attached file system to Amazon <a
			href="#S3">S3</a> and copy the files directly. Create a lifecycle policy to transition the <a
			href="#S3">S3</a> objects to Amazon <a href="#S3">S3</a> Glacier.<br />D. Order 10 AWS Snowball appliances
		and select an Amazon <a href="#S3">S3</a> bucket as the destination. Create a lifecycle policy to transition the
		<a href="#S3">S3</a> objects to Amazon <a href="#S3">S3</a> Glacier.<br /><br /><b>Correct Answer:</b><br />D.
		Order 10 AWS Snowball appliances and select an Amazon <a href="#S3">S3</a> bucket as the destination. Create a
		lifecycle policy to transition the <a href="#S3">S3</a> objects to Amazon <a href="#S3">S3</a>
		Glacier.<br /><br />Answer Description:<br />Regional Limitations for AWS Snowball<br />The AWS Snowball service
		has two device types, the standard Snowball and the Snowball Edge. The following table highlights which of these
		devices are available in which regions.<br /><br />The following table highlights which of these devices are
		available in which regions.<br /><br />The following table highlights which of these devices are available in
		which regions.<br /><br />Limitations on Jobs in AWS Snowball<br /><br />The following limitations exist for
		creating jobs in AWS Snowball:<br /><br />For security purposes, data transfers must be completed within 90 days
		of the Snowball being prepared.<br /><br />Currently, AWS Snowball Edge device doesn&apost support
		server&#8211;side encryption with customer&#8211;provided keys (SSE&#8211;C). AWS Snowball Edge device does
		support server&#8211;side encryption with Amazon <a href="#S3">S3</a>&#8212;managed encryption keys
		(SSE&#8211;<a href="#S3">S3</a>) and server&#8211;side encryption with AWS Key Management Service &#8212;
		managed keys (SSE&#8211;KMS). For more information, see Protecting Data Using Server&#8211;Side Encryption in
		the Amazon Simple Storage Service Developer Guide.<br /><br />In the US regions, Snowballs come in two sizes: 50
		TB and 80 TB. <a href="#All">All</a> other regions have the 80 TB Snowballs only. If you&aposre using Snowball
		to import data, and you need to transfer more data than will fit on a single Snowball, create additional jobs.
		Each export job can use multiple Snowballs.<br /><br />The default service limit for the number of Snowballs you
		can have at one time is 1. If you want to increase your service limit, contact AWS Support.<br /><br /><a
			href="#All">All</a> objects transferred to the Snowball have their metadata changed. The only metadata that
		remains the same is filename and filesize. <a href="#All">All</a> other metadata is set as in the following
		example: &#8211;rw&#8211;rw&#8211;r&#8212; 1 root root [filesize] Dec 31 1969 [path/filename].<br /><br />Object
		lifecycle management<br />To manage your objects so that they are stored cost effectively throughout their
		lifecycle, configure their Amazon <a href="#S3">S3</a> Lifecycle. An <a href="#S3">S3</a> Lifecycle
		configuration is a set of rules that define actions that Amazon <a href="#S3">S3</a> applies to a group of
		objects. There are two types of actions:<br /><br />Transition actions &#8212; Define when objects transition to
		another storage class. For example, you might choose to transition objects to the <a href="#S3">S3</a>
		Standard&#8211;IA storage class 30 days after you created them, or archive objects to the <a href="#S3">S3</a>
		Glacier storage class one year after creating them.<br /><br />Expiration actions &#8212; Define when objects
		expire. Amazon <a href="#S3">S3</a> deletes expired objects on your behalf. The lifecycle expiration costs
		depend on when you choose to expire objects.<br /><br />As the company&aposs internet link is
		low&#8211;bandwidth uploading directly to Amazon <a href="#S3">S3</a> (ready for transition to Glacier) would
		saturate the link. The best alternative is to use AWS Snowball appliances. The Snowball Edge appliance can hold
		up to 75 TB of data so 10 devices would be required to migrate 750 TB of data.<br /><br />Snowball moves data
		into AWS using a hardware device and the data is then copied into an Amazon <a href="#S3">S3</a> bucket of your
		choice. From there, lifecycle policies can transition the <a href="#S3">S3</a> objects to Amazon <a
			href="#S3">S3</a> Glacier.<br /><br />CORRECT: &quot;Order 10 AWS Snowball appliances and select an Amazon
		<a href="#S3">S3</a> bucket as the destination. Create a lifecycle policy to transition the <a href="#S3">S3</a>
		objects to Amazon <a href="#S3">S3</a> Glacier&quot; is the correct answer.<br /><br />INCORRECT: &quot;Order 10
		AWS Snowball appliances and select an <a href="#S3">S3</a> Glacier vault as the destination. Create a bucket
		policy to enforce a <a href="#VPC">VPC</a> endpoint&quot; is incorrect as you cannot set a Glacier vault as the
		destination, it must be an <a href="#S3">S3</a> bucket. You also can&apost enforce a <a href="#VPC">VPC</a>
		endpoint using a bucket policy.<br /><br />INCORRECT: &quot;Create an AWS Direct Connect connection and migrate
		the data straight into Amazon Glacier&quot; is incorrect as this is not the most cost&#8211;effective option and
		takes time to setup. INCORRECT: &quot;Use AWS Global Accelerator to accelerate upload and optimize usage of the
		available bandwidth&quot; is incorrect as this service is not used for accelerating or optimizing the upload of
		data from on&#8211;premises networks.<br /><br />References:<br /><br />AWS Snowball Edge Developer Guide > AWS
		Snowball Edge Specifications</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 33<br />An application hosted on AWS is
		experiencing performance problems, and the application vendor wants to perform an analysis of the log file to
		troubleshoot further. The log file is stored on Amazon <a href="#S3">S3</a> and is 10 GB in size. The
		application owner will make the log file available to the vendor for a limited time.<br /><br />What is the MOST
		secure way to do this?<br /><br />A. Enable public read on the <a href="#S3">S3</a> object and provide the link
		to the vendor.<br />B. Upload the file to Amazon WorkDocs and share the public link with the vendor.<br />C.
		Generate a presigned URL and have the vendor download the log file before it expires.<br />D. Create an <a
			href="#IAM">IAM</a> user for the vendor to provide access to the <a href="#S3">S3</a> bucket and the
		application. Enforce multi&#8211;factor authentication.<br /><br /><b>Correct Answer:</b><br />C. Generate a
		presigned URL and have the vendor download the log file before it expires.<br /><br />Answer
		Description:<br />Share an object with others<br /><a href="#All">All</a> objects by default are private. Only
		the object owner has permission to access these objects. However, the object owner can optionally share objects
		with others by creating a presigned URL, using their own security credentials, to grant time&#8211;limited
		permission to download the objects.<br /><br />When you create a presigned URL for your object, you must provide
		your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the
		object) and expiration date and time. The presigned URLs are valid only for the specified
		duration.<br /><br />Anyone who receives the presigned URL can then access the object. For example, if you have
		a video in your bucket and both the bucket and the object are private, you can share the video with others by
		generating a presigned URL.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 34<br />A company allows its developers to attach
		existing <a href="#IAM">IAM</a> policies to existing <a href="#IAM">IAM</a> roles to enable faster
		experimentation and agility. However, the security operations team is concerned that the developers could attach
		the existing administrator policy, which would allow the developers to circumvent any other security
		policies.<br /><br />How should a solutions architect address this issue?<br /><br />A. Create an Amazon SNS
		topic to send an alert every time a developer creates a new policy.<br />B. Use service control policies to
		disable <a href="#IAM">IAM</a> activity across all account in the organizational unit.<br />C. Prevent the
		developers from attaching any policies and assign all <a href="#IAM">IAM</a> duties to the security operations
		team.<br />D. Set an <a href="#IAM">IAM</a> permissions boundary on the developer <a href="#IAM">IAM</a> role
		that explicitly denies attaching the administrator policy.<br /><br /><b>Correct Answer:</b><br />D. Set an <a
			href="#IAM">IAM</a> permissions boundary on the developer <a href="#IAM">IAM</a> role that explicitly denies
		attaching the administrator policy.<br /><br />Answer Description:<br />The permissions boundary for an <a
			href="#IAM">IAM</a> entity (user or role) sets the maximum permissions that the entity can have. This can
		change the effective permissions for that user or role. The effective permissions for an entity are the
		permissions that are granted by all the policies that affect the user or role. Within an account, the
		permissions for an entity can be affected by identity&#8211;based policies, resource&#8211;based policies,
		permissions boundaries, Organizations SCPs, or session policies.<br /><br />Therefore, the solutions architect
		can set an <a href="#IAM">IAM</a> permissions boundary on the developer <a href="#IAM">IAM</a> role that
		explicitly denies attaching the administrator policy.<br /><br />CORRECT: &quot;Set an <a href="#IAM">IAM</a>
		permissions boundary on the developer <a href="#IAM">IAM</a> role that explicitly denies attaching the
		administrator policy&quot; is the correct answer.<br /><br />INCORRECT: &quot;Create an Amazon SNS topic to send
		an alert every time a developer creates a new policy&quot; is incorrect as this would mean investigating every
		incident which is not an efficient solution.<br /><br />INCORRECT: &quot;Use service control policies to disable
		<a href="#IAM">IAM</a> activity across all accounts in the organizational unit&quot; is incorrect as this would
		prevent the developers from being able to work with <a href="#IAM">IAM</a> completely.<br /><br />INCORRECT:
		&quot;Prevent the developers from attaching any policies and assign all <a href="#IAM">IAM</a> duties to the
		security operations team&quot; is incorrect as this is not necessary. The requirement is to allow developers to
		work with policies, the solution needs to find a secure way of achieving this.<br /><br />References:<br />AWS
		Identity and Access Management > User Guide > Permissions boundaries for <a href="#IAM">IAM</a> entities<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 35<br />A company has a multi&#8211;tier
		application that runs six front&#8211;end web servers in an Amazon <a href="#EC2">EC2</a> Auto Scaling group in
		a single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs to modify the
		infrastructure to be highly available without modifying the application.<br /><br />Which architecture should
		the solutions architect choose that provides high availability?<br /><br />A. Create an Auto Scaling group that
		uses three instances across each of two Regions.<br />B. Modify the Auto Scaling group to use three instances
		across each of two Availability Zones.<br />C. Create an Auto Scaling template that can be used to quickly
		create more instances in another Region.<br />D. Change the ALB in front of the Amazon <a href="#EC2">EC2</a>
		instances in a round&#8211;robin configuration to balance traffic to the web tier.<br /><br /><b>Correct
			Answer:</b><br />B. Modify the Auto Scaling group to use three instances across each of two Availability
		Zones.<br /><br />Answer Description:<br />Expanding Your Scaled and Load&#8211;Balanced Application to an
		Additional Availability Zone.<br /><br />When one Availability Zone becomes unhealthy or unavailable, Amazon <a
			href="#EC2">EC2</a> Auto Scaling launches new instances in an unaffected zone. When the unhealthy
		Availability Zone returns to a healthy state, Amazon <a href="#EC2">EC2</a> Auto Scaling automatically
		redistributes the application instances evenly across all of the zones for your Auto Scaling group. Amazon <a
			href="#EC2">EC2</a> Auto Scaling does this by attempting to launch new instances in the Availability Zone
		with the fewest instances. If the attempt fails, however, Amazon <a href="#EC2">EC2</a> Auto Scaling attempts to
		launch in other Availability Zones until it succeeds.<br /><br />You can expand the availability of your scaled
		and load&#8211;balanced application by adding an Availability Zone to your Auto Scaling group and then enabling
		that zone for your load balancer. After you&aposve enabled the new Availability Zone, the load balancer begins
		to route traffic equally among all the enabled zones.<br /><br />High availability can be enabled for this
		architecture quite simply by modifying the existing Auto Scaling group to use multiple availability zones. The
		ASG will automatically balance the load so you don&apost actually need to specify the instances per
		AZ.<br /><br />The architecture for the web tier will look like the one below:<br /><br />CORRECT: &quot;Modify
		the Auto Scaling group to use four instances across each of two Availability Zones&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Create an Auto Scaling group that uses four instances across each of two
		Regions&quot; is incorrect as <a href="#EC2">EC2</a> Auto Scaling does not support multiple
		regions.<br /><br />INCORRECT: &quot;Create an Auto Scaling template that can be used to quickly create more
		instances in another Region&quot; is incorrect as <a href="#EC2">EC2</a> Auto Scaling does not support multiple
		regions. INCORRECT: &quot;Create an Auto Scaling group that uses four instances across each of two subnets&quot;
		is incorrect as the subnets could be in the same AZ.<br /><br />References:<br /><br />Amazon <a
			href="#EC2">EC2</a> Auto Scaling</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 36<br />A company runs an application on a group
		of Amazon Linux <a href="#EC2">EC2</a> instances. The application writes log files using standard API calls. For
		compliance reasons, all log files must be retained indefinitely and will be analyzed by a reporting tool that
		must access all files concurrently.<br /><br />Which storage service should a solutions architect use to provide
		the MOST cost&#8211;effective solution?<br /><br />A. Amazon EBS<br />B. Amazon EFS<br />C. Amazon <a
			href="#EC2">EC2</a> instance store<br />D. Amazon <a href="#S3">S3</a><br /><br /><b>Correct
			Answer:</b><br />D. Amazon <a href="#S3">S3</a><br /><br />Answer Description:<br />Amazon <a
			href="#S3">S3</a>: Requests to Amazon <a href="#S3">S3</a> can be authenticated or anonymous. Authenticated
		access requires credentials that AWS can use to authenticate your requests. When making REST API calls directly
		from your code, you create a signature using valid credentials and include the signature in your request. Amazon
		Simple Storage Service (Amazon <a href="#S3">S3</a>) is an object storage service that offers
		industry&#8211;leading scalability, data availability, security, and performance. This means customers of all
		sizes and industries can use it to store and protect any amount of data for a range of use cases, such as
		websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data
		analytics. Amazon <a href="#S3">S3</a> provides easy&#8211;to&#8211;use management features so you can organize
		your data and configure finely&#8211;tuned access controls to meet your specific business, organizational, and
		compliance requirements. Amazon <a href="#S3">S3</a> is designed for 99.999999999% (11 9&aposs) of durability,
		and stores data for millions of applications for companies all around the world.<br /><br />The application is
		writing the files using API calls which means it will be compatible with Amazon <a href="#S3">S3</a> which uses
		a REST API. <a href="#S3">S3</a> is a massively scalable key&#8211;based object store that is well&#8211;suited
		to allowing concurrent access to the files from many instances.<br /><br />Amazon <a href="#S3">S3</a> will also
		be the most cost&#8211;effective choice. A rough calculation using the AWS pricing calculator shows the cost
		differences between 1TB of storage on EBS, EFS, and <a href="#S3">S3</a> Standard.<br /><br />CORRECT:
		&quot;Amazon <a href="#S3">S3</a>&quot; is the correct answer.<br /><br />INCORRECT: &quot;Amazon EFS&quot; is
		incorrect as though this does offer concurrent access from many <a href="#EC2">EC2</a> Linux instances, it is
		not the most cost&#8211;effective solution.<br /><br />INCORRECT: &quot;Amazon EBS&quot; is incorrect. The
		Elastic Block Store (EBS) is not a good solution for concurrent access from many <a href="#EC2">EC2</a>
		instances and is not the most cost&#8211;effective option either. EBS volumes are mounted to a single instance
		except when using multi&#8211;attach which is a new feature and has several constraints.<br /><br />INCORRECT:
		&quot;Amazon <a href="#EC2">EC2</a> instance store&quot; is incorrect as this is an ephemeral storage solution
		which means the data is lost when powered down.<br /><br />Therefore, this is not an option for long&#8211;term
		data storage.<br /><br />References:<br /><br />Amazon Simple Storage Service > User Guide > Best practices
		design patterns: optimizing Amazon <a href="#S3">S3</a> performance</div><a href="#All">All(200)</a> <a href="#"
		<i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 37<br />A media streaming company collects
		real&#8211;time data and stores it in a disk&#8211;optimized database system. The company is not getting the
		expected throughput and wants an in&#8211;memory database storage solution that performs faster and provides
		high availability using data replication.<br /><br />Which database should a solutions architect
		recommend?<br /><br />A. Amazon RDS for MySQL<br />B. Amazon RDS for PostgreSQL.<br />C. Amazon ElastiCache for
		Redis<br />D. Amazon ElastiCache for Memcached<br /><br /><b>Correct Answer:</b><br />C. Amazon ElastiCache for
		Redis<br /><br />Answer Description:<br />In&#8211;memory databases on AWS Amazon Elasticache for
		Redis.<br />Amazon ElastiCache for Redis is a blazing fast in&#8211;memory data store that provides
		submillisecond latency to power internet&#8211;scale, real&#8211;time applications. Developers can use
		ElastiCache for Redis as an in&#8211;memory nonrelational database. The ElastiCache for Redis cluster
		configuration supports up to 15 shards and enables customers to run Redis workloads with up to 6.1 TB of
		in&#8211;memory capacity in a single cluster.<br /><br />ElastiCache for Redis also provides the ability to add
		and remove shards from a running cluster. You can dynamically scale out and even scale in your Redis cluster
		workloads to adapt to changes in demand.<br /><br />Amazon ElastiCache is an in&#8211;memory database. With
		ElastiCache Memcached there is no data replication or high availability. As you can see in the diagram, each
		node is a separate partition of data:<br /><br />Therefore, the Redis engine must be used which does support
		both data replication and clustering. The following diagram shows a Redis architecture with cluster mode
		enabled:<br /><br />CORRECT: &quot;Amazon ElastiCache for Redis&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Amazon ElastiCache for Memcached&quot; is incorrect as Memcached does not
		support data replication or high availability.<br /><br />INCORRECT: &quot;Amazon RDS for MySQL&quot; is
		incorrect as this is not an in&#8211;memory database. INCORRECT: &quot;Amazon RDS for PostgreSQL&quot; is
		incorrect as this is not an in&#8211;memory database.<br /><br />References:<br /><br />Amazon ElastiCache for
		Redis</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 38<br />A company has on&#8211;premises servers
		running a relational database. The current database serves high read traffic for users in different locations.
		The company wants to migrate to AWS with the least amount of effort.<br />The database solution should support
		disaster recovery and not affect the company&aposs current traffic flow.<br /><br />Which solution meets these
		requirements?<br /><br />A. Use a database in Amazon RDS with Multi&#8211;AZ and at least one read
		replica.<br />B. Use a database in Amazon RDS with Multi&#8211;AZ and at least one standby replica.<br />C. Use
		databases hosted on multiple Amazon <a href="#EC2">EC2</a> instances in different AWS Regions.<br />D. Use
		databases hosted on Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer in different
		Availability Zones.<br /><br /><b>Correct Answer:</b><br />A. Use a database in Amazon RDS with Multi&#8211;AZ
		and at least one read replica.<br /><br />References:<br /><br />Enabling data classification for Amazon RDS
		database with Macie</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 39<br />A company&aposs application is running on
		Amazon <a href="#EC2">EC2</a> instances within an Auto Scaling group behind an Elastic Load Balancer. Based on
		the application&aposs history the company anticipates a spike in traffic during a holiday each year. A solutions
		architect must design a strategy to ensure that the Auto Scaling group proactively increases capacity to
		minimize any performance impact on application users.<br /><br />Which solution will meet these
		requirements?<br /><br />A. Create an Amazon CloudWatch alarm to scale up the <a href="#EC2">EC2</a> instances
		when CPU utilization exceeds 90%.<br />B. Create a recurring scheduled action to scale up the Auto Scaling group
		before the expected period of peak demand.<br />C. Increase the minimum and maximum number of <a
			href="#EC2">EC2</a> instances in the Auto Scaling group during the peak demand period.<br />D. Configure an
		Amazon Simple Notification Service (Amazon SNS) notification to send alerts when there are autoscaling <a
			href="#EC2">EC2</a>_INSTANCE_LAUNCH events.<br /><br /><b>Correct Answer:</b><br />B. Create a recurring
		scheduled action to scale up the Auto Scaling group before the expected period of peak demand.<br /><br />Answer
		Description:<br />AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain
		steady, predictable performance at the lowest possible cost. AWS Auto Scaling refers to a collection of Auto
		Scaling capabilities across several AWS services.<br /><br />The services within the AWS Auto Scaling family
		include: Amazon <a href="#EC2">EC2</a> (known as Amazon <a href="#EC2">EC2</a> Auto Scaling).<br /><br />Amazon
		<a href="#ECS">ECS</a>. Amazon DynamoDB. Amazon Aurora.<br /><br />The scaling options define the triggers and
		when instances should be provisioned/de&#8211;provisioned. There are four scaling options:<br /><br />Maintain
		&#8212; keep a specific or minimum number of instances running. Manual &#8212; use maximum, minimum, or a
		specific number of instances.<br /><br />Scheduled &#8212; increase or decrease the number of instances based on
		a schedule. Dynamic &#8212; scale based on real&#8211;time system metrics (e.g. CloudWatch
		metrics).<br /><br />The following table describes the scaling options available and when to use
		them:<br /><br />The scaling options are configured through Scaling Policies which determine when, if, and how
		the ASG scales and shrinks.<br /><br />The following table describes the scaling policy types available for
		dynamic scaling policies and when to use them (more detail further down the page):<br /><br />The diagram below
		depicts an Auto Scaling group with a Scaling policy set to a minimum size of 1 instance, a desired capacity of 2
		instances, and a maximum size of 4 instances:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling supports
		sending Amazon SNS notifications when the following events occur.<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 40<br />An Amazon <a href="#EC2">EC2</a>
		administrator created the following policy associated with an <a href="#IAM">IAM</a> group containing several
		users:<br /><br />An Amazon <a href="#EC2">EC2</a> administrator created the following policy associated with an
		<a href="#IAM">IAM</a> group containing several users.<br /><br />What is the effect of this
		policy?<br /><br />A. Users can terminate an <a href="#EC2">EC2</a> instance in any AWS Region except
		us&#8211;east&#8211;1.<br />B. Users can terminate an <a href="#EC2">EC2</a> instance with the IP address
		10.100.100.1 in the us&#8211;east&#8211;1 Region.<br />C. Users can terminate an <a href="#EC2">EC2</a> instance
		in the us&#8211;east&#8211;1 Region when the user&aposs source IP is 10.100.100.254.<br />D. Users cannot
		terminate an <a href="#EC2">EC2</a> instance in the us&#8211;east&#8211;1 Region when the user&aposs source IP
		is 10.100.100.254.<br /><br /><b>Correct Answer:</b><br />C. Users can terminate an <a href="#EC2">EC2</a>
		instance in the us&#8211;east&#8211;1 Region when the user&aposs source IP is 10.100.100.254.<br /><br />Answer
		Description:<br />What the policy means:<br />1. <a href="#All">All</a>ow termination of any instance if
		user&aposs source IP address is 100.100.254.<br />2. Deny termination of instances that are not in the
		us&#8211;east&#8211;1 Combining this two, you get:<br />&quot;<a href="#All">All</a>ow instance termination in
		the us&#8211;east&#8211;1 region if the user&aposs source IP address is 10.100.100.254. Deny termination
		operation on other regions.&quot;<br /><br /><br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 41<br />A solutions architect is optimizing a
		website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be
		available on demand. The event is expected to attract a global online audience.<br /><br />Which service will
		improve the performance of both the real&#8211;time and on&#8211;demand streaming?<br /><br />A. Amazon <a
			href="#CloudFront">CloudFront</a><br />B. AWS Global Accelerator<br />C. Amazon Route <a
			href="#S3">S3</a><br />D. Amazon <a href="#S3">S3</a> Transfer Acceleration<br /><br /><b>Correct
			Answer:</b><br />A. Amazon <a href="#CloudFront">CloudFront</a><br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 42<br />A company built a food ordering
		application that captures user data and stores it for future analysis. The application&aposs static front end is
		deployed on an Amazon <a href="#EC2">EC2</a> instance. The front&#8211;end application sends the requests to the
		backend application running on separate <a href="#EC2">EC2</a> instance. The backend application then stores the
		data in Amazon RDS.<br /><br />What should a solutions architect do to decouple the architecture and make it
		scalable?<br /><br />A. Use Amazon <a href="#S3">S3</a> to serve the front&#8211;end application, which sends
		requests to Amazon <a href="#EC2">EC2</a> to execute the backend application. The backend application will
		process and store the data in Amazon RDS.<br />B. Use Amazon <a href="#S3">S3</a> to serve the front&#8211;end
		application and write requests to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon <a
			href="#EC2">EC2</a> instances to the HTTP/HTTPS endpoint of the topic, and process and store the data in
		Amazon RDS.<br />C. Use an <a href="#EC2">EC2</a> instance to serve the front end and write requests to an
		Amazon SQS queue. Place the backend instance in an Auto Scaling group, and scale based on the queue depth to
		process and store the data in Amazon RDS.<br />D. Use Amazon <a href="#S3">S3</a> to serve the static
		front&#8211;end application and send requests to Amazon API Gateway, which writes the requests to an Amazon SQS
		queue. Place the backend instances in an Auto Scaling group, and scale based on the queue depth to process and
		store the data in Amazon RDS.<br /><br /><b>Correct Answer:</b><br />D. Use Amazon <a href="#S3">S3</a> to serve
		the static front&#8211;end application and send requests to Amazon API Gateway, which writes the requests to an
		Amazon SQS queue. Place the backend instances in an Auto Scaling group, and scale based on the queue depth to
		process and store the data in Amazon RDS.<br /><br />Answer Description:<br />Keyword: Static + Decouple +
		Scalable Static=<a href="#S3">S3</a><br /><br />Decouple=SQS Queue Scalable=ASG<br /><br />Option B will not be
		there in the race due to Auto&#8211;Scaling unavailability. Option A will not be there in the race due to
		Decouple unavailability.<br /><br />Option C & D will be in the race and Option D will be correct answers due to
		all 3 combination matches [Static=<a href="#S3">S3</a>; Decouple=SQS Queue; Scalable=ASG] & Option C will loose
		due to Static option unavailability<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 43<br />A solutions architect is designing a web
		application that will run on Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer (ALB).
		The company strictly requires that the application be resilient against malicious internet activity and attacks,
		and protect against new common vulnerabilities and exposures.<br /><br />What should the solutions architect
		recommend?<br /><br />A. Leverage Amazon <a href="#CloudFront">CloudFront</a> with the ALB endpoint as the
		origin.<br />B. Deploy an appropriate managed rule for AWS WAF and associate it with the ALB.<br />C. Subscribe
		to AWS Shield Advanced and ensure common vulnerabilities and exposures are blocked.<br />D. Configure network
		ACLs and security groups to allow only ports 80 and 443 to access the <a href="#EC2">EC2</a>
		instances.<br /><br /><b>Correct Answer:</b><br />B. Deploy an appropriate managed rule for AWS WAF and
		associate it with the ALB.<br /><br />References:<br /><br />AWS WAF &#8212; Web Application Firewall<br />AWS
		Shield<br />AWS Shield Features</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 44<br />A company is managing health records
		on&#8211;premises. The company must keep these records indefinitely, disable any modifications to the records
		once they are stored, and granularly audit access at all levels. The chief technology officer (CTO) is concerned
		because there are already millions of records not being used by any application, and the current infrastructure
		is running out of space. The CTO has requested a solutions architect design a solution to move existing data and
		support future records.<br /><br />Which services can the solutions architect recommend to meet these
		requirements?<br /><br />A. Use AWS DataSync to move existing data to AWS. Use Amazon <a href="#S3">S3</a> to
		store existing and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable AWS CloudTrail with data
		events.<br />B. Use AWS Storage Gateway to move existing data to AWS. Use Amazon <a href="#S3">S3</a> to store
		existing and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable AWS CloudTrail with management
		events.<br />C. Use AWS DataSync to move existing data to AWS. Use Amazon <a href="#S3">S3</a> to store existing
		and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable AWS CloudTrail with management
		events.<br />D. Use AWS Storage Gateway to move existing data to AWS. Use Amazon Elastic Block Store (Amazon
		EBS) to store existing and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable Amazon <a
			href="#S3">S3</a> server access logging.<br /><br /><b>Correct Answer:</b><br />D. Use AWS Storage Gateway
		to move existing data to AWS. Use Amazon Elastic Block Store (Amazon EBS) to store existing and new data. Enable
		Amazon <a href="#S3">S3</a> object lock and enable Amazon <a href="#S3">S3</a> server access
		logging.<br /><br />Answer Description:<br />Keyword: Move existing data and support future records + Granular
		audit access at all levels<br /><br />Use AWS DataSync to migrate existing data to Amazon <a href="#S3">S3</a>,
		and then use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for
		ongoing updates from your on&#8211;premises file&#8211;based applications.<br /><br />Need a solution to move
		existing data and support future records = AWS DataSync should be used for migration.<br /><br />Need granular
		audit access at all levels = Data Events should be used in CloudTrail, Management Events is enabled by
		default.<br /><br />CORRECT: &quot;Use AWS DataSync to move existing data to AWS. Use Amazon <a
			href="#S3">S3</a> to store existing and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable
		AWS CloudTrail with data events&quot; is the correct answer.<br /><br />INCORRECT: &quot;Use AWS Storage Gateway
		to move existing data to AWS. Use Amazon <a href="#S3">S3</a> to store existing and new data. Enable Amazon <a
			href="#S3">S3</a> object lock and enable AWS CloudTrail with management events&quot; is incorrect as
		&quot;current infrastructure is running out of space&quot; INCORRECT: &quot;Use AWS DataSync to move existing
		data to AWS. Use Amazon <a href="#S3">S3</a> to store existing and new data. Enable Amazon <a href="#S3">S3</a>
		object lock and enable AWS CloudTrail with management events.&quot; is incorrect as &quot;Management Events is
		enabled by default&quot; INCORRECT: &quot;Use AWS Storage Gateway to move existing data to AWS. Use Amazon
		Elastic Block Store (Amazon EBS) to store existing and new data. Enable Amazon <a href="#S3">S3</a> object lock
		and enable Amazon <a href="#S3">S3</a> server access logging.&quot; is incorrect as &quot;current infrastructure
		is running out of space&quot;<br /><br />References:<br /><br />AWS DataSync<br />AWS CloudTrail<br />AWS
		Storage Gateway</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 45<br />A company wants to use Amazon <a
			href="#S3">S3</a> for the secondary copy of its on&#8211;premises dataset. The company would rarely need to
		access this copy. The storage solution&aposs cost should be minimal.<br /><br />Which storage solution meets
		these requirements?<br /><br />A. <a href="#S3">S3</a> Standard<br />B. <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br />C. <a href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a>
		Standard&#8211;IA)<br />D. <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />D. <a href="#S3">S3</a> One Zone&#8211;Infrequent Access
		(<a href="#S3">S3</a> One Zone&#8211;IA)<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 46<br />A company&aposs operations team has an
		existing Amazon <a href="#S3">S3</a> bucket configured to notify an Amazon SQS queue when new objects are
		created within the bucket. The development team also wants to receive events when new objects are created. The
		existing operations team workflow must remain intact.<br /><br />Which solution would satisfy these
		requirements?<br /><br />A. Create another SQS queue. Update the <a href="#S3">S3</a> events in the bucket to
		also update the new queue when a new object is created.<br />B. Create a new SQS queue that only allows Amazon
		<a href="#S3">S3</a> to access the queue. Update Amazon <a href="#S3">S3</a> to update this queue when a new
		object is created.<br />C. Create an Amazon SNS topic and SQS queue for the bucket updates. Update the bucket to
		send events to the new topic. Updates both queues to poll Amazon SNS.<br />D. Create an Amazon SNS topic and SQS
		queue for the bucket updates. Update the bucket to send events to the new topic. Add subscriptions for both
		queues in the topic.<br /><br /><b>Correct Answer:</b><br />D. Create an Amazon SNS topic and SQS queue for the
		bucket updates. Update the bucket to send events to the new topic. Add subscriptions for both queues in the
		topic.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 47<br />An application runs on Amazon <a
			href="#EC2">EC2</a> instances in private subnets. The application needs to access an Amazon DynamoDB table.
		What is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS
		network?<br /><br />A. Use a <a href="#VPC">VPC</a> endpoint for DynamoDB.<br />B. Use a NAT gateway in a public
		subnet.<br />C. Use a NAT instance in a private subnet.<br />D. Use the internet gateway attached to the <a
			href="#VPC">VPC</a>.<br /><br /><b>Correct Answer:</b><br />A. Use a <a href="#VPC">VPC</a> endpoint for
		DynamoDB.<br /><br />Answer Description:<br />An Interface endpoint uses AWS PrivateLink and is an elastic
		network interface (ENI) with a private IP address that serves as an entry point for traffic destined to a
		supported service.<br /><br />Using PrivateLink you can connect your <a href="#VPC">VPC</a> to supported AWS
		services, services hosted by other AWS accounts (<a href="#VPC">VPC</a> endpoint services), and supported AWS
		Marketplace partner services.<br /><br />AWS PrivateLink access over Inter&#8211;Region <a href="#VPC">VPC</a>
		Peering:<br /><br />Applications in an AWS <a href="#VPC">VPC</a> can securely access AWS PrivateLink endpoints
		across AWS Regions using Inter&#8211;Region <a href="#VPC">VPC</a> Peering.<br /><br />AWS PrivateLink allows
		you to privately access services hosted on AWS in a highly available and scalable manner, without using public
		IPs, and without requiring the traffic to traverse the Internet.<br /><br />Customers can privately connect to a
		service even if the service endpoint resides in a different AWS Region.<br /><br />Traffic using
		Inter&#8211;Region <a href="#VPC">VPC</a> Peering stays on the global AWS backbone and never traverses the
		public Internet.<br /><br />A gateway endpoint is a gateway that is a target for a specified route in your route
		table, used for traffic destined to a supported AWS service.<br /><br />An interface <a href="#VPC">VPC</a>
		endpoint (interface endpoint) enables you to connect to services powered by AWS
		PrivateLink.<br /><br />References:<br />Amazon DynamoDB > Developer Guide > What Is Amazon DynamoDB?<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 48<br />A company has created a <a
			href="#VPC">VPC</a> with multiple private subnets in multiple Availability Zones (AZs) and one public subnet
		in one of the AZs. The public subnet is used to launch a NAT gateway. There are instances in the private subnets
		that use a NAT gateway to connect to the internet. In case of an AZ failure, the company wants to ensure that
		the instances are not all experiencing internet connectivity issues and that there is a backup plan
		ready.<br /><br />Which solution should a solutions architect recommend that is MOST highly
		available?<br /><br />A. Create a new public subnet with a NAT gateway in the same AZ. Distribute the traffic
		between the two NAT gateways.<br />B. Create an Amazon <a href="#EC2">EC2</a> NAT instance in a new public
		subnet. Distribute the traffic between the NAT gateway and the NAT instance.<br />C. Create public subnets in
		each AZ and launch a NAT gateway in each subnet. Configure the traffic from the private subnets in each AZ to
		the respective NAT gateway.<br />D. Create an Amazon <a href="#EC2">EC2</a> NAT instance in the same public
		subnet. Replace the NAT gateway with the NAT instance and associate the instance with an Auto Scaling group with
		an appropriate scaling policy.<br /><br /><b>Correct Answer:</b><br />C. Create public subnets in each AZ and
		launch a NAT gateway in each subnet. Configure the traffic from the private subnets in each AZ to the respective
		NAT gateway.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 49<br />A company recently deployed a new auditing
		system to centralize information about operating system versions, patching, and installed software for Amazon <a
			href="#EC2">EC2</a> instances. A solutions architect must ensure all instances provisioned through <a
			href="#EC2">EC2</a> Auto Scaling groups successfully send reports to the auditing system as soon as they are
		launched and terminated.<br /><br />Which solution achieves these goals MOST efficiently?<br /><br />A. Use a
		scheduled AWS Lambda function and execute a script remotely on all <a href="#EC2">EC2</a> instances to send data
		to the audit system.<br />B. Use <a href="#EC2">EC2</a> Auto Scaling lifecycle hooks to execute a custom script
		to send data to the audit system when instances are launched and terminated.<br />C. Use an <a
			href="#EC2">EC2</a> Auto Scaling launch configuration to execute a custom script through user data to send
		data to the audit system when instances are launched and terminated.<br />D. Execute a custom script on the
		instance operating system to send data to the audit system. Configure the script to be executed by the <a
			href="#EC2">EC2</a> Auto Scaling group when the instance starts and is terminated.<br /><br /><b>Correct
			Answer:</b><br />B. Use <a href="#EC2">EC2</a> Auto Scaling lifecycle hooks to execute a custom script to
		send data to the audit system when instances are launched and terminated.<br /></div><a href="#All">All(200)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 50<br />A company recently implemented hybrid
		cloud connectivity using AWS Direct Connect and is migrating data to Amazon <a href="#S3">S3</a>. The company is
		looking for a fully managed solution that will automate and accelerate the replication of data between the
		on&#8211;premises storage systems and AWS storage services.<br /><br />Which solution should a solutions
		architect recommend to keep the data private?<br /><br />A. Deploy an AWS DataSync agent for the
		on&#8211;premises environment. Configure a sync job to replicate the data and connect it with an AWS service
		endpoint.<br />B. Deploy an AWS DataSync agent for the on&#8211;premises environment. Schedule a batch job to
		replicate point&#8211;in&#8211;time snapshots to AWS.<br />C. Deploy an AWS Storage Gateway volume gateway for
		the on&#8211;premises environment. Configure it to store data locally, and asynchronously back up
		point&#8211;in&#8211;time snapshots to AWS.<br />D. Deploy an AWS Storage Gateway file gateway for the
		on&#8211;premises environment. Configure it to store data locally, and asynchronously back up
		point&#8211;in&#8211;time snapshots to AWS.<br /><br /><b>Correct Answer:</b><br />A. Deploy an AWS DataSync
		agent for the on&#8211;premises environment. Configure a sync job to replicate the data and connect it with an
		AWS service endpoint.<br /><br />Answer Description:<br />You can use AWS DataSync with your Direct Connect link
		to access public service endpoints or private <a href="#VPC">VPC</a> endpoints. When using <a
			href="#VPC">VPC</a> endpoints, data transferred between the DataSync agent and AWS services does not
		traverse the public internet or need public IP addresses, increasing the security of data as it is copied over
		the network.<br /><br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 51<br />A company has 150 TB of archived image
		data stored on&#8211;premises that needs to be moved to the AWS Cloud within the next month. The company&aposs
		current network connection allows up to 100 Mbps uploads for this purpose during the night only.<br /><br />What
		is the MOST cost&#8211;effective mechanism to move this data and meet the migration deadline?<br /><br />A. Use
		AWS Snowmobile to ship the data to AWS.<br />B. Order multiple AWS Snowball devices to ship the data to
		AWS.<br />C. Enable Amazon <a href="#S3">S3</a> Transfer Acceleration and securely upload the data.<br />D.
		Create an Amazon <a href="#S3">S3</a> <a href="#VPC">VPC</a> endpoint and establish a VPN to upload the
		data.<br /><br /><b>Correct Answer:</b><br />B. Order multiple AWS Snowball devices to ship the data to
		AWS.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 52<br />A company is seeing access requests by
		some suspicious IP addresses. The security team discovers the requests are from different IP addresses under the
		same CIDR range.<br /><br />What should a solutions architect recommend to the team?<br /><br />A. Add a rule in
		the inbound table of the security to deny the traffic from that CIDR range.<br />B. Add a rule in the outbound
		table of the security group to deny the traffic from that CIDR range.<br />C. Add a deny rule in the inbound
		table of the network ACL with a lower number than other rules.<br />D. Add a deny rule in the outbound table of
		the network ACL with a lower rule number than other rules.<br /><br /><b>Correct Answer:</b><br />C. Add a deny
		rule in the inbound table of the network ACL with a lower number than other rules.<br /><br />Answer
		Description:<br />You can only create deny rules with network ACLs, it is not possible with security groups.
		Network ACLs process rules in order from the lowest numbered rules to the highest until they reach and allow or
		deny. The following table describes some of the differences between security groups and network
		ACLs:<br /><br />Therefore, the solutions architect should add a deny rule in the inbound table of the network
		ACL with a lower rule number than other rules.<br /><br />CORRECT: &quot;Add a deny rule in the inbound table of
		the network ACL with a lower rule number than other rules&quot; is the correct answer.<br /><br />INCORRECT:
		&quot;Add a deny rule in the outbound table of the network ACL with a lower rule number than other rules&quot;
		is incorrect as this will only block outbound traffic.<br /><br />INCORRECT: &quot;Add a rule in the inbound
		table of the security group to deny the traffic from that CIDR range&quot; is incorrect as you cannot create a
		deny rule with a security group.<br /><br />INCORRECT: &quot;Add a rule in the outbound table of the security
		group to deny the traffic from that CIDR range&quot; is incorrect as you cannot create a deny rule with a
		security group.<br /><br />References:<br />Amazon Virtual Private Cloud > User Guide > Network ACLs<br /></div>
	<a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 53<br />A company recently expanded globally and
		wants to make its application accessible to users in those geographic locations. The application is deployed on
		Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer in an Auto Scaling group. The
		company needs the ability shift traffic from resources in one region to another.<br /><br />What should a
		solutions architect recommend?<br /><br />A. Configure an Amazon Route 53 latency routing policy.<br />B.
		Configure an Amazon Route 53 geolocation routing policy.<br />C. Configure an Amazon Route 53 geoproximity
		routing policy.<br />D. Configure an Amazon Route 53 multivalue answer routing policy.<br /><br /><b>Correct
			Answer:</b><br />C. Configure an Amazon Route 53 geoproximity routing policy.<br /><br />Answer
		Description:<br />Keyword: Users in those Geographic Locations<br /><br />Condition: Ability Shift traffic from
		resources in One Region to Another Region The following table highlights the key function of each type of
		routing policy:<br /><br />Geo&#8211;location:<br /><br />Caters to different users in different countries and
		different languages.<br /><br />Contains users within a particular geography and offers them a customized
		version of the workload based on their specific needs.<br /><br />Geolocation can be used for localizing content
		and presenting some or all of your website in the language of your users.<br /><br />Can also protect
		distribution rights.<br /><br />Can be used for spreading load evenly between regions.<br /><br />If you have
		multiple records for overlapping regions, Route 53 will route to the smallest geographic region.<br /><br />You
		can create a default record for IP addresses that do not map to a geographic
		location.<br /><br />References:<br /><br />Amazon Route 53 > Developer Guide > Choosing a routing
		policy<br />Amazon Route 53</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 54<br />A company wants to replicate its data to
		AWS to recover in the event of a disaster. Today, a system administrator has scripts that copy data to a NFS
		share Individual backup files need to be accessed with low latency by application administrators to deal with
		errors in processing.<br /><br />What should a solutions architect recommend to meet these
		requirements?<br /><br />A. Modify the script to copy data to an Amazon <a href="#S3">S3</a> bucket instead of
		the on&#8211;premises NFS share.<br />B. Modify the script to copy data to an Amazon <a href="#S3">S3</a>
		Glacier Archive instead of the on&#8211;premises NFS share.<br />C. Modify the script to copy data to an Amazon
		Elastic File System (Amazon EFS) volume instead of the on&#8211;premises NFS share.<br />D. Modify the script to
		copy data to an AWS Storage Gateway for File Gateway virtual appliance instead of the on&#8211;premises NFS
		share.<br /><br /><b>Correct Answer:</b><br />D. Modify the script to copy data to an AWS Storage Gateway for
		File Gateway virtual appliance instead of the on&#8211;premises NFS share.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 55<br />A solutions architect is designing a
		mission&#8211;critical web application. It will consist of Amazon <a href="#EC2">EC2</a> instances behind an
		Application Load Balancer and a relational database. The database should be highly available and fault
		tolerant.<br /><br />Which database implementations will meet these requirements? (Choose two.)<br /><br />A.
		Amazon Redshift<br />B. Amazon DynamoDB<br />C. Amazon RDS for MySQL<br />D. MySQL&#8211;compatible Amazon
		Aurora Multi&#8211;AZ<br />E. Amazon RDS for SQL Server Standard Edition Multi&#8211;AZ<br /><br /><b>Correct
			Answer:</b><br />D. MySQL&#8211;compatible Amazon Aurora Multi&#8211;AZ<br />E. Amazon RDS for SQL Server
		Standard Edition Multi&#8211;AZ<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 56<br />A company&aposs web application is running
		on Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer.<br /><br />The company recently
		changed its policy, which now requires the application to be accessed from one specific country
		only.<br /><br />Which configuration will meet this requirement?<br /><br />A. Configure the security group for
		the <a href="#EC2">EC2</a> instances.<br />B. Configure the security group on the Application Load
		Balancer.<br />C. Configure AWS WAF on the Application Load Balancer in a <a href="#VPC">VPC</a>.<br />D.
		Configure the network ACL for the subnet that contains the <a href="#EC2">EC2</a>
		instances.<br /><br /><b>Correct Answer:</b><br />C. Configure AWS WAF on the Application Load Balancer in a <a
			href="#VPC">VPC</a>.<br /><br />References:<br /><br />AWS Security Blog > How to use AWS WAF to filter
		incoming traffic from embargoed countries</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 57<br />A solutions architect has created two <a
			href="#IAM">IAM</a> policies: Policy1 and Policy2. Both policies are attached to an <a href="#IAM">IAM</a>
		group.<br /><br />A solutions architect has created two <a href="#IAM">IAM</a> policies: Policy1 and Policy2.
		Both policies are attached to an <a href="#IAM">IAM</a> group.<br /><br />A cloud engineer is added as an <a
			href="#IAM">IAM</a> user to the <a href="#IAM">IAM</a> group. Which action will the cloud engineer be able
		to perform?<br /><br />A. Deleting <a href="#IAM">IAM</a> users<br />B. Deleting directories<br />C. Deleting
		Amazon <a href="#EC2">EC2</a> instances<br />D. Deleting logs from Amazon CloudWatch Logs<br /><br /><b>Correct
			Answer:</b><br />C. Deleting Amazon <a href="#EC2">EC2</a> instances<br /></div><a href="#All">All(200)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 58<br />A company has an Amazon <a
			href="#EC2">EC2</a> instance running on a private subnet that needs to access a public website to download
		patches and updates.<br /><br />The company does not want external websites to see the <a href="#EC2">EC2</a>
		instance IP address or initiate connections to it.<br /><br />How can a solutions architect achieve this
		objective?<br /><br />A. Create a site&#8211;to&#8211;site VPN connection between the private subnet and the
		network in which the public site is deployed.<br />B. Create a NAT gateway in a public subnet. Route outbound
		traffic from the private subnet through the NAT gateway.<br />C. Create a network ACL for the private subnet
		where the <a href="#EC2">EC2</a> instance deployed only allows access from the IP address range of the public
		website.<br />D. Create a security group that only allows connections from the IP address range of the public
		website. Attach the security group to the <a href="#EC2">EC2</a> instance.<br /><br /><b>Correct
			Answer:</b><br />B. Create a NAT gateway in a public subnet. Route outbound traffic from the private subnet
		through the NAT gateway.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 59<br />A company must migrate 20 TB of data from
		a data center to the AWS Cloud within 30 days. The company&aposs network bandwidth is limited to 15 Mbps and
		cannot exceed 70% utilization. What should a solutions architect do to meet these requirements?<br /><br />A.
		Use AWS Snowball.<br />B. Use AWS DataSync.<br />C. Use a secure VPN connection.<br />D. Use Amazon <a
			href="#S3">S3</a> Transfer Acceleration.<br /><br /><b>Correct Answer:</b><br />A. Use AWS Snowball.<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 60<br />A company has a website running on Amazon
		<a href="#EC2">EC2</a> instances across two Availability Zones. The company is expecting spikes in traffic on
		specific holidays, and wants to provide a consistent user experience. How can a solutions architect meet this
		requirement?<br /><br />A. Use step scaling.<br />B. Use simple scaling.<br />C. Use lifecycle hooks.<br />D.
		Use scheduled scaling.<br /><br /><b>Correct Answer:</b><br />D. Use scheduled scaling.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 61<br />A company has an on&#8211;premises data
		center that is running out of storage capacity. The company wants to migrate its storage infrastructure to AWS
		while minimizing bandwidth costs. The solution must allow for immediate retrieval of data at no additional
		cost.<br /><br />How can these requirements be met?<br /><br />A. Deploy Amazon <a href="#S3">S3</a> Glacier
		Vault and enable expedited retrieval. Enable provisioned retrieval capacity for the workload.<br />B. Deploy AWS
		Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon <a href="#S3">S3</a> while
		retaining copies of frequently accessed data subsets locally.<br />C. Deploy AWS Storage Gateway using stored
		volumes to store data locally. Use Storage Gateway to asynchronously back up point&#8211;in&#8211;time snapshots
		of the data to Amazon <a href="#S3">S3</a>.<br />D. Deploy AWS Direct Connect to connect with the
		on&#8211;premises data center. Configure AWS Storage Gateway to store data locally. Use Storage Gateway to
		asynchronously back up point&#8211;in&#8211;time snapshots of the data to Amazon <a
			href="#S3">S3</a>.<br /><br /><b>Correct Answer:</b><br />C. Deploy AWS Storage Gateway using stored volumes
		to store data locally. Use Storage Gateway to asynchronously back up point&#8211;in&#8211;time snapshots of the
		data to Amazon <a href="#S3">S3</a>.<br /><br />Answer Description:<br />Volume Gateway provides an iSCSI
		target, which enables you to create block storage volumes and mount them as iSCSI devices from your
		on&#8211;premises or <a href="#EC2">EC2</a> application servers. The Volume Gateway runs in either a cached or
		stored mode:<br /><br />In the cached mode, your primary data is written to <a href="#S3">S3</a>, while
		retaining your frequently accessed data locally in a cache for low&#8211;latency access.<br /><br />In the
		stored mode, your primary data is stored locally and your entire dataset is available for low&#8211;latency
		access while asynchronously backed up to AWS.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 62<br />A company delivers files in Amazon <a
			href="#S3">S3</a> to certain users who do not have AWS credentials. These users must be given access for a
		limited time.<br /><br />What should a solutions architect do to securely meet these requirements?<br /><br />A.
		Enable public access on an Amazon <a href="#S3">S3</a> bucket.<br />B. Generate a presigned URL to share with
		the users.<br />C. Encrypt files using AWS KMS and provide keys to the users.<br />D. Create and assign <a
			href="#IAM">IAM</a> roles that will grant GetObject permissions to the users.<br /><br /><b>Correct
			Answer:</b><br />B. Generate a presigned URL to share with the users.<br /></div><a href="#All">All(200)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 63<br />A company is investigating potential
		solutions that would collect, process, and store users&apos service usage data. The business objective is to
		create an analytics capability that will enable the company to gather operational insights quickly using
		standard SQL queries. The solution should be highly available and ensure Atomicity, Consistency, Isolation, and
		Durability (ACID) compliance in the data tier.<br /><br />Which solution should a solutions architect
		recommend?<br /><br />A. Use Amazon DynamoDB transactions.<br />B. Create an Amazon Neptune database in a
		Multi&#8211;AZ design<br />C. Use a fully managed Amazon RDS for MySQL database in a Multi&#8211;AZ
		design.<br />D. Deploy PostgreSQL on an Amazon <a href="#EC2">EC2</a> instance that uses Amazon EBS Throughput
		Optimized HDD (st1) storage.<br /><br /><b>Correct Answer:</b><br />C. Use a fully managed Amazon RDS for MySQL
		database in a Multi&#8211;AZ design.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 64<br />An application running on an Amazon <a
			href="#EC2">EC2</a> instance in <a href="#VPC">VPC</a>&#8211;A needs to access files in another <a
			href="#EC2">EC2</a> instance in <a href="#VPC">VPC</a>&#8211;B. Both are in separate. AWS accounts. The
		network administrator needs to design a solution to enable secure access to <a href="#EC2">EC2</a> instance in
		<a href="#VPC">VPC</a>&#8211;B from <a href="#VPC">VPC</a>&#8211;A. The connectivity should not have a single
		point of failure or bandwidth concerns.<br /><br />Which solution will meet these requirements?<br /><br />A.
		Set up a <a href="#VPC">VPC</a> peering connection between <a href="#VPC">VPC</a>&#8211;A and <a
			href="#VPC">VPC</a>&#8211;B.<br />B. Set up <a href="#VPC">VPC</a> gateway endpoints for the <a
			href="#EC2">EC2</a> instance running in <a href="#VPC">VPC</a>&#8211;B.<br />C. Attach a virtual private
		gateway to <a href="#VPC">VPC</a>&#8211;B and enable routing from <a href="#VPC">VPC</a>&#8211;A.<br />D. Create
		a private virtual interface (VIF) for the <a href="#EC2">EC2</a> instance running in <a
			href="#VPC">VPC</a>&#8211;B and add appropriate routes from <a
			href="#VPC">VPC</a>&#8211;B.<br /><br /><b>Correct Answer:</b><br />A. Set up a <a href="#VPC">VPC</a>
		peering connection between <a href="#VPC">VPC</a>&#8211;A and <a href="#VPC">VPC</a>&#8211;B.<br /><br />Answer
		Description:<br />A <a href="#VPC">VPC</a> peering connection is a networking connection between two <a
			href="#VPC">VPC</a>s that enables you to route traffic between them using private IPv4 addresses or IPv6
		addresses. Instances in either <a href="#VPC">VPC</a> can communicate with each other as if they are within the
		same network. You can create a <a href="#VPC">VPC</a> peering connection between your own <a
			href="#VPC">VPC</a>s, or with a <a href="#VPC">VPC</a> in another AWS account.<br /><br />The traffic
		remains in the private IP space. <a href="#All">All</a> inter&#8211;region traffic is encrypted with no single
		point of failure, or bandwidth bottleneck.<br /><br />References:<br />Amazon Virtual Private Cloud > <a
			href="#VPC">VPC</a> Peering > What is <a href="#VPC">VPC</a> peering?<br /></div><a href="#All">All(200)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 65<br />A company&aposs application hosted on
		Amazon <a href="#EC2">EC2</a> instances needs to access an Amazon <a href="#S3">S3</a> bucket. Due to data
		sensitivity, traffic cannot traverse the internet.<br /><br />How should a solutions architect configure
		access?<br /><br />A. Create a private hosted zone using Amazon Route 53.<br />B. Configure a <a
			href="#VPC">VPC</a> gateway endpoint for Amazon <a href="#S3">S3</a> in the <a href="#VPC">VPC</a>.<br />C.
		Configure AWS PrivateLink between the <a href="#EC2">EC2</a> instance and the <a href="#S3">S3</a>
		bucket.<br />D. Set up a site&#8211;to&#8211;site VPN connection between the <a href="#VPC">VPC</a> and the <a
			href="#S3">S3</a> bucket.<br /><br /><b>Correct Answer:</b><br />B. Configure a <a href="#VPC">VPC</a>
		gateway endpoint for Amazon <a href="#S3">S3</a> in the <a href="#VPC">VPC</a>.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 66<br />A company has two applications it wants to
		migrate to AWS. Both applications process a large set of files by accessing the same files at the same time.
		Both applications need to read the files with low latency.<br /><br />Which architecture should a solutions
		architect recommend for this situation?<br /><br />A. Configure two AWS Lambda functions to run the
		applications. Create an Amazon <a href="#EC2">EC2</a> instance with an instance store volume to store the
		data.<br />B. Configure two AWS Lambda functions to run the applications. Create an Amazon <a
			href="#EC2">EC2</a> instance with an Amazon Elastic Block Store (Amazon EBS) volume to store the
		data.<br />C. Configure one memory optimized Amazon <a href="#EC2">EC2</a> instance to run both applications
		simultaneously. Create an Amazon Elastic Block Store (Amazon EBS) volume with Provisioned IOPS to store the
		data.<br />D. Configure two Amazon <a href="#EC2">EC2</a> instances to run both applications. Configure Amazon
		Elastic File System (Amazon EFS) with General Purpose performance mode and Bursting Throughput mode to store the
		data.<br /><br /><b>Correct Answer:</b><br />D. Configure two Amazon <a href="#EC2">EC2</a> instances to run
		both applications. Configure Amazon Elastic File System (Amazon EFS) with General Purpose performance mode and
		Bursting Throughput mode to store the data.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 67<br />An eCommerce company has noticed
		performance degradation of its Amazon RDS based web application.<br /><br />The performance degradation is
		attributed to an increase in the number of read&#8211;only SQL queries triggered by business analysts. A
		solutions architect needs to solve the problem with minimal changes to the existing web application. What should
		the solutions architect recommend?<br /><br />A. Export the data to Amazon DynamoDB and have the business
		analysts run their queries.<br />B. Load the data into Amazon ElastiCache and have the business analysts run
		their queries.<br />C. Create a read replica of the primary database and have the business analysts run their
		queries.<br />D. Copy the data into an Amazon Redshift cluster and have the business analysts run their
		queries.<br /><br /><b>Correct Answer:</b><br />C. Create a read replica of the primary database and have the
		business analysts run their queries.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 68<br />A company is running a highly sensitive
		application on Amazon <a href="#EC2">EC2</a> backed by an Amazon RDS database.<br /><br />Compliance regulations
		mandate that all personally identifiable information (PII) be encrypted at rest.<br /><br />Which solution
		should a solutions architect recommend to meet this requirement with the LEAST amount of changes to the
		infrastructure?<br /><br />A. Deploy AWS Certificate Manager to generate certificates. Use the certificates to
		encrypt the database volume.<br />B. Deploy AWS CloudHSM, generate encryption keys, and use the customer master
		key (CMK) to encrypt database volumes.<br />C. Configure SSL encryption using AWS Key Management Service
		customer master keys (AWS KMS CMKs) to encrypt database volumes.<br />D. Configure Amazon Elastic Block Store
		(Amazon EBS) encryption and Amazon RDS encryption with AWS Key Management Service (AWS KMS) keys to encrypt
		instance and database volumes.<br /><br /><b>Correct Answer:</b><br />D. Configure Amazon Elastic Block Store
		(Amazon EBS) encryption and Amazon RDS encryption with AWS Key Management Service (AWS KMS) keys to encrypt
		instance and database volumes.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 69<br />A company running an on&#8211;premises
		application is migrating the application to AWS to increase its elasticity and availability. The current
		architecture uses a Microsoft SQL Server database with heavy read activity.<br />The company wants to explore
		alternate database options and migrate database engines, if needed. Every 4 hours, the development team does a
		full copy of the production database to populate a test database.<br /><br />During this period, users
		experience latency. What should a solutions architect recommend as replacement database?<br /><br />A. Use
		Amazon Aurora with Multi&#8211;AZ Aurora Replicas and restore from mysqldump for the test database.<br />B. Use
		Amazon Aurora with Multi&#8211;AZ Aurora Replicas and restore snapshots from Amazon RDS for the test
		database.<br />C. Use Amazon RDS for MySQL with a Multi&#8211;AZ deployment and read replicas, and use the
		standby instance for the test database.<br />D. Use Amazon RDS for SQL Server with a Multi&#8211;AZ deployment
		and read replicas, and restore snapshots from RDS for the test database.<br /><br /><b>Correct
			Answer:</b><br />D. Use Amazon RDS for SQL Server with a Multi&#8211;AZ deployment and read replicas, and
		restore snapshots from RDS for the test database.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 70<br />A company has enabled AWS CloudTrail logs
		to deliver log files to an Amazon <a href="#S3">S3</a> bucket for each of its developer accounts. The company
		has created a central AWS account for streamlining management and audit reviews. An internal auditor needs to
		access the CloudTrail logs, yet access needs to be restricted for all developer account users. The solution must
		be secure and optimized.<br /><br />How should a solutions architect meet these requirements?<br /><br />A.
		Configure an AWS Lambda function in each developer account to copy the log files to the central account. Create
		an <a href="#IAM">IAM</a> role in the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy
		providing read only permissions to the bucket.<br />B. Configure CloudTrail from each developer account to
		deliver the log files to an <a href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a>
		user in the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing full permissions
		to the bucket.<br />C. Configure CloudTrail from each developer account to deliver the log files to an <a
			href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a> role in the central
		account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing read only permissions to the
		bucket.<br />D. Configure an AWS Lambda function in the central account to copy the log files from the <a
			href="#S3">S3</a> bucket in each developer account. Create an <a href="#IAM">IAM</a> user in the central
		account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing full permissions to the
		bucket.<br /><br /><b>Correct Answer:</b><br />C. Configure CloudTrail from each developer account to deliver
		the log files to an <a href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a> role in
		the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing read only permissions to
		the bucket.<br /><br /> Go to dashboard</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 71<br />A company has a Microsoft
		Windows&#8211;based application that must be migrated to AWS. This application requires the use of a shared
		Windows file system attached to multiple Amazon <a href="#EC2">EC2</a> Windows instances.<br /><br />What should
		a solutions architect do to accomplish this?<br /><br />A. Configure a volume using Amazon EFS. Mount the EFS
		volume to each Windows instance.<br />B. Configure AWS Storage Gateway in Volume Gateway mode. Mount the volume
		to each Windows instance.<br />C. Configure Amazon FSx for Windows File Server. Mount the Amazon FSx volume to
		each Windows instance.<br />D. Configure an Amazon EBS volume with the required size. Attach each <a
			href="#EC2">EC2</a> instance to the volume. Mount the file system within the volume to each Windows
		instance.<br /><br /><b>Correct Answer:</b><br />C. Configure Amazon FSx for Windows File Server. Mount the
		Amazon FSx volume to each Windows instance.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 72<br />A company has implemented one of its
		microservices on AWS Lambda that accesses an Amazon DynamoDB table named Books. A solutions architect is
		designing an <a href="#IAM">IAM</a> policy to be attached to the Lambda function&aposs <a href="#IAM">IAM</a>
		role, giving it access to put, update, and delete items in the Books table.<br /><br />The <a
			href="#IAM">IAM</a> policy must prevent function from performing any other actions on the Books table or any
		other.<br /><br />Which <a href="#IAM">IAM</a> policy would fulfill these needs and provide the LEAST privileged
		access?<br /><br />A.<br /><br />B.<br /><br />C.<br /><br />D.<br /><br /><b>Correct Answer:</b><br />A<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 73<br />A company runs a website on Amazon <a
			href="#EC2">EC2</a> instances behind an ELB Application Load Balancer. Amazon Route 53 is used for the DNS.
		The company wants to set up a backup website with a message including a phone number and email address that
		users can reach if the primary website is down.<br /><br />How should the company deploy this
		solution?<br /><br />A. Use Amazon <a href="#S3">S3</a> website hosting for the backup website and Route 53
		failover routing policy.<br />B. Use Amazon <a href="#S3">S3</a> website hosting for the backup website and
		Route 53 latency routing policy.<br />C. Deploy the application in another AWS Region and use ELB health checks
		for failover routing.<br />D. Deploy the application in another AWS Region and use server&#8211;side redirection
		on the primary website.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon <a href="#S3">S3</a> website
		hosting for the backup website and Route 53 failover routing policy.<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 74<br />A media company is evaluating the
		possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the maximum
		possible I/O performance for video processing, 300 TB of very durable storage for storing media content, and 900
		TB of storage to meet requirements for archival media that is not in use anymore.<br /><br />Which set of
		services should a solutions architect recommend to meet these requirements?<br /><br />A. Amazon EBS for maximum
		performance, Amazon <a href="#S3">S3</a> for durable data storage, and Amazon <a href="#S3">S3</a> Glacier for
		archival storage<br />B. Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon <a
			href="#S3">S3</a> Glacier for archival storage<br />C. Amazon <a href="#EC2">EC2</a> instance store for
		maximum performance, Amazon EFS for durable data storage, and Amazon <a href="#S3">S3</a> for archival
		storage<br />D. Amazon <a href="#EC2">EC2</a> instance store for maximum performance, Amazon <a
			href="#S3">S3</a> for durable data storage, and Amazon <a href="#S3">S3</a> Glacier for archival
		storage<br /><br /><b>Correct Answer:</b><br />A. Amazon EBS for maximum performance, Amazon <a
			href="#S3">S3</a> for durable data storage, and Amazon <a href="#S3">S3</a> Glacier for archival
		storage<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 75<br />An application is running on Amazon <a
			href="#EC2">EC2</a> instances. Sensitive information required for the application is stored in an Amazon <a
			href="#S3">S3</a> bucket. The bucket needs to be protected from internet access while only allowing services
		within the <a href="#VPC">VPC</a> access to the bucket.<br /><br />Which combination of actions should solutions
		archived take to accomplish this? (Choose two.)<br /><br />A. Create a <a href="#VPC">VPC</a> endpoint for
		Amazon <a href="#S3">S3</a>.<br />B. Enable server access logging on the bucket.<br />C. Apply a bucket policy
		to restrict access to the <a href="#S3">S3</a> endpoint.<br />D. Add an <a href="#S3">S3</a> ACL to the bucket
		that has sensitive information.<br />E. Restrict users using the <a href="#IAM">IAM</a> policy to use the
		specific bucket.<br /><br /><b>Correct Answer:</b><br />A. Create a <a href="#VPC">VPC</a> endpoint for Amazon
		<a href="#S3">S3</a>.<br />C. Apply a bucket policy to restrict access to the <a href="#S3">S3</a>
		endpoint.<br /><br />Answer Description:<br />ACL is a property at object level not at bucket level. Also by
		just adding ACL you cant let the services in <a href="#VPC">VPC</a> allow access to the bucket.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 76<br />A web application runs on Amazon <a
			href="#EC2">EC2</a> instances behind an Application Load Balancer. The application allows users to create
		custom reports of historical weather data. Generating a report can take up to 5 minutes. These
		long&#8211;running requests use many of the available incoming connections, making the system unresponsive to
		other users.<br /><br />How can a solutions architect make the system more responsive?<br /><br />A. Use Amazon
		SQS with AWS Lambda to generate reports.<br />B. Increase the idle timeout on the Application Load Balancer to 5
		minutes.<br />C. Update the client&#8211;side application code to increase its request timeout to 5
		minutes.<br />D. Publish the reports to Amazon <a href="#S3">S3</a> and use Amazon <a
			href="#CloudFront">CloudFront</a> for downloading to the user.<br /><br /><b>Correct Answer:</b><br />A. Use
		Amazon SQS with AWS Lambda to generate reports.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 77<br />A solutions architect must create a highly
		available bastion host architecture. The solution needs to be resilient within a single AWS Region and should
		require only minimal effort to maintain.<br /><br />What should the solutions architect do to meet these
		requirements?<br /><br />A. Create a Network Load Balancer backed by an Auto Scaling group with a UDP
		listener.<br />B. Create a Network Load Balancer backed by a Spot Fleet with instances in a partition placement
		group.<br />C. Create a Network Load Balancer backed by the existing servers in different Availability Zones as
		the target.<br />D. Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple
		Availability Zones as the target.<br /><br /><b>Correct Answer:</b><br />D. Create a Network Load Balancer
		backed by an Auto Scaling group with instances in multiple Availability Zones as the target.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 78<br />A three&#8211;tier web application
		processes orders from customers. The web tier consists of Amazon <a href="#EC2">EC2</a> instances behind an
		Application Load Balancer, a middle tier of three <a href="#EC2">EC2</a> instances decoupled from the web tier
		using Amazon SQS, and an Amazon DynamoDB backend. At peak times, customers who submit orders using the site have
		to wait much longer than normal to receive confirmations due to lengthy processing times. A solutions architect
		needs to reduce these processing times.<br /><br />Which action will be MOST effective in accomplishing
		this?<br /><br />A. Replace the SQS queue with Amazon Kinesis Data Firehose.<br />B. Use Amazon ElastiCache for
		Redis in front of the DynamoDB backend tier.<br />C. Add an Amazon <a href="#CloudFront">CloudFront</a>
		distribution to cache the responses for the web tier.<br />D. Use Amazon <a href="#EC2">EC2</a> Auto Scaling to
		scale out the middle tier instances based on the SQS queue depth.<br /><br /><b>Correct Answer:</b><br />D. Use
		Amazon <a href="#EC2">EC2</a> Auto Scaling to scale out the middle tier instances based on the SQS queue
		depth.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 79<br />A solutions architect is designing an
		architecture for a new application that requires low network latency and high network throughput between Amazon
		<a href="#EC2">EC2</a> instances. Which component should be included in the architectural design?<br /><br />A.
		An Auto Scaling group with Spot Instance types.<br />B. A placement group using a cluster placement
		strategy.<br />C. A placement group using a partition placement strategy.<br />D. An Auto Scaling group with
		On&#8211;Demand instance types.<br /><br /><b>Correct Answer:</b><br />B. A placement group using a cluster
		placement strategy.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 80<br />A company has global users accessing an
		application deployed in different AWS Regions, exposing public static IP addresses. The users are experiencing
		poor performance when accessing the application over the internet.<br /><br />What should a solutions architect
		recommend to reduce internet latency?<br /><br />A. Set up AWS Global Accelerator and add endpoints.<br />B. Set
		up AWS Direct Connect locations in multiple Regions.<br />C. Set up an Amazon <a
			href="#CloudFront">CloudFront</a> distribution to access an application.<br />D. Set up an Amazon Route 53
		geoproximity routing policy to route traffic.<br /><br /><b>Correct Answer:</b><br />A. Set up AWS Global
		Accelerator and add endpoints.<br /><br />Answer Description:<br />AWS Global Accelerator is a service in which
		you create accelerators to improve availability and performance of your applications for local and global users.
		Global Accelerator directs traffic to optimal endpoints over the AWS global network. This improves the
		availability and performance of your internet applications that are used by a global audience. Global
		Accelerator is a global service that supports endpoints in multiple AWS Regions, which are listed in the AWS
		Region Table.<br /><br />By default, Global Accelerator provides you with two static IP addresses that you
		associate with your accelerator. (Or, instead of using the IP addresses that Global Accelerator provides, you
		can configure these entry points to be IPv4 addresses from your own IP address ranges that you bring to Global
		Accelerator.)<br /><br />The static IP addresses are anycast from the AWS edge network and distribute incoming
		application traffic across multiple endpoint resources in multiple AWS Regions, which increases the availability
		of your applications. Endpoints can be Network Load Balancers, Application Load Balancers, <a
			href="#EC2">EC2</a> instances, or Elastic IP addresses that are located in one AWS Region or multiple
		Regions.<br /><br />CORRECT: &quot;Set up AWS Global Accelerator and add endpoints&quot; is the correct answer.
		INCORRECT: &quot;Set up AWS Direct Connect locations in multiple Regions&quot; is incorrect as this is used to
		connect from an on&#8211;premises data center to AWS. It does not improve performance for users who are not
		connected to the on&#8211;premises data center.<br /><br />INCORRECT: &quot;Set up an Amazon <a
			href="#CloudFront">CloudFront</a> distribution to access an application&quot; is incorrect as <a
			href="#CloudFront">CloudFront</a> cannot expose static public IP addresses.<br /><br />INCORRECT: &quot;Set
		up an Amazon Route 53 geoproximity routing policy to route traffic&quot; is incorrect as this does not reduce
		internet latency as well as using Global Accelerator. GA will direct users to the closest edge location and then
		use the AWS global network.<br /><br />References:<br /><br />AWS Global Accelerator > Developer Guide > What is
		AWS Global Accelerator?<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 81<br />A company wants to migrate a workload to
		AWS. The chief information security officer requires that all data be encrypted at rest when stored in the
		cloud. The company wants complete control of encryption key lifecycle management.<br /><br />The company must be
		able to immediately remove the key material and audit key usage independently of AWS CloudTrail. The chosen
		services should integrate with other storage services that will be used on AWS.<br /><br />Which services
		satisfies these security requirements?<br /><br />A. AWS CloudHSM with the CloudHSM client<br />B. AWS Key
		Management Service (AWS KMS) with AWS CloudHSM<br />C. AWS Key Management Service (AWS KMS) with an external key
		material origin<br />D. AWS Key Management Service (AWS KMS) with AWS managed customer master keys
		(CMKs)<br /><br /><b>Correct Answer:</b><br />B. AWS Key Management Service (AWS KMS) with AWS
		CloudHSM<br /><br />Answer Description:<br />Took a bit of reading. Key points in question:<br /><br />&quot;The
		company must be able to immediately remove the key material and audit key usage
		independently&quot;<br /><br />&quot;The chosen services should integrate with other storage services that will
		be used on AWS&quot; Point 1: Q: Can I use CloudHSM to store keys or encrypt data used by other AWS services?
		Ans: Yes. You can do all encryption in your CloudHSM&#8211;integrated application. In this case, AWS services
		such as Amazon <a href="#S3">S3</a> or Amazon Elastic Block Store (EBS) would only see your data
		encrypted.<br /><br />Point 2: AWS manages the hardware security module (HSM) appliance, but does not have
		access to your keys. You control and manage your own keys<br /><br />References:<br /><br />AWS CloudHSM
		features<br />AWS CloudHSM FAQs</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 82<br />A company has an application with a
		REST&#8211;based interface that allows data to be received in near&#8211;real time from a third&#8211;party
		vendor. Once received, the application processes and stores the data for further analysis.<br />The application
		is running on Amazon <a href="#EC2">EC2</a> instances.<br /><br />The third&#8211;party vendor has received many
		503 Service Unavailable Errors when sending data to the application. When the data volume spikes, the compute
		capacity reaches its maximum limit and the application is unable to process all requests.<br /><br />Which
		design should a solutions architect recommend to provide a more scalable solution?<br /><br />A. Use Amazon
		Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.<br />B. Use Amazon API
		Gateway on top of the existing application. Create a usage plan with a quota limit for the third&#8211;party
		vendor.<br />C. Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the <a
			href="#EC2">EC2</a> instances in an Auto Scaling group behind an Application Load Balancer.<br />D.
		Repackage the application as a container. Deploy the application using Amazon Elastic Container Service (Amazon
		<a href="#ECS">ECS</a>) using the <a href="#EC2">EC2</a> launch type with an Auto Scaling
		group.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon Kinesis Data Streams to ingest the data. Process the
		data using AWS Lambda functions.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 83<br />A solutions architect is working on
		optimizing a legacy document management application running on Microsoft Windows Server in an on&#8211;premises
		data center. The application stores a large number of files on a network file share. The chief information
		officer wants to reduce the on&#8211;premises data center footprint and minimize storage costs by moving
		on&#8211;premises storage to AWS.<br /><br />What should the solutions architect do to meet these
		requirements?<br /><br />A. Set up an AWS Storage Gateway file gateway.<br />B. Set up Amazon Elastic File
		System (Amazon EFS)<br />C. Set up AWS Storage Gateway as a volume gateway<br />D. Set up an Amazon Elastic
		Block Store (Amazon EBS) volume.<br /><br /><b>Correct Answer:</b><br />A. Set up an AWS Storage Gateway file
		gateway.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 84<br />A solutions architect is designing a
		hybrid application using the AWS cloud. The network between the on premises data center and AWS will use an AWS
		Direct Connect (DX) connection. The application connectivity between AWS and the on&#8211;premises data center
		must be highly resilient.<br /><br />Which DX configuration should be implemented to meet these
		requirements?<br /><br />A. Configure a DX connection with a VPN on top of it.<br />B. Configure DX connections
		at multiple DX locations.<br />C. Configure a DX connection using the most reliable DX partner.<br />D.
		Configure multiple virtual interfaces on top of a DX connection.<br /><br /><b>Correct Answer:</b><br />B.
		Configure DX connections at multiple DX locations.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 85<br />A company runs an application on Amazon <a
			href="#EC2">EC2</a> instances. The application is deployed in private subnets in three Availability Zones of
		the us&#8211;east&#8211;1 Region. The instances must be able to connect to the internet to download files. The
		company wants a design that is highly available across the Region.<br /><br />Which solution should be
		implemented to ensure that there are no disruptions to internet connectivity?<br /><br />A. Deploy a NAT
		instance in a private subnet of each Availability Zone.<br />B. Deploy a NAT gateway in a public subnet of each
		Availability Zone.<br />C. Deploy a transit gateway in a private subnet of each Availability Zone.<br />D.
		Deploy an internet gateway in a public subnet of each Availability Zone.<br /><br /><b>Correct
			Answer:</b><br />B. Deploy a NAT gateway in a public subnet of each Availability Zone.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 86<br />A company is running a two&#8211;tier
		eCommerce website using services. The current architect uses a public facing Elastic Load Balancer that sends
		traffic to Amazon <a href="#EC2">EC2</a> instances in a private subnet. The static content is hosted on <a
			href="#EC2">EC2</a> instances, and the dynamic content is retrieved from a MYSQL database. The application
		is running in the United States. The company recently started selling to users in Europe and Australia. A
		solutions architect needs to design solution so their international users have an improved browsing
		experience.<br /><br />Which solution is MOST cost&#8211;effective?<br /><br />A. Host the entire website on
		Amazon <a href="#S3">S3</a>.<br />B. Use Amazon <a href="#CloudFront">CloudFront</a> and Amazon <a
			href="#S3">S3</a> to host static images.<br />C. Increase the number of public load balancers and <a
			href="#EC2">EC2</a> instances.<br />D. Deploy the two&#8211;tier website in AWS Regions in Europe and
		Australia.<br /><br /><b>Correct Answer:</b><br />B. Use Amazon <a href="#CloudFront">CloudFront</a> and Amazon
		<a href="#S3">S3</a> to host static images.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 87<br />A company&aposs website provides users
		with downloadable historical performance reports. The website needs a solution that will scale to meet the
		company&aposs website demands globally. The solution should be cost effective, limit the provisioning of
		infrastructure resources, and provide the fastest possible response time.<br /><br />Which combination should a
		solutions architect recommend to meet these requirements?<br /><br />A. Amazon <a
			href="#CloudFront">CloudFront</a> and Amazon <a href="#S3">S3</a><br />B. AWS Lambda and Amazon
		DynamoDB<br />C. Application Load Balancer with Amazon <a href="#EC2">EC2</a> Auto Scaling<br />D. Amazon Route
		53 with internal Application Load Balancers<br /><br /><b>Correct Answer:</b><br />A. Amazon <a
			href="#CloudFront">CloudFront</a> and Amazon <a href="#S3">S3</a><br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 88<br />A company wants to deploy a shared file
		system for its .NET application servers and Microsoft SQL Server databases running on Amazon <a
			href="#EC2">EC2</a> instances with Windows Server 2016. The solution must be able to be integrated into the
		corporate Active Directory domain, be highly durable, be managed by AWS, and provide high levels of throughput
		and IOPS.<br /><br />Which solution meets these requirements?<br /><br />A. Use Amazon FSx for Windows File
		Server.<br />B. Use Amazon Elastic File System (Amazon EFS).<br />C. Use AWS Storage Gateway in file gateway
		mode.<br />D. Deploy a Windows file server on two On Demand instances across two Availability
		Zones.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon FSx for Windows File
		Server.<br /><br />References:<br /><br />Amazon FSx for Windows File Server</div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 89<br />A company that develops web applications
		has launched hundreds of Application Load Balancers (ALBs) in multiple Regions. The company wants to create an
		allow list (or the IPs of all the load balancers on its firewall device. A solutions architect is looking for a
		one&#8211;time, highly available solution to address this request, which will also help reduce the number of IPs
		that need to be allowed by the firewall.<br /><br />What should the solutions architect recommend to meet these
		requirements?<br /><br />A. Create a AWS Lambda function to keep track of the IPs for all the ALBs in different
		Regions. Keep refreshing this list.<br />B. Set up a Network Load Balancer (NLB) with Elastic IPs. Register the
		private IPs of all the ALBs as targets to this NLB.<br />C. Launch AWS Global Accelerator and create endpoints
		for all the Regions. Register all the ALBs in different Regions to the corresponding endpoints.<br />D. Set up
		an Amazon <a href="#EC2">EC2</a> instance, assign an Elastic IP to this <a href="#EC2">EC2</a> instance, and
		configure the instance as a proxy to forward traffic to all the ALBs.<br /><br /><b>Correct Answer:</b><br />C.
		Launch AWS Global Accelerator and create endpoints for all the Regions. Register all the ALBs in different
		Regions to the corresponding endpoints.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 90<br />A company is planning to migrate its
		virtual server&#8211;based workloads to AWS. The company has internet facing load balancers backed by
		application servers. The application servers rely on patches from an internet&#8211;hosted
		repository.<br /><br />Which services should a solutions architect recommend be hosted on the public subnet?
		(Choose two.)<br /><br />A. NAT gateway<br />B. Amazon RDS DB instances<br />C. Application Load
		Balancers<br />D. Amazon <a href="#EC2">EC2</a> application servers<br />E. Amazon Elastic File System (Amazon
		EFS) volumes<br /><br /><b>Correct Answer:</b><br />A. NAT gateway<br />C. Application Load
		Balancers<br /><br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 91<br />A company has established a new AWS
		account. The account is newly provisioned and no changed have been made to the default settings. The company is
		concerned about the security of the AWS account root user.<br /><br />What should be done to secure the root
		user?<br /><br />A. Create <a href="#IAM">IAM</a> users for daily administrative tasks. Disable the root
		user.<br />B. Create <a href="#IAM">IAM</a> users for daily administrative tasks. Enable multi&#8211;factor
		authentication on the root user.<br />C. Generate an access key for the root user. Use the access key for daily
		administration tasks instead of the AWS Management Console.<br />D. Provide the root user credentials to the
		most senior solutions architect. Have the solutions architect use the root user for daily administration
		tasks.<br /><br /><b>Correct Answer:</b><br />B. Create <a href="#IAM">IAM</a> users for daily administrative
		tasks. Enable multi&#8211;factor authentication on the root user.<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 92<br />A company requires a durable backup
		storage solution for its on&#8211;premises database servers while ensuring on&#8211;premises applications
		maintain access to these backups for quick recovery. The company will use AWS storage services as the
		destination for these backups. A solutions architect is designing a solution with minimal operational
		overhead.<br /><br />Which solution should the solutions architect implement?<br /><br />A. Deploy an AWS
		Storage Gateway file gateway on&#8211;premises and associate it with an Amazon <a href="#S3">S3</a>
		bucket.<br />B. Back up the databases to an AWS Storage Gateway volume gateway and access it using the Amazon <a
			href="#S3">S3</a> API.<br />C. Transfer the database backup files to an Amazon Elastic Block Store (Amazon
		EBS) volume attached to an Amazon <a href="#EC2">EC2</a> instance.<br />D. Back up the database directly to an
		AWS Snowball device and use lifecycle rules to move the data to Amazon <a href="#S3">S3</a> Glacier Deep
		Archive.<br /><br /><b>Correct Answer:</b><br />A. Deploy an AWS Storage Gateway file gateway on&#8211;premises
		and associate it with an Amazon <a href="#S3">S3</a> bucket.<br /><br />Answer Description:<br />Network Load
		Balancer overview<br /><br />A Network Load Balancer functions at the fourth layer of the Open Systems
		Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a
		connection request, it selects a target from the target group for the default rule. It attempts to open a TCP
		connection to the selected target on the port specified in the listener configuration.<br /><br />When you
		enable an Availability Zone for the load balancer, Elastic Load Balancing creates a load balancer node in the
		Availability Zone. By default, each load balancer node distributes traffic across the registered targets in its
		Availability Zone only. If you enable cross&#8211;zone load balancing, each load balancer node distributes
		traffic across the registered targets in all enabled Availability Zones. For more information, see Availability
		Zones.<br /><br />If you enable multiple Availability Zones for your load balancer and ensure that each target
		group has at least one target in each enabled Availability Zone, this increases the fault tolerance of your
		applications. For example, if one or more target groups does not have a healthy target in an Availability Zone,
		we remove the IP address for the corresponding subnet from DNS, but the load balancer nodes in the other
		Availability Zones are still available to route traffic. If a client doesn&apost honor the
		time&#8211;to&#8211;live (TTL) and sends requests to the IP address after it is removed from DNS, the requests
		fail.<br /><br />For TCP traffic, the load balancer selects a target using a flow hash algorithm based on the
		protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number. The
		TCP connections from a client have different source ports and sequence numbers, and can be routed to different
		targets. Each individual TCP connection is routed to a single target for the life of the
		connection.<br /><br />For UDP traffic, the load balancer selects a target using a flow hash algorithm based on
		the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the
		same source and destination, so it is consistently routed to a single target throughout its lifetime. Different
		UDP flows have different source IP addresses and ports, so they can be routed to different
		targets.<br /><br />An Auto Scaling group contains a collection of Amazon <a href="#EC2">EC2</a> instances that
		are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group
		also enables you to use Amazon <a href="#EC2">EC2</a> Auto Scaling features such as health check replacements
		and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling
		are the core functionality of the Amazon <a href="#EC2">EC2</a> Auto Scaling service.<br /><br />The size of an
		Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its
		size to meet demand, either manually or by using automatic scaling.<br /><br />An Auto Scaling group starts by
		launching enough instances to meet its desired capacity. It maintains this number of instances by performing
		periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed
		number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group
		terminates the unhealthy instance and launches another instance to replace it.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 93<br />A company decides to migrate its
		three&#8211;tier web application from on&#8211;premises to the AWS Cloud. The new database must be capable of
		dynamically scaling storage capacity and performing table joins.<br /><br />Which AWS service meets these
		requirements?<br /><br />A. Amazon Aurora<br />B. Amazon RDS for SqlServer<br />C. Amazon DynamoDB
		Streams<br />D. Amazon DynamoDB on&#8211;demand<br /><br /><b>Correct Answer:</b><br />A. Amazon Aurora<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 94<br />A company is using a <a
			href="#VPC">VPC</a> peering strategy to connect its <a href="#VPC">VPC</a>s in a single Region to allow for
		cross communication.<br /><br />A recent increase in account creations and <a href="#VPC">VPC</a>s has made it
		difficult to maintain the <a href="#VPC">VPC</a> peering strategy, and the company expects to grow to hundreds
		of <a href="#VPC">VPC</a>s. There are also new requests to create site&#8211;to&#8211;site VPNs with some of the
		<a href="#VPC">VPC</a>s. A solutions architect has been tasked with creating a centrally managed networking
		setup for multiple accounts, <a href="#VPC">VPC</a>s, and VPNs.<br /><br />Which networking solution meets these
		requirements?<br /><br />A. Configure shared <a href="#VPC">VPC</a>s and VPNs and share to each other.<br />B.
		Configure a hub&#8211;and&#8211;spoke <a href="#VPC">VPC</a> and route all traffic through <a
			href="#VPC">VPC</a> peering.<br />C. Configure an AWS Direct Connect connection between all <a
			href="#VPC">VPC</a>s and VPNs.<br />D. Configure a transit gateway with AWS Transit Gateway and connect all
		<a href="#VPC">VPC</a>s and VPNs.<br /><br /><b>Correct Answer:</b><br />D. Configure a transit gateway with AWS
		Transit Gateway and connect all <a href="#VPC">VPC</a>s and VPNs.<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 95<br />A solutions architect is helping a
		developer design a new eCommerce shopping cart application using AWS services. The developer is unsure of the
		current database schema and expects to make changes as the eCommerce site grows. The solution needs to be highly
		resilient and capable of automatically scaling read and write capacity.<br /><br />Which database solution meets
		these requirements?<br /><br />A. Amazon Aurora PostgreSQL<br />B. Amazon DynamoDB with on&#8211;demand
		enabled<br />C. Amazon DynamoDB with DynamoDB Streams enabled<br />D. Amazon SQS and Amazon Aurora
		PostgreSQL<br /><br /><b>Correct Answer:</b><br />B. Amazon DynamoDB with on&#8211;demand
		enabled<br /><br />References:<br /><br />Anúncio do Amazon DynamoDB sob demanda</div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 96<br />A solutions architect must migrate a
		Windows internet information Services (IIS) web application to AWS.<br /><br />The application currently relies
		on a file share hosted in the user&aposs on&#8211;premises network&#8211;attached storage (NAS). The solutions
		architected has proposed migrating the IIS web servers.<br /><br />Which replacement to the on&#8211;premises
		file share is MOST resilient and durable?<br /><br />A. Migrate the file Share to Amazon RDS.<br />B. Migrate
		the file Share to AWS Storage Gateway<br />C. Migrate the file Share to Amazon FSx for Windows File
		Server.<br />D. Migrate the file share to Amazon Elastic File System (Amazon EFS)<br /><br /><b>Correct
			Answer:</b><br />C. Migrate the file Share to Amazon FSx for Windows File Server.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 97<br />A company needs to implement a relational
		database with a multi&#8211;Region disaster recovery Recovery Point Objective (RPO) of 1 second and a Recovery
		Time Objective (RTO) of 1 minute.<br /><br />Which AWS solution can achieve this?<br /><br />A. Amazon Aurora
		Global Database<br />B. Amazon DynamoDB global tables<br />C. Amazon RDS for MySQL with Multi&#8211;AZ
		enabled<br />D. Amazon RDS for MySQL with a cross&#8211;Region snapshot copy<br /><br /><b>Correct
			Answer:</b><br />A. Amazon Aurora Global Database<br /><br />Answer Description:<br />Cross&#8211;Region
		Disaster Recovery: If your primary region suffers a performance degradation or outage, you can promote one of
		the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute
		even in the event of a complete regional outage. This provides your application with an effective Recovery Point
		Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong
		foundation for a global business continuity plan.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 98<br />A company is reviewing its AWS Cloud
		deployment to ensure its data is not accessed by anyone without appropriate authorization. A solutions architect
		is tasked with identifying all open Amazon <a href="#S3">S3</a> buckets and recording any <a href="#S3">S3</a>
		bucket configuration changes.<br /><br />What should the solutions architect do to accomplish
		this?<br /><br />A. Enable AWS Config service with the appropriate rules<br />B. Enable AWS Trusted Advisor with
		the appropriate checks.<br />C. Write a script using an AWS SDK to generate a bucket report<br />D. Enable
		Amazon <a href="#S3">S3</a> server access logging and configure Amazon CloudWatch Events.<br /><br /><b>Correct
			Answer:</b><br />A. Enable AWS Config service with the appropriate rules<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 99<br />A company is planning to build a new web
		application on AWS. The company expects predictable traffic most of the year and very high traffic on occasion.
		The web application needs to be highly available and fault tolerant with minimal latency.<br /><br />What should
		a solutions architect recommend to meet these requirements?<br /><br />A. Use an Amazon Route 53 routing policy
		to distribute requests to two AWS Regions, each with one Amazon <a href="#EC2">EC2</a> instance.<br />B. Use
		Amazon <a href="#EC2">EC2</a> instances in an Auto Scaling group with an Application Load Balancer across
		multiple Availability Zones.<br />C. Use Amazon <a href="#EC2">EC2</a> instances in a cluster placement group
		with an Application Load Balancer across multiple Availability Zones.<br />D. Use Amazon <a href="#EC2">EC2</a>
		instances in a cluster placement group and include the cluster placement group within a new Auto Scaling
		group.<br /><br /><b>Correct Answer:</b><br />B. Use Amazon <a href="#EC2">EC2</a> instances in an Auto Scaling
		group with an Application Load Balancer across multiple Availability Zones.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 100<br />A solutions architect has configured the
		following <a href="#IAM">IAM</a> policy.<br /><br />A solutions architect has configured the following <a
			href="#IAM">IAM</a> policy.<br /><br />A solutions architect has configured the following <a
			href="#IAM">IAM</a> policy.<br /><br />Which action will be allowed by the policy?<br /><br />A. An AWS
		Lambda function can be deleted from any network.<br />B. An AWS Lambda function can be created from any
		network.<br />C. An AWS Lambda function can be deleted from the 100.220.0.0/20 network.<br />D. An AWS Lambda
		function can be deleted from the 220.100.16.0/20 network.<br /><br /><b>Correct Answer:</b><br />C. An AWS
		Lambda function can be deleted from the 100.220.0.0/20 network.<br /><br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 101<br />A solutions architect is using Amazon <a
			href="#S3">S3</a> to design the storage architecture of a new digital media application. The media files
		must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are
		rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and
		retrieving the media files.<br /><br />Which storage option meets these requirements?<br /><br />A. <a
			href="#S3">S3</a> Standard<br />B. <a href="#S3">S3</a> Intelligent&#8211;Tiering<br />C. <a
			href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a> Standard&#8211;IA)<br />D. <a
			href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />B. <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br /><br />Answer Description:<br /><a href="#S3">S3</a> Intelligent&#8211;Tiering is
		a new Amazon <a href="#S3">S3</a> storage class designed for customers who want to optimize storage costs
		automatically when data access patterns change, without performance impact or operational overhead. <a
			href="#S3">S3</a> Intelligent&#8211;Tiering is the first cloud object storage class that delivers automatic
		cost savings by moving data between two access tiers &#8212; frequent access and infrequent access &#8212; when
		access patterns change, and is ideal for data with unknown or changing access patterns.<br /><br /><a
			href="#S3">S3</a> Intelligent&#8211;Tiering stores objects in two access tiers: one tier that is optimized
		for frequent access and another lower&#8211;cost tier that is optimized for infrequent access. For a small
		monthly monitoring and automation fee per object, <a href="#S3">S3</a> Intelligent&#8211;Tiering monitors access
		patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access tier.
		There are no retrieval fees in <a href="#S3">S3</a> Intelligent&#8211;Tiering. If an object in the infrequent
		access tier is accessed later, it is automatically moved back to the frequent access tier. No additional tiering
		fees apply when objects are moved between access tiers within the <a href="#S3">S3</a> Intelligent&#8211;Tiering
		storage class. <a href="#S3">S3</a> Intelligent&#8211;Tiering is designed for 99.9% availability and
		99.999999999% durability, and offers the same low latency and high throughput performance of <a
			href="#S3">S3</a> Standard.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 102<br />A company is running a three&#8211;tier
		web application to process credit card payments. The front&#8211;end user interface consists of static webpages.
		The application tier can have long&#8211;running processes. The database tier uses MySQL.<br /><br />The
		application is currently running on a single, general&#8211;purpose large Amazon <a href="#EC2">EC2</a>
		instance. A solutions architect needs to decouple the services to make the web application highly
		available.<br /><br />Which solution would provide the HIGHEST availability?<br /><br />A. Move static assets to
		Amazon <a href="#CloudFront">CloudFront</a>. Leave the application in <a href="#EC2">EC2</a> in an Auto Scaling
		group. Move the database to Amazon RDS to deploy Multi&#8211;AZ.<br />B. Move static assets and the application
		into a medium <a href="#EC2">EC2</a> instance. Leave the database on the large instance. Place both instances in
		an Auto Scaling group.<br />C. Move static assets to Amazon <a href="#S3">S3</a>, Move the application to AWS
		Lambda with the concurrency limit set. Move the database to Amazon DynamoDB with on&#8211;demand
		enabled.<br />D. Move static assets to Amazon <a href="#S3">S3</a>. Move the application to Amazon Elastic
		Container Service (Amazon <a href="#ECS">ECS</a>) containers with Auto Scaling enabled. Move the database to
		Amazon RDS to deploy Multi&#8211;AZ.<br /><br /><b>Correct Answer:</b><br />B. Move static assets and the
		application into a medium <a href="#EC2">EC2</a> instance. Leave the database on the large instance. Place both
		instances in an Auto Scaling group.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 103<br />A solutions architect is designing the
		cloud architecture for a new application being deployed to AWS. The application allows users to interactively
		download and upload files. Files older than 2 years will be accessed less frequently. The solutions architect
		needs to ensure that the application can scale to any number of files while maintaining high availability and
		durability.<br /><br />Which scalable solutions should the solutions architect recommend? (Choose
		two.)<br /><br />A. Store the files on Amazon <a href="#S3">S3</a> with a lifecycle policy that moves objects
		older than 2 years to <a href="#S3">S3</a> Glacier.<br />B. Store the files on Amazon <a href="#S3">S3</a> with
		a lifecycle policy that moves objects older than 2 years to <a href="#S3">S3</a> Standard&#8211;Infrequent
		Access (<a href="#S3">S3</a> Standard&#8211;IA)<br />C. Store the files on Amazon Elastic File System (Amazon
		EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent Access (EFS IA).<br />D.
		Store the files in Amazon Elastic Block Store (Amazon EBS) volumes. Schedule snapshots of the volumes. Use the
		snapshots to archive data older than 2 years.<br />E. Store the files in RAID&#8211;striped Amazon Elastic Block
		Store (Amazon EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2
		years.<br /><br /><b>Correct Answer:</b><br />A. Store the files on Amazon <a href="#S3">S3</a> with a lifecycle
		policy that moves objects older than 2 years to <a href="#S3">S3</a> Glacier.<br />C. Store the files on Amazon
		Elastic File System (Amazon EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent
		Access (EFS IA).<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 104<br />A company has recently updated its
		internal security standards. The company must now ensure all Amazon <a href="#S3">S3</a> buckets and Amazon
		Elastic Block Store (Amazon EBS) volumes are encrypted with keys created and periodically rotated by internal
		security specialists. The company is looking for a native, software&#8211;based AWS service to accomplish this
		goal.<br /><br />What should a solutions architect recommend as a solution?<br /><br />A. Use AWS Secrets
		Manager with customer master keys (CMKs) to store master key material and apply a routine to create a new CMK
		periodically and replace it in AWS Secrets Manager.<br />B. Use AWS Key Management Service (AWS KMS) with
		customer master keys (CMKs) to store master key material and apply a routine to re&#8211;create a new key
		periodically and replace it in AWS KMS.<br />C. Use an AWS CloudHSM cluster with customer master keys (CMKs) to
		store master key material and apply a routine to re&#8211;create a new key periodically and replace it in the
		CloudHSM cluster nodes.<br />D. Use AWS Systems Manager Parameter Store with customer master keys (CMKs) to
		store master key material and apply a routine to re&#8211;create a new key periodically and replace it in the
		Parameter Store.<br /><br /><b>Correct Answer:</b><br />A. Use AWS Secrets Manager with customer master keys
		(CMKs) to store master key material and apply a routine to create a new CMK periodically and replace it in AWS
		Secrets Manager.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 105<br />A company&aposs dynamic website is hosted
		using on&#8211;premises servers in the United States. The company is launching its product in Europe, and it
		wants to optimize site loading times for new European users. The site&aposs backend must remain in the United
		States. The product is being launched in a few days, and an immediate solution is needed.<br /><br />What should
		the solutions architect recommend?<br /><br />A. Launch an Amazon <a href="#EC2">EC2</a> instance in
		us&#8211;east&#8211;1 and migrate the site to it.<br />B. Move the website to Amazon <a href="#S3">S3</a>. Use
		cross&#8211;Region replication between Regions.<br />C. Use Amazon <a href="#CloudFront">CloudFront</a> with a
		custom origin pointing to the on&#8211;premises servers.<br />D. Use an Amazon Route 53 geo&#8211;proximity
		routing policy pointing to on&#8211;premises servers.<br /><br /><b>Correct Answer:</b><br />C. Use Amazon <a
			href="#CloudFront">CloudFront</a> with a custom origin pointing to the on&#8211;premises servers.<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 106<br />A company is hosting multiple websites
		for several lines of business under its registered parent domain.<br /><br />Users accessing these websites will
		be routed to appropriate backend Amazon <a href="#EC2">EC2</a> instances based on the subdomain. The websites
		host static webpages, images, and server&#8211;side scripts like PHP and JavaScript. Some of the websites
		experience peak access during the first two hours of business with constant usage throughout the rest of the
		day. A solutions architect needs to design a solution that will automatically adjust capacity to these traffic
		patterns while keeping costs low.<br /><br />Which combination of AWS services or features will meet these
		requirements? (Choose two.)<br /><br />A. AWS Batch<br />B. Network Load Balancer<br />C. Application Load
		Balancer<br />D. Amazon <a href="#EC2">EC2</a> Auto Scaling<br />E. Amazon <a href="#S3">S3</a> website
		hosting<br /><br /><b>Correct Answer:</b><br />C. Application Load Balancer<br />D. Amazon <a
			href="#EC2">EC2</a> Auto Scaling<br /><br />References:<br /><br />Amazon Simple Storage Service > User
		Guide > Hosting a static website using Amazon <a href="#S3">S3</a></div><a href="#All">All(200)</a> <a href="#"
		<i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 107<br />A company uses an Amazon <a
			href="#S3">S3</a> bucket to store static images for its website. The company configured permissions to allow
		access to Amazon <a href="#S3">S3</a> objects by privileged users only.<br /><br />What should a solutions
		architect do to protect against data loss? (Choose two.)<br /><br />A. Enable versioning on the <a
			href="#S3">S3</a> bucket.<br />B. Enable access logging on the <a href="#S3">S3</a> bucket.<br />C. Enable
		server&#8211;side encryption on the <a href="#S3">S3</a> bucket.<br />D. Configure an <a href="#S3">S3</a>
		lifecycle rule to transition objects to Amazon <a href="#S3">S3</a> Glacier.<br />E. Use MFA Delete to require
		multi&#8211;factor authentication to delete an object.<br /><br /><b>Correct Answer:</b><br />A. Enable
		versioning on the <a href="#S3">S3</a> bucket.<br />E. Use MFA Delete to require multi&#8211;factor
		authentication to delete an object.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 108<br />An operations team has a standard that
		states <a href="#IAM">IAM</a> policies should not be applied directly to users. Some new team members have not
		been following this standard. The operations manager needs a way to easily identify the users with attached
		policies.<br /><br />What should a solutions architect do to accomplish this?<br /><br />A. Monitor using AWS
		CloudTrail.<br />B. Create an AWS Config rule to run daily.<br />C. Publish <a href="#IAM">IAM</a> user changes
		to Amazon SNS.<br />D. Run AWS Lambda when a user is modified.<br /><br /><b>Correct Answer:</b><br />B. Create
		an AWS Config rule to run daily.<br /><br />Answer Description:<br />A new AWS Config rule is deployed in the
		account after you enable AWS Security Hub. The AWS Config rule reacts to resource configuration and compliance
		changes and send these change items to AWS CloudWatch. When AWS CloudWatch receives the compliance change, a
		CloudWatch event rule triggers the AWS Lambda function.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 109<br />A company hosts its website on AWS. To
		address the highly variable demand, the company has implemented Amazon <a href="#EC2">EC2</a> Auto
		Scaling.<br /><br />Management is concerned that the company is over&#8211;provisioning its infrastructure,
		especially at the front end of the three&#8211;tier application. A solutions architect needs to ensure costs are
		optimized without impacting performance.<br /><br />What should the solutions architect do to accomplish
		this?<br /><br />A. Use Auto Scaling with Reserved Instances.<br />B. Use Auto Scaling with a scheduled scaling
		policy.<br />C. Use Auto Scaling with the suspend&#8211;resume feature.<br />D. Use Auto Scaling with a target
		tracking scaling policy.<br /><br /><b>Correct Answer:</b><br />D. Use Auto Scaling with a target tracking
		scaling policy.<br /><br />References:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling > User Guid >
		Target tracking scaling policies for Amazon <a href="#EC2">EC2</a> Auto Scaling</div><a href="#All">All(200)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 110<br />A solutions architect is performing a
		security review of a recently migrated workload. The workload is a web application that consists of Amazon <a
			href="#EC2">EC2</a> instances in an Auto Scaling group behind an Application Load Balancer.<br /><br />The
		solution architect must improve the security posture and minimize the impact of a DDoS attack on
		resources.<br /><br />Which solution is MOST effective?<br /><br />A. Configure an AWS WAF ACL with
		rate&#8211;based rules. Create an Amazon <a href="#CloudFront">CloudFront</a> distribution that points to the
		Application Load Balancer. Enable the WAF ACL on the <a href="#CloudFront">CloudFront</a> distribution.<br />B.
		Create a custom AWS Lambda function that adds identified attacks into a common vulnerability pool to capture a
		potential DDoS attack. Use the identified information to modify a network ACL to block access.<br />C. Enable <a
			href="#VPC">VPC</a> Flow Logs and store them in Amazon <a href="#S3">S3</a>. Create a custom AWS Lambda
		functions that parses the logs looking for a DDoS attack. Modify a network ACL to block identified source IP
		addresses.<br />D. Enable Amazon GuardDuty and configure findings written to Amazon CloudWatch. Create an event
		with CloudWatch Events for DDoS alerts that triggers Amazon Simple Notification Service (Amazon SNS). Have
		Amazon SNS invoke a custom AWS Lambda function that parses the logs, looking for a DDoS attack. Modify a network
		ACL to block identified source IP addresses.<br /><br /><b>Correct Answer:</b><br />B. Create a custom AWS
		Lambda function that adds identified attacks into a common vulnerability pool to capture a potential DDoS
		attack. Use the identified information to modify a network ACL to block access.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 111<br />A company has multiple AWS accounts for
		various departments. One of the departments wants to share an Amazon <a href="#S3">S3</a> bucket with all other
		departments.<br /><br />Which solution will require the LEAST amount of effort?<br /><br />A. Enable
		cross&#8211;account <a href="#S3">S3</a> replication for the bucket.<br />B. Create a pre&#8211;signed URL for
		the bucket and share it with other departments.<br />C. Set the <a href="#S3">S3</a> bucket policy to allow
		cross&#8211;account access to other departments.<br />D. Create <a href="#IAM">IAM</a> users for each of the
		departments and configure a read&#8211;only <a href="#IAM">IAM</a> policy.<br /><br /><b>Correct
			Answer:</b><br />C. Set the <a href="#S3">S3</a> bucket policy to allow cross&#8211;account access to other
		departments.<br /><br />Answer Description:<br /><a href="#S3">S3</a> standard is the best choice in this
		scenario for a short term storage solution. In this case the size and number of logs is unknown and it would be
		difficult to fully assess the access patterns at this stage. Therefore, using <a href="#S3">S3</a> standard is
		best as it is cost&#8211;effective, provides immediate access, and there are no retrieval fees or minimum
		capacity charge per object.<br /><br />CORRECT: &quot;Amazon <a href="#S3">S3</a> Standard&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Amazon <a href="#S3">S3</a> Intelligent&#8211;Tiering&quot; is incorrect as
		there is an additional fee for using this service and for a short&#8211;term requirement it may not be
		beneficial.<br /><br />INCORRECT: &quot;Amazon <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a
			href="#S3">S3</a> One Zone&#8211;IA)&quot; is incorrect as this storage class has a minimum capacity charge
		per object (128 KB) and a per GB retrieval fee. INCORRECT: &quot;Amazon <a href="#S3">S3</a> Glacier Deep
		Archive&quot; is incorrect as this storage class is used for archiving data. There are retrieval fees and it
		take hours to retrieve data from an archive.<br /><br />References:<br /><br />Amazon <a href="#S3">S3</a>
		Storage Classes</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 112<br />A company needs to share an Amazon <a
			href="#S3">S3</a> bucket with an external vendor. The bucket owner must be able to access all
		objects.<br /><br />Which action should be taken to share the <a href="#S3">S3</a> bucket?<br /><br />A. Update
		the bucket to be a Requester Pays bucket.<br />B. Update the bucket to enable cross&#8211;origin resource
		sharing (CORS).<br />C. Create a bucket policy to require users to grant
		bucket&#8211;owner&#8211;full&#8211;control when uploading objects.<br />D. Create an <a href="#IAM">IAM</a>
		policy to require users to grant bucket&#8211;owner&#8211;full&#8211;control when uploading
		objects.<br /><br /><b>Correct Answer:</b><br />C. Create a bucket policy to require users to grant
		bucket&#8211;owner&#8211;full&#8211;control when uploading objects.<br /><br />Answer Description:<br />By
		default, an <a href="#S3">S3</a> object is owned by the AWS account that uploaded it. This is true even when the
		bucket is owned by another account. To get access to the object, the object owner must explicitly grant you (the
		bucket owner) access. The object owner can grant the bucket owner full control of the object by updating the
		access control list (ACL) of the object. The object owner can update the ACL either during a put or copy
		operation, or after the object is added to the bucket.<br /><br />Resolution Add a bucket policy that grants
		users access to put objects in your bucket only when they grant you (the bucket owner) full control of the
		object.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 113<br />A company has a custom application
		running on an Amazon EC instance that:<br /><br />Reads a large amount of data from Amazon <a
			href="#S3">S3</a><br />Performs a multi&#8211;stage analysis<br />Writes the results to Amazon
		DynamoDB<br />The application writes a significant number of large, temporary files during the multi&#8211;stage
		analysis. The process performance depends on the temporary storage performance.<br /><br />What would be the
		fastest storage option for holding the temporary files?<br /><br />A. Multiple Amazon <a href="#S3">S3</a>
		buckets with Transfer Acceleration for storage.<br />B. Multiple Amazon EBS drives with Provisioned IOPS and EBS
		optimization.<br />C. Multiple Amazon EFS volumes using the Network File System version 4.1 (NFSv4.1)
		protocol.<br />D. Multiple instance store volumes with software RAID 0.<br /><br /><b>Correct
			Answer:</b><br />A. Multiple Amazon <a href="#S3">S3</a> buckets with Transfer Acceleration for
		storage.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 114<br />A leasing company generates and emails
		PDF statements every month for all its customers. Each statement is about 400 KB in size.<br /><br />Customers
		can download their statements from the website for up to 30 days from when the statements were generated. At the
		end of their 3&#8211;year lease, the customers are emailed a ZIP file that contains all the
		statements.<br /><br />What is the MOST cost&#8211;effective storage solution for this situation?<br /><br />A.
		Store the statements using the Amazon <a href="#S3">S3</a> Standard storage class. Create a lifecycle policy to
		move the statements to Amazon <a href="#S3">S3</a> Glacier storage after 1 day.<br />B. Store the statements
		using the Amazon <a href="#S3">S3</a> Glacier storage class. Create a lifecycle policy to move the statements to
		Amazon <a href="#S3">S3</a> Glacier Deep Archive storage after 30 days.<br />C. Store the statements using the
		Amazon <a href="#S3">S3</a> Standard storage class. Create a lifecycle policy to move the statements to Amazon
		<a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One Zone&#8211;IA) storage after 30
		days.<br />D. Store the statements using the Amazon <a href="#S3">S3</a> Standard&#8211;Infrequent Access (<a
			href="#S3">S3</a> Standard&#8211;IA) storage class. Create a lifecycle policy to move the statements to
		Amazon <a href="#S3">S3</a> Glacier storage after 30 days.<br /><br /><b>Correct Answer:</b><br />B. Store the
		statements using the Amazon <a href="#S3">S3</a> Glacier storage class. Create a lifecycle policy to move the
		statements to Amazon <a href="#S3">S3</a> Glacier Deep Archive storage after 30 days.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 115<br />A company recently released a new type of
		internet&#8211;connected sensor. The company is expecting to sell thousands of sensors, which are designed to
		stream high volumes of data each second to a central location. A solutions architect must design a solution that
		ingests and stores data so that engineering teams can analyze it in near&#8211;real&#8211;time with millisecond
		responsiveness.<br /><br />Which solution should the solutions architect recommend?<br /><br />A. Use an Amazon
		SQS queue to ingest the data. Consume the data with an AWS Lambda function, which then stores the data in Amazon
		Redshift.<br />B. Use an Amazon SQS queue to ingest the data. Consume the data with an AWS Lambda function,
		which then stores the data in Amazon DynamoDB.<br />C. Use Amazon Kinesis Data Streams to ingest the data.
		Consume the data with an AWS Lambda function, which then stores the data in Amazon Redshift.<br />D. Use Amazon
		Kinesis Data Streams to ingest the data. Consume the data with an AWS Lambda function, which then stores the
		data in Amazon DynamoDB.<br /><br /><b>Correct Answer:</b><br />D. Use Amazon Kinesis Data Streams to ingest the
		data. Consume the data with an AWS Lambda function, which then stores the data in Amazon
		DynamoDB.<br /><br />References:<br /><br />AWS Big Data Blog > Analyze data in Amazon DynamoDB using Amazon
		SageMaker for real&#8211;time prediction<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 116<br />A website runs a web application that
		receives a burst of traffic each day at noon. The users upload new pictures and content daily, but have been
		complaining of timeouts. The architecture uses Amazon <a href="#EC2">EC2</a> Auto Scaling groups, and the custom
		application consistently takes 1 minute to initiate upon boot up before responding to user
		requests.<br /><br />How should a solutions architect redesign the architecture to better respond to changing
		traffic?<br /><br />A. Configure a Network Load Balancer with a slow start configuration.<br />B. Configure AWS
		ElastiCache for Redis to offload direct requests to the servers.<br />C. Configure an Auto Scaling step scaling
		policy with an instance warmup condition.<br />D. Configure Amazon <a href="#CloudFront">CloudFront</a> to use
		an Application Load Balancer as the origin.<br /><br /><b>Correct Answer:</b><br />C. Configure an Auto Scaling
		step scaling policy with an instance warmup condition.<br /><br />Answer Description:<br />If you are creating a
		step policy, you can specify the number of seconds that it takes for a newly launched instance to warm up. Until
		its specified warm&#8211;up time has expired, an instance is not counted toward the aggregated metrics of the
		Auto Scaling group. Using the example in the Step Adjustments section, suppose that the metric gets to 60, and
		then it gets to 62 while the new instance is still warming up. The current capacity is still 10 instances, so 1
		instance is added (10 percent of 10 instances). However, the desired capacity of the group is already 11
		instances, so the scaling policy does not increase the desired capacity further. If the metric gets to 70 while
		the new instance is still warming up, we should add 3 instances (30 percent of 10 instances). However, the
		desired capacity of the group is already 11, so we add only 2 instances, for a new desired capacity of 13
		instances.<br /><br />References:<br />Amazon <a href="#EC2">EC2</a> Auto Scaling > User Guide > Step and simple
		scaling policies for Amazon <a href="#EC2">EC2</a> Auto Scaling<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 117<br />A company has an application that posts
		messages to Amazon SQS. Another application polls the queue and processes the messages in an I/O&#8211;intensive
		operation. The company has a service level agreement (SLA) that specifies the maximum amount of time that can
		elapse between receiving the messages and responding to the users. Due to an increase in the number of messages,
		the company has difficulty meeting its SLA consistently.<br /><br />What should a solutions architect do to help
		improve the application&aposs processing time and ensure it can handle the load at any level?<br /><br />A.
		Create an Amazon Machine Image (AMI) from the instance used for processing. Terminate the instance and replace
		it with a larger size.<br />B. Create an Amazon Machine Image (AMI) from the instance used for processing.
		Terminate the instance and replace it with an Amazon <a href="#EC2">EC2</a> Dedicated Instance.<br />C. Create
		an Amazon Machine Image (AMI) from the instance used for processing. Create an Auto Scaling group using this
		image in its launch configuration. Configure the group with a target tracking policy to keep its aggregate CPU
		utilization below 70%.<br />D. Create an Amazon Machine Image (AMI) from the instance used for processing.
		Create an Auto Scaling group using this image in its launch configuration. Configure the group with a target
		tracking policy based on the age of the oldest message in the SQS queue.<br /><br /><b>Correct
			Answer:</b><br />D. Create an Amazon Machine Image (AMI) from the instance used for processing. Create an
		Auto Scaling group using this image in its launch configuration. Configure the group with a target tracking
		policy based on the age of the oldest message in the SQS queue.<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 118<br />A company is designing a new web service
		that will run on Amazon <a href="#EC2">EC2</a> instances behind an Elastic Load Balancer. However, many of the
		web service clients can only reach IP addresses whitelisted on their firewalls.<br /><br />What should a
		solutions architect recommend to meet the clients&apos needs?<br /><br />A. A Network Load Balancer with an
		associated Elastic IP address.<br />B. An Application Load Balancer with an associated Elastic IP
		address<br />C. An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address<br />D. An <a
			href="#EC2">EC2</a> instance with a public IP address running as a proxy in front of the load
		balancer<br /><br /><b>Correct Answer:</b><br />C. An A record in an Amazon Route 53 hosted zone pointing to an
		Elastic IP address<br /><br />Answer Description:<br />Route 53 routes end users to Internet applications so the
		correct answer is C. Map one of the whitelisted IP addresses using an A record to the Elastic IP address.<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 119<br />A company&aposs packaged application
		dynamically creates and returns single&#8211;use text files in response to user requests. The company is using
		Amazon <a href="#CloudFront">CloudFront</a> for distribution, but wants to further reduce data transfer costs.
		The company cannot modify the application&aposs source code.<br /><br />What should a solutions architect do to
		reduce costs?<br /><br />A. Use Lambda@Edge to compress the files as they are sent to users.<br />B. Enable
		Amazon <a href="#S3">S3</a> Transfer Acceleration to reduce the response times.<br />C. Enable caching on the <a
			href="#CloudFront">CloudFront</a> distribution to store generated files at the edge.<br />D. Use Amazon <a
			href="#S3">S3</a> multipart uploads to move the files to Amazon <a href="#S3">S3</a> before returning them
		to users.<br /><br /><b>Correct Answer:</b><br />A. Use Lambda@Edge to compress the files as they are sent to
		users.<br /><br />Answer Description:<br />B seems more expensive; C does not seem right because they are single
		use files and will not be needed again from the cache; D multipart mainly for large files and will not reduce
		data and cost; A seems the best: change the application code to compress the files and reduce the amount of data
		transferred to save costs.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 120<br />A company is planning to deploy an Amazon
		RDS DB instance running Amazon Aurora. The company has a backup retention policy requirement of 90 days. Which
		solution should a solutions architect recommend?<br /><br />A. Set the backup retention period to 90 days when
		creating the RDS DB instance.<br />B. Configure RDS to copy automated snapshots to a user&#8211;managed Amazon
		<a href="#S3">S3</a> bucket with a lifecycle policy set to delete after 90 days.<br />C. Create an AWS Backup
		plan to perform a daily snapshot of the RDS database with the retention set to 90 days. Create an AWS Backup job
		to schedule the execution of the backup plan daily.<br />D. Use a daily scheduled event with Amazon CloudWatch
		Events to execute a custom AWS Lambda function that makes a copy of the RDS automated snapshot. Purge snapshots
		older than 90 days.<br /><br /><b>Correct Answer:</b><br />B. Configure RDS to copy automated snapshots to a
		user&#8211;managed Amazon <a href="#S3">S3</a> bucket with a lifecycle policy set to delete after 90 days.<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 121<br />A company currently has 250 TB of backup
		files stored in Amazon <a href="#S3">S3</a> in a vendor&aposs proprietary format.<br /><br />Using a
		Linux&#8211;based software application provided by the vendor, the company wants to retrieve files from Amazon
		<a href="#S3">S3</a>, transform the files to an industry&#8211;standard format, and re&#8211;upload them to
		Amazon <a href="#S3">S3</a>. The company wants to minimize the data transfer charges associated with this
		conversation.<br /><br />What should a solutions architect do to accomplish this?<br /><br />A. Install the
		conversion software as an Amazon <a href="#S3">S3</a> batch operation so the data is transformed without leaving
		Amazon <a href="#S3">S3</a>.<br />B. Install the conversion software onto an on&#8211;premises virtual machine.
		Perform the transformation and reupload the files to Amazon <a href="#S3">S3</a> from the virtual
		machine.<br />C. Use AWS Snowball Edge devices to export the data and install the conversion software onto the
		devices. Perform the data transformation and re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the
		Snowball Edge devices.<br />D. Launch an Amazon <a href="#EC2">EC2</a> instance in the same Region as Amazon <a
			href="#S3">S3</a> and install the conversion software onto the instance. Perform the transformation and
		re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the <a href="#EC2">EC2</a>
		instance.<br /><br /><b>Correct Answer:</b><br />D. Launch an Amazon <a href="#EC2">EC2</a> instance in the same
		Region as Amazon <a href="#S3">S3</a> and install the conversion software onto the instance. Perform the
		transformation and re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the <a href="#EC2">EC2</a>
		instance.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 122<br />A company is migrating a NoSQL database
		cluster to Amazon <a href="#EC2">EC2</a>. The database automatically replicates data to maintain at least three
		copies of the data. I/O throughput of the servers is the highest priority. Which instance type should a
		solutions architect recommend for the migration?<br /><br />A. Storage optimized instances with instance
		store<br />B. Burstable general purpose instances with an Amazon Elastic Block Store (Amazon EBS) volume<br />C.
		Memory optimized instances with Amazon Elastic Block Store (Amazon EBS) optimization enabled<br />D. Compute
		optimized instances with Amazon Elastic Block Store (Amazon EBS) optimization enabled<br /><br /><b>Correct
			Answer:</b><br />A. Storage optimized instances with instance store<br /></div><a href="#All">All(200)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 123<br />A company has a large Microsoft
		SharePoint deployment running on&#8211;premises that requires Microsoft Windows shared file storage. The company
		wants to migrate this workload to the AWS Cloud and is considering various storage options. The storage solution
		must be highly available and integrated with Active Directory for access control.<br /><br />Which solution will
		satisfy these requirements?<br /><br />A. Configure Amazon EFS storage and set the Active Directory domain for
		authentication.<br />B. Create an SMB file share on an AWS Storage Gateway file gateway in two Availability
		Zones.<br />C. Create an Amazon <a href="#S3">S3</a> bucket and configure Microsoft Windows Server to mount it
		as a volume.<br />D. Create an Amazon FSx for Windows File Server file system on AWS and set the Active
		Directory domain for authentication.<br /><br /><b>Correct Answer:</b><br />D. Create an Amazon FSx for Windows
		File Server file system on AWS and set the Active Directory domain for authentication.<br /><br />Answer
		Description:<br />Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file
		storage that is accessible over the industry&#8211;standard Server Message Block (SMB) protocol. It is built on
		Windows Server, delivering a wide range of administrative features such as user quotas, end&#8211;user file
		restore, and Microsoft Active Directory (AD) integration. It offers single&#8211;AZ and multi&#8211;AZ
		deployment options, fully managed backups, and encryption of data at rest and in transit. You can optimize cost
		and performance for your workload needs with SSD and HDD storage options; and you can scale storage and change
		the throughput performance of your file system at any time. Amazon FSx file storage is accessible from Windows,
		Linux, and macOS compute instances and devices running on AWS or on premises.<br /><br />Works with Microsoft
		Active Directory (AD) to easily integrate file systems with Windows environments.<br /><br />CORRECT:
		&quot;Amazon FSx&quot; is the correct answer.<br /><br />INCORRECT: &quot;Amazon EFS&quot; is incorrect as EFS
		only supports Linux systems. INCORRECT: &quot;Amazon <a href="#S3">S3</a>&quot; is incorrect as this is not a
		suitable replacement for a Microsoft filesystem.<br /><br />INCORRECT: &quot;AWS Storage Gateway&quot; is
		incorrect as this service is primarily used for connecting on&#8211;premises storage to cloud storage. It
		consists of a software device installed on&#8211;premises and can be used with SMB shares but it actually stores
		the data on <a href="#S3">S3</a>. It is also used for migration. However, in this case the company need to
		replace the file server farm and Amazon FSx is the best choice for this job.<br /><br />References:<br />Amazon
		FSx for Windows File Server > Windows User Guide > Availability and durability: Single&#8211;AZ and
		Multi&#8211;AZ file systems<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 124<br />A company has a web application with
		sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of
		each week, and unpredictable usage during the week. The application consists of a web server and a MySQL
		database server running inside the data center. The company would like to move the application to the AWS Cloud,
		and needs to select a cost&#8211;effective database platform that will not require database
		modifications.<br /><br />Which solution will meet these requirements?<br /><br />A. Amazon DynamoDB<br />B.
		Amazon RDS for MySQL<br />C. MySQL&#8211;compatible Amazon Aurora Serverless<br />D. MySQL deployed on Amazon <a
			href="#EC2">EC2</a> in an Auto Scaling group<br /><br /><b>Correct Answer:</b><br />B. Amazon RDS for
		MySQL<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 125<br />A solutions architect is creating an
		application that will handle batch processing of large amounts of data.<br /><br />The input data will be held
		in Amazon <a href="#S3">S3</a> and the output data will be stored in a different <a href="#S3">S3</a> bucket.
		For processing, the application will transfer the data over the network between multiple Amazon <a
			href="#EC2">EC2</a> instances.<br /><br />What should the solutions architect do to reduce the overall data
		transfer costs?<br /><br />A. Place all the <a href="#EC2">EC2</a> instances in an Auto Scaling group.<br />B.
		Place all the <a href="#EC2">EC2</a> instances in the same AWS Region.<br />C. Place all the <a
			href="#EC2">EC2</a> instances in the same Availability Zone.<br />D. Place all the <a href="#EC2">EC2</a>
		instances in private subnets in multiple Availability Zones.<br /><br /><b>Correct Answer:</b><br />C. Place all
		the <a href="#EC2">EC2</a> instances in the same Availability Zone.<br /><br />Answer Description:<br />The
		transfer is between <a href="#EC2">EC2</a> instances and not just between <a href="#S3">S3</a> and <a
			href="#EC2">EC2</a>.<br /><br />Also, be aware of inter&#8211;Availability Zones data transfer charges
		between Amazon <a href="#EC2">EC2</a> instances, even within the same region. If possible, the instances in a
		development or test environment that need to communicate with each other should be co&#8211;located within the
		same Availability Zone to avoid data transfer charges. (This doesn&apost apply to production workloads which
		will most likely need to span multiple Availability Zones for high
		availability.)<br /><br />References:<br /><br />AWS Management & Governance Blog > Using AWS Cost Explorer to
		analyze data transfer costs<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 126<br />A company operates an eCommerce website
		on Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer (ALB) in an Auto Scaling group.
		The site is experiencing performance issues related to a high request rate from illegitimate external systems
		with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The
		company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate
		users.<br /><br />What should a solutions architect recommend?<br /><br />A. Deploy Amazon Inspector and
		associate it with the ALB.<br />B. Deploy AWS WAF, associate it with the ALB, and configure a
		rate&#8211;limiting rule.<br />C. Deploy rules to the network ACLs associated with the ALB to block the incoming
		traffic.<br />D. Deploy Amazon GuardDuty and enable rate&#8211;limiting protection when configuring
		GuardDuty.<br /><br /><b>Correct Answer:</b><br />B. Deploy AWS WAF, associate it with the ALB, and configure a
		rate&#8211;limiting rule.<br /><br />Answer Description:<br />Rate limit<br /><br />For a rate&#8211;based rule,
		enter the maximum number of requests to allow in any five&#8211;minute period from an IP address that matches
		the rule&aposs conditions. The rate limit must be at least 100.<br /><br />You can specify a rate limit alone,
		or a rate limit and conditions. If you specify only a rate limit, AWS WAF places the limit on all IP addresses.
		If you specify a rate limit and conditions, AWS WAF places the limit on IP addresses that match the
		conditions.<br /><br />When an IP address reaches the rate limit threshold, AWS WAF applies the assigned action
		(block or count) as quickly as possible, usually within 30 seconds. Once the action is in place, if five minutes
		pass with no requests from the IP address, AWS WAF resets the counter to zero.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 127<br />A company receives structured and
		semi&#8211;structured data from various sources once every day. A solutions architect needs to design a solution
		that leverages big data processing frameworks. The data should be accessible using SQL queries and business
		intelligence tools.<br /><br />What should the solutions architect recommend to build the MOST
		high&#8211;performing solution?<br /><br />A. Use AWS Glue to process data and Amazon <a href="#S3">S3</a> to
		store data.<br />B. Use Amazon EMR to process data and Amazon Redshift to store data.<br />C. Use Amazon <a
			href="#EC2">EC2</a> to process data and Amazon Elastic Block Store (Amazon EBS) to store data.<br />D. Use
		Amazon Kinesis Data Analytics to process data and Amazon Elastic File System (Amazon EFS) to store
		data.<br /><br /><b>Correct Answer:</b><br />B. Use Amazon EMR to process data and Amazon Redshift to store
		data.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 128<br />A company is hosting an election
		reporting website on AWS for users around the world. The website uses Amazon <a href="#EC2">EC2</a> instances
		for the web and application tiers in an Auto Scaling group with Application Load Balancers. The database tier
		uses an Amazon RDS for MySQL database. The website is updated with election results once an hour and has
		historically observed hundreds of users accessing the reports.<br /><br />The company is expecting a significant
		increase in demand because of upcoming elections in different countries. A solutions architect must improve the
		website&aposs ability to handle additional demand while minimizing the need for additional <a
			href="#EC2">EC2</a> instances.<br /><br />Which solution will meet these requirements?<br /><br />A. Launch
		an Amazon ElastiCache cluster to cache common database queries.<br />B. Launch an Amazon <a
			href="#CloudFront">CloudFront</a> web distribution to cache commonly requested website content.<br />C.
		Enable disk&#8211;based caching on the <a href="#EC2">EC2</a> instances to cache commonly requested website
		content.<br />D. Deploy a reverse proxy into the design using an <a href="#EC2">EC2</a> instance with caching
		enabled for commonly requested website content.<br /><br /><b>Correct Answer:</b><br />B. Launch an Amazon <a
			href="#CloudFront">CloudFront</a> web distribution to cache commonly requested website content.<br /></div>
	<a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 129<br />A company has a 143 TB MySQL database
		that it wants to migrate to AWS. The plan is to use Amazon Aurora MySQL as the platform going forward. The
		company has a 100 Mbps AWS Direct Connect connection to Amazon <a href="#VPC">VPC</a>.<br /><br />Which solution
		meets the company&aposs needs and takes the LEAST amount of time?<br /><br />A. Use a gateway endpoint for
		Amazon <a href="#S3">S3</a>. Migrate the data to Amazon <a href="#S3">S3</a>. Import the data into
		Aurora.<br />B. Upgrade the Direct Connect link to 500 Mbps. Copy the data to Amazon <a href="#S3">S3</a>.
		Import the data into Aurora.<br />C. Order an AWS Snowmobile and copy the database backup to it. Have AWS import
		the data into Amazon <a href="#S3">S3</a>. Import the backup into Aurora.<br />D. Order four 50&#8211;TB AWS
		Snowball devices and copy the database backup onto them. Have AWS import the data into Amazon <a
			href="#S3">S3</a>. Import the data into Aurora.<br /><br /><b>Correct Answer:</b><br />D. Order four
		50&#8211;TB AWS Snowball devices and copy the database backup onto them. Have AWS import the data into Amazon <a
			href="#S3">S3</a>. Import the data into Aurora.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 130<br />A company hosts an online shopping
		application that stores all orders in an Amazon RDS for PostgreSQL Single&#8211;AZ DB
		instance.<br /><br />Management wants to eliminate single points of failure and has asked a solutions architect
		to recommend an approach to minimize database downtime without requiring any changes to the application
		code.<br /><br />Which solution meets these requirements?<br /><br />A. Convert the existing database instance
		to a Multi&#8211;AZ deployment by modifying the database instance and specifying the Multi&#8211;AZ
		option.<br />B. Create a new RDS Multi&#8211;AZ deployment. Take a snapshot of the current RDS instance and
		restore the new Multi&#8211;AZ deployment with the snapshot.<br />C. Create a read&#8211;only replica of the
		PostgreSQL database in another Availability Zone. Use Amazon Route 53 weighted record sets to distribute
		requests across the databases.<br />D. Place the RDS for PostgreSQL database in an Amazon <a href="#EC2">EC2</a>
		Auto Scaling group with a minimum group size of two. Use Amazon Route 53 weighted record sets to distribute
		requests across instances.<br /><br /><b>Correct Answer:</b><br />A. Convert the existing database instance to a
		Multi&#8211;AZ deployment by modifying the database instance and specifying the Multi&#8211;AZ
		option.<br /><br /><br /><br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 131<br />A company has a 10 Gbps AWS Direct
		Connect connection from its on&#8211;premises servers to AWS. The workloads using the connection are critical.
		The company requires a disaster recovery strategy with maximum resiliency that maintains the current connection
		bandwidth at a minimum.<br /><br />What should a solutions architect recommend?<br /><br />A. Set up a new
		Direct Connect connection in another AWS Region.<br />B. Set up a new AWS managed VPN connection in another AWS
		Region.<br />C. Set up two new Direct Connect connections: one in the current AWS Region and one in another
		Region.<br />D. Set up two new AWS managed VPN connections: one in the current AWS Region and one in another
		Region.<br /><br /><b>Correct Answer:</b><br />C. Set up two new Direct Connect connections: one in the current
		AWS Region and one in another Region.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 132<br />As part of budget planning, management
		wants a report of AWS billed items listed by user. The data will be used to create department budgets. A
		solutions architect needs to determine the most efficient way to obtain this report
		information.<br /><br />Which solution meets these requirements?<br /><br />A. Run a query with Amazon Athena to
		generate the report.<br />B. Create a report in Cost Explorer and download the report.<br />C. Access the bill
		details from the billing dashboard and download the bill.<br />D. Modify a cost budget in AWS Budgets to alert
		with Amazon Simple Email Service (Amazon SES).<br /><br /><b>Correct Answer:</b><br />B. Create a report in Cost
		Explorer and download the report.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 133<br />A company with facilities in North
		America, Europe, and Asia is designing new distributed application to optimize its global supply chain and
		manufacturing process. The orders booked on one continent should be visible to all Regions in a second or less.
		The database should be able to support failover with a short Recovery Time Objective (RTO). The uptime of the
		application is important to ensure that manufacturing is not impacted.<br /><br />What should a solutions
		architect recommend?<br /><br />A. Use Amazon DynamoDB global tables.<br />B. Use Amazon Aurora Global
		Database.<br />C. Use Amazon RDS for MySQL with a cross&#8211;Region read replica.<br />D. Use Amazon RDS for
		PostgreSQL with a cross&#8211;Region read replica.<br /><br /><b>Correct Answer:</b><br />B. Use Amazon Aurora
		Global Database.<br /><br />Answer Description:<br />Cross&#8211;Region Disaster Recovery: If your primary
		region suffers a performance degradation or outage, you can promote one of the secondary regions to take
		read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete
		regional outage. This provides your application with an effective Recovery Point Objective (RPO) of 1 second and
		a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business
		continuity plan.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 134<br />A company&aposs
		near&#8211;real&#8211;time streaming application is running on AWS. As the data is ingested, a job runs on the
		data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of
		incoming data. A solutions architect needs to design a scalable and serverless solution to enhance
		performance.<br /><br />Which combination of steps should the solutions architect take? (Choose
		two.)<br /><br />A. Use Amazon Kinesis Data Firehose to ingest the data.<br />B. Use AWS Lambda with AWS Step
		Functions to process the data.<br />C. Use AWS Database Migration Service (AWS DMS) to ingest the data.<br />D.
		Use Amazon <a href="#EC2">EC2</a> instances in an Auto Scaling group to process the data.<br />E. Use AWS
		Fargate with Amazon Elastic Container Service (Amazon <a href="#ECS">ECS</a>) to process the
		data.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon Kinesis Data Firehose to ingest the data.<br />E. Use
		AWS Fargate with Amazon Elastic Container Service (Amazon <a href="#ECS">ECS</a>) to process the data.<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 135<br />An application running on an Amazon <a
			href="#EC2">EC2</a> instance needs to access an Amazon DynamoDB table. Both the <a href="#EC2">EC2</a>
		instance and the DynamoDB table are in the same AWS account. A solutions architect must configure the necessary
		permissions.<br /><br />Which solution will allow least privilege access to the DynamoDB table from the <a
			href="#EC2">EC2</a> instance?<br /><br />A. Create an <a href="#IAM">IAM</a> role with the appropriate
		policy to allow access to the DynamoDB table. Create an instance profile to assign this <a href="#IAM">IAM</a>
		role to the <a href="#EC2">EC2</a> instance.<br />B. Create an <a href="#IAM">IAM</a> role with the appropriate
		policy to allow access to the DynamoDB table. Add the <a href="#EC2">EC2</a> instance to the trust relationship
		policy document to allow it to assume the role.<br />C. Create an <a href="#IAM">IAM</a> user with the
		appropriate policy to allow access to the DynamoDB table. Store the credentials in an Amazon <a
			href="#S3">S3</a> bucket and read them from within the application code directly.<br />D. Create an <a
			href="#IAM">IAM</a> user with the appropriate policy to allow access to the DynamoDB table. Ensure that the
		application stores the <a href="#IAM">IAM</a> credentials securely on local storage and uses them to make the
		DynamoDB calls.<br /><br /><b>Correct Answer:</b><br />A. Create an <a href="#IAM">IAM</a> role with the
		appropriate policy to allow access to the DynamoDB table. Create an instance profile to assign this <a
			href="#IAM">IAM</a> role to the <a href="#EC2">EC2</a> instance.<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 136<br />A solutions architect is designing a
		solution that involves orchestrating a series of Amazon Elastic Container Service (Amazon <a
			href="#ECS">ECS</a>) task types running on Amazon <a href="#EC2">EC2</a> instances that are part of an <a
			href="#ECS">ECS</a> cluster. The output and state data for all tasks needs to be stored. The amount of data
		output by each task is approximately 10 MB, and there could be hundreds of tasks running at a time. The system
		should be optimized for high&#8211;frequency reading and writing. As old outputs are archived and deleted, the
		storage size is not expected to exceed 1 TB.<br /><br />Which storage solution should the solutions architect
		recommend?<br /><br />A. An Amazon DynamoDB table accessible by all <a href="#ECS">ECS</a> cluster
		instances.<br />B. An Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode.<br />C. An
		Amazon Elastic File System (Amazon EFS) file system with Bursting Throughput mode.<br />D. An Amazon Elastic
		Block Store (Amazon EBS) volume mounted to the <a href="#ECS">ECS</a> cluster instances.<br /><br /><b>Correct
			Answer:</b><br />C. An Amazon Elastic File System (Amazon EFS) file system with Bursting Throughput
		mode.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 137<br />A company wants to migrate its MySQL
		database from on premises to AWS. The company recently experienced a database outage that significantly impacted
		the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that
		minimizes data loss and stores every transaction on at least two nodes.<br /><br />Which solution meets these
		requirements?<br /><br />A. Create an Amazon RDS DB instance with synchronous replication to three nodes in
		three Availability Zones.<br />B. Create an Amazon RDS MySQL DB instance with Multi&#8211;AZ functionality
		enabled to synchronously replicate the data.<br />C. Create an Amazon RDS MySQL DB instance and then create a
		read replica in a separate AWS Region that synchronously replicates the data.<br />D. Create an Amazon <a
			href="#EC2">EC2</a> instance with a MySQL engine installed that triggers an AWS Lambda function to
		synchronously replicate the data to an Amazon RDS MySQL DB instance.<br /><br /><b>Correct Answer:</b><br />A.
		Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 138<br />A company stores user data in AWS. The
		data is used continuously with peak usage during business hours. Access patterns vary, with some data not being
		used for months at a time. A solutions architect must choose a cost&#8211;effective solution that maintains the
		highest level of durability while maintaining high availability.<br /><br />Which storage solution meets these
		requirements?<br /><br />A. Amazon <a href="#S3">S3</a> Standard<br />B. Amazon <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br />C. Amazon <a href="#S3">S3</a> Glacier Deep Archive<br />D. Amazon <a
			href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />B. Amazon <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 139<br />A company receives inconsistent service
		from its data center provider because the company is headquartered in an area affected by natural disasters. The
		company is not ready to fully migrate to the AWS Cloud, but it wants a failure environment on AWS in case the
		on&#8211;premises data center fails.<br /><br />The company runs web servers that connect to external vendors.
		The data available on AWS and on&#8211;premises must be uniform.<br /><br />Which solution should a solutions
		architect recommend that has the LEAST amount of downtime?<br /><br />A. Configure an Amazon Route 53 failover
		record. Run application servers on Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer
		in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon <a
			href="#S3">S3</a>.<br />B. Configure an Amazon Route 53 failover record. Execute an AWS CloudFormation
		template from a script to create Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer.
		Set up AWS Storage Gateway with stored volumes to back up data to Amazon <a href="#S3">S3</a>.<br />C. Configure
		an Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a <a href="#VPC">VPC</a> and
		the data center. Run application servers on Amazon <a href="#EC2">EC2</a> in an Auto Scaling group. Run an AWS
		Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer.<br />D.
		Configure an Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation
		template to launch two Amazon <a href="#EC2">EC2</a> instances. Set up AWS Storage Gateway with stored volumes
		to back up data to Amazon <a href="#S3">S3</a>. Set up an AWS Direct Connect connection between a <a
			href="#VPC">VPC</a> and the data center.<br /><br /><b>Correct Answer:</b><br />A. Configure an Amazon Route
		53 failover record. Run application servers on Amazon <a href="#EC2">EC2</a> instances behind an Application
		Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon
		<a href="#S3">S3</a>.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 140<br />What should a solutions architect do to
		ensure that all objects uploaded to an Amazon <a href="#S3">S3</a> bucket are encrypted?<br /><br />A. Update
		the bucket policy to deny if the PutObject does not have an s3:x&#8211;amz&#8211;acl header set.<br />B. Update
		the bucket policy to deny if the PutObject does not have an s3:x&#8211;amz&#8211;acl header set to
		private.<br />C. Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header
		set to true.<br />D. Update the bucket policy to deny if the PutObject does not have an
		x&#8211;amz&#8211;server&#8211;side&#8211;encryption header set.<br /><br /><b>Correct Answer:</b><br />D.
		Update the bucket policy to deny if the PutObject does not have an
		x&#8211;amz&#8211;server&#8211;side&#8211;encryption header set.<br /><br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 141<br />A company uses Application Load Balancers
		(ALBs) in different AWS Regions. The ALBs receive inconsistent traffic that can spike and drop throughout the
		year. The company&aposs networking team needs to allow the IP addresses of the ALBs in the on&#8211;premises
		firewall to enable connectivity.<br /><br />Which solution is the MOST scalable with minimal configuration
		changes?<br /><br />A. Write an AWS Lambda script to get the IP addresses of the ALBs in different Regions.
		Update the on&#8211;premises firewall&aposs rule to allow the IP addresses of the ALBs.<br />B. Migrate all ALBs
		in different Regions to the Network Load Balancer (NLBs). Update the on&#8211;premises firewall&aposs rule to
		allow the Elastic IP addresses of all the NLBs.<br />C. Launch AWS Global Accelerator. Register the ALBs in
		different Regions to the accelerator. Update the on&#8211;premises firewall&aposs rule to allow static IP
		addresses associated with the accelerator.<br />D. Launch a Network Load Balancer (NLB) in one Region. Register
		the private IP addresses of the ALBs in different Regions with the NLB. Update the on&#8211;premises
		firewall&aposs rule to allow the Elastic IP address attached to the NLB.<br /><br /><b>Correct
			Answer:</b><br />C. Launch AWS Global Accelerator. Register the ALBs in different Regions to the
		accelerator. Update the on&#8211;premises firewall&aposs rule to allow static IP addresses associated with the
		accelerator.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 142<br />A company runs a high performance
		computing (HPC) workload on AWS. The workload required low latency network performance and high network
		throughput with tightly coupled node&#8211;to&#8211;node communication. The Amazon <a href="#EC2">EC2</a>
		instances are properly sized for compute and storage capacity, and are launched using default
		options.<br /><br />What should a solutions architect propose to improve the performance of the
		workload?<br /><br />A. Choose a cluster placement group while launching Amazon <a href="#EC2">EC2</a>
		instances.<br />B. Choose dedicated instance tenancy while launching Amazon <a href="#EC2">EC2</a>
		instances.<br />C. Choose an Elastic Inference accelerator while launching Amazon <a href="#EC2">EC2</a>
		instances.<br />D. Choose the required capacity reservation while launching Amazon <a href="#EC2">EC2</a>
		instances.<br /><br /><b>Correct Answer:</b><br />A. Choose a cluster placement group while launching Amazon <a
			href="#EC2">EC2</a> instances.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 143<br />A solutions architect is designing a high
		performance computing (HPC) workload on Amazon <a href="#EC2">EC2</a>. The <a href="#EC2">EC2</a> instances need
		to communicate to each other frequently and require network performance with low latency and high
		throughput.<br /><br />Which <a href="#EC2">EC2</a> configuration meets these requirements?<br /><br />A. Launch
		the <a href="#EC2">EC2</a> instances in a cluster placement group in one Availability Zone.<br />B. Launch the
		<a href="#EC2">EC2</a> instances in a spread placement group in one Availability Zone.<br />C. Launch the <a
			href="#EC2">EC2</a> instances in an Auto Scaling group in two Regions and peer the <a
			href="#VPC">VPC</a>s.<br />D. Launch the <a href="#EC2">EC2</a> instances in an Auto Scaling group spanning
		multiple Availability Zones.<br /><br /><b>Correct Answer:</b><br />A. Launch the <a href="#EC2">EC2</a>
		instances in a cluster placement group in one Availability Zone.<br /><br />Answer Description:<br />When you
		launch a new <a href="#EC2">EC2</a> instance, the <a href="#EC2">EC2</a> service attempts to place the instance
		in such a way that all of your instances are spread out across underlying hardware to minimize correlated
		failures. You can use placement groups to influence the placement of a group of interdependent instances to meet
		the needs of your workload.<br /><br />Depending on the type of workload, you can create a placement group using
		one of the following placement strategies:<br /><br />Cluster • packs instances close together inside an
		Availability Zone. This strategy enables workloads to achieve the low&#8211;latency network performance
		necessary for tightly&#8211;coupled node&#8211;to&#8211;node communication that is typical of HPC
		applications.<br /><br />Partition • spreads your instances across logical partitions such that groups of
		instances in one partition do not share the underlying hardware with groups of instances in different
		partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop,
		Cassandra, and Kafka.<br /><br />Spread • strictly places a small group of instances across distinct underlying
		hardware to reduce correlated failures.<br /><br />For this scenario, a cluster placement group should be used
		as this is the best option for providing low&#8211;latency network performance for a HPC
		application.<br /><br />CORRECT: &quot;Launch the <a href="#EC2">EC2</a> instances in a cluster placement group
		in one Availability Zone&quot; is the correct answer.<br /><br />INCORRECT: &quot;Launch the <a
			href="#EC2">EC2</a> instances in a spread placement group in one Availability Zone&quot; is incorrect as the
		spread placement group is used to spread instances across distinct underlying hardware.<br /><br />INCORRECT:
		&quot;Launch the <a href="#EC2">EC2</a> instances in an Auto Scaling group in two Regions. Place a Network Load
		Balancer in front of the instances&quot; is incorrect as this does not achieve the stated requirement to provide
		low&#8211;latency, high throughput network performance between instances. Also, you cannot use an ELB across
		Regions.<br /><br />INCORRECT: &quot;Launch the <a href="#EC2">EC2</a> instances in an Auto Scaling group
		spanning multiple Availability Zones&quot; is incorrect as this does not reduce network latency or improve
		performance.<br /><br />References:<br /><br />Amazon Elastic Compute Cloud > User Guide for Linux Instances >
		Placement groups</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 144<br />A company&aposs application is running on
		Amazon <a href="#EC2">EC2</a> instances in a single Region. In the event of a disaster, a solutions architect
		needs to ensure that the resources can also be deployed to a second Region.<br /><br />Which combination of
		actions should the solutions architect take to accomplish this? (Choose two.)<br /><br />A. Detach a volume on
		an <a href="#EC2">EC2</a> instance and copy it to Amazon <a href="#S3">S3</a>.<br />B. Launch a new <a
			href="#EC2">EC2</a> instance from an Amazon Machine Image (AMI) in a new Region.<br />C. Launch a new <a
			href="#EC2">EC2</a> instance in a new Region and copy a volume from Amazon <a href="#S3">S3</a> to the new
		instance.<br />D. Copy an Amazon Machine Image (AMI) of an <a href="#EC2">EC2</a> instance and specify a
		different Region for the destination.<br />E. Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon
		<a href="#S3">S3</a> and launch an <a href="#EC2">EC2</a> instance in the destination Region using that EBS
		volume.<br /><br /><b>Correct Answer:</b><br />B. Launch a new <a href="#EC2">EC2</a> instance from an Amazon
		Machine Image (AMI) in a new Region.<br />D. Copy an Amazon Machine Image (AMI) of an <a href="#EC2">EC2</a>
		instance and specify a different Region for the destination.<br /><br />Answer Description:<br />Cross Region <a
			href="#EC2">EC2</a> AMI Copy<br />We know that you want to build applications that span AWS Regions and
		we&aposre working to provide you with the services and features needed to do so. We started out by launching the
		EBS Snapshot Copy feature late last year. This feature gave you the ability to copy a snapshot from Region to
		Region with just a couple of clicks. In addition, last month we made a significant reduction (26% to 83%) in the
		cost of transferring data between AWS Regions, making it less expensive to operate in more than one AWS
		region.<br /><br />Today we are introducing a new feature: Amazon Machine Image (AMI) Copy. AMI Copy enables you
		to easily copy your Amazon Machine Images between AWS Regions. AMI Copy helps enable several key scenarios
		including:<br /><br />Simple and Consistent Multi&#8211;Region Deployment &#8212; You can copy an AMI from one
		region to another, enabling you to easily launch consistent instances based on the same AMI into different
		regions.<br /><br />Scalability &#8212; You can more easily design and build world&#8211;scale applications that
		meet the needs of your users, regardless of their location.<br /><br />Performance &#8212; You can increase
		performance by distributing your application and locating critical components of your application in closer
		proximity to your users. You can also take advantage of region specific features such as instance types or other
		AWS services.<br /><br />Even Higher Availability &#8212; You can design and deploy applications across AWS
		regions, to increase availability. Once the new AMI is in an Available state the copy is
		complete.<br /><br />Once the new AMI is in an Available state the copy is complete.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 145<br />A manufacturing company wants to
		implement predictive maintenance on its machinery equipment. The company will install thousands of IoT sensors
		that will send data to AWS in real time. A solutions architect is tasked with implementing a solution that will
		receive events in an ordered manner for each machinery asset and ensure that data is saved for further
		processing at a later time.<br /><br />Which solution would be MOST efficient?<br /><br />A. Use Amazon Kinesis
		Data Streams for real&#8211;time events with a partition for each equipment asset. Use Amazon Kinesis Data
		Firehose to save data to Amazon <a href="#S3">S3</a>.<br />B. Use Amazon Kinesis Data Streams for
		real&#8211;time events with a shard for each equipment asset. Use Amazon Kinesis Data Firehose to save data to
		Amazon EBS.<br />C. Use an Amazon SQS FIFO queue for real&#8211;time events with one queue for each equipment
		asset. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS.<br />D. Use an Amazon SQS
		standard queue for real&#8211;time events with one queue for each equipment asset. Trigger an AWS Lambda
		function from the SQS queue to save data to Amazon <a href="#S3">S3</a>.<br /><br /><b>Correct
			Answer:</b><br />A. Use Amazon Kinesis Data Streams for real&#8211;time events with a partition for each
		equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon <a href="#S3">S3</a>.<br /><br />Answer
		Description:<br />Amazon SQS Introduces FIFO Queues with Exactly&#8211;Once Processing and Lower Prices for
		Standard Queues<br /><br />You can now use Amazon Simple Queue Service (SQS) for applications that require
		messages to be processed in a strict sequence and exactly once using First&#8211;in, First&#8211;out (FIFO)
		queues. FIFO queues are designed to ensure that the order in which messages are sent and received is strictly
		preserved and that each message is processed exactly once.<br /><br />Amazon SQS is a reliable and
		highly&#8211;scalable managed message queue service for storing messages in transit between application
		components. FIFO queues complement the existing Amazon SQS standard queues, which offer high throughput,
		best&#8211;effort ordering, and at&#8211;least&#8211;once delivery. FIFO queues have essentially the same
		features as standard queues, but provide the added benefits of supporting ordering and exactly&#8211;once
		processing. FIFO queues provide additional features that help prevent unintentional duplicates from being sent
		by message producers or from being received by message consumers. Additionally, message groups allow multiple
		separate ordered message streams within the same queue.<br /><br />Amazon Kinesis Data Streams collect and
		process data in real time. A Kinesis data stream is a set of shards. Each shard has a sequence of data records.
		Each data record has a sequence number that is assigned by Kinesis Data Streams. A shard is a uniquely
		identified sequence of data records in a stream.<br /><br />A partition key is used to group data by shard
		within a stream. Kinesis Data Streams segregates the data records belonging to a stream into multiple shards. It
		uses the partition key that is associated with each data record to determine which shard a given data record
		belongs to.<br /><br />For this scenario, the solutions architect can use a partition key for each device. This
		will ensure the records for that device are grouped by shard and the shard will ensure ordering. Amazon <a
			href="#S3">S3</a> is a valid destination for saving the data records.<br /><br />CORRECT: &quot;Use Amazon
		Kinesis Data Streams for real&#8211;time events with a partition key for each device. Use Amazon Kinesis Data
		Firehose to save data to Amazon <a href="#S3">S3</a>&quot; is the correct answer.<br /><br />INCORRECT:
		&quot;Use Amazon Kinesis Data Streams for real&#8211;time events with a shard for each device. Use Amazon
		Kinesis Data Firehose to save data to Amazon EBS&quot; is incorrect as you cannot save data to EBS from
		Kinesis.<br /><br />INCORRECT: &quot;Use an Amazon SQS FIFO queue for real&#8211;time events with one queue for
		each device. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS&quot; is incorrect as
		SQS is not the most efficient service for streaming, real time data.<br /><br />INCORRECT: &quot;Use an Amazon
		SQS standard queue for real&#8211;time events with one queue for each device. Trigger an AWS Lambda function
		from the SQS queue to save data to Amazon <a href="#S3">S3</a>&quot; is incorrect as SQS is not the most
		efficient service for streaming, real time data.<br /><br />References:<br /><br />Amazon Kinesis Data Streams >
		Developer Guide > Amazon Kinesis Data Streams Terminology and Concepts</div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 146<br />A company currently operates a web
		application backed by an Amazon RDS MySQL database. It has automated backups that are run daily and are not
		encrypted. A security audit requires future backups to be encrypted and the unencrypted backups to be destroyed.
		The company will make at least one encrypted backup before destroying the old backups.<br /><br />What should be
		done to enable encryption for future backups?<br /><br />A. Enable default encryption for the Amazon <a
			href="#S3">S3</a> bucket where backups are stored.<br />B. Modify the backup section of the database
		configuration to toggle the Enable encryption check box.<br />C. Create a snapshot of the database. Copy it to
		an encrypted snapshot. Restore the database from the encrypted snapshot.<br />D. Enable an encrypted read
		replica on RDS for MySQL. Promote the encrypted read replica to primary. Remove the original database
		instance.<br /><br /><b>Correct Answer:</b><br />C. Create a snapshot of the database. Copy it to an encrypted
		snapshot. Restore the database from the encrypted snapshot.<br /><br />Answer Description:<br />However, because
		you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB
		instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that
		snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of
		your original DB instance.<br /><br />DB instances that are encrypted can&apost be modified to disable
		encryption.<br /><br />You can&apost have an encrypted read replica of an unencrypted DB instance or an
		unencrypted read replica of an encrypted DB instance.<br /><br />Encrypted read replicas must be encrypted with
		the same key as the source DB instance when both are in the same AWS Region.<br /><br />You can&apost restore an
		unencrypted backup or snapshot to an encrypted DB instance.<br /><br />To copy an encrypted snapshot from one
		AWS Region to another, you must specify the KMS key identifier of the destination AWS Region. This is because
		KMS encryption keys are specific to the AWS Region that they are created in.<br /><br />Amazon RDS uses
		snapshots for backup. Snapshots are encrypted when created only if the database is encrypted and you can only
		select encryption for the database when you first create it. In this case the database, and hence the snapshots,
		ad unencrypted.<br /><br />However, you can create an encrypted copy of a snapshot. You can restore using that
		snapshot which creates a new DB instance that has encryption enabled. From that point on encryption will be
		enabled for all snapshots.<br /><br />CORRECT: &quot;Create a snapshot of the database. Copy it to an encrypted
		snapshot. Restore the database from the encrypted snapshot&quot; is the correct answer. INCORRECT: &quot;Enable
		an encrypted read replica on RDS for MySQL. Promote the encrypted read replica to primary.<br /><br />Remove the
		original database instance&quot; is incorrect as you cannot create an encrypted read replica from an unencrypted
		master.<br /><br />INCORRECT: &quot;Modify the backup section of the database configuration to toggle the Enable
		encryption check box&quot; is incorrect as you cannot add encryption for an existing
		database.<br /><br />INCORRECT: &quot;Enable default encryption for the Amazon <a href="#S3">S3</a> bucket where
		backups are stored&quot; is incorrect because you do not have access to the <a href="#S3">S3</a> bucket in which
		snapshots are stored.<br /><br />References:<br /><br />Amazon Relational Database Service > User Guide >
		Encrypting Amazon RDS resources</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 147<br />An application runs on Amazon <a
			href="#EC2">EC2</a> instances across multiple Availability Zones. The instances run in an Amazon <a
			href="#EC2">EC2</a> Auto Scaling group behind an Application Load Balancer. The application performs best
		when the CPU utilization of the <a href="#EC2">EC2</a> instances is at or near 40%.<br /><br />What should a
		solutions architect do to maintain the desired performance across all instances in the group?<br /><br />A. Use
		a simple scaling policy to dynamically scale the Auto Scaling group.<br />B. Use a target tracking policy to
		dynamically scale the Auto Scaling group.<br />C. Use an AWS Lambda function to update the desired Auto Scaling
		group capacity.<br />D. Use scheduled scaling actions to scale up and scale down the Auto Scaling
		group.<br /><br /><b>Correct Answer:</b><br />B. Use a target tracking policy to dynamically scale the Auto
		Scaling group.<br /><br />Answer Description:<br />&quot;With target tracking scaling policies, you select a
		scaling metric and set a target value. Amazon <a href="#EC2">EC2</a> AutoScaling creates and manages the
		CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and
		the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to,
		the specified target value. In addition to keeping the metric close to the target value, a target tracking
		scaling policy also adjusts to changes in the metric due to a changing load pattern. For example, you can use
		target tracking scaling to: Configure a target tracking scaling policy to keep the average aggregate CPU
		utilization of your Auto Scaling group at 40 percent. Configure a target tracking scaling policy to keep the
		request count per target of your Application Load Balancer target group at 1000 for your AutoScaling
		group.&quot;<br /><br />With target tracking scaling policies, you select a scaling metric and set a target
		value. Amazon <a href="#EC2">EC2</a> Auto Scaling creates and manages the CloudWatch alarms that trigger the
		scaling policy and calculates the scaling adjustment based on the metric and the target value.<br /><br />The
		scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target
		value. In addition to keeping the metric close to the target value, a target tracking scaling policy also
		adjusts to the changes in the metric due to a changing load pattern.<br /><br />CORRECT: &quot;Use a target
		tracking policy to dynamically scale the Auto Scaling group&quot; is the correct answer.<br /><br />INCORRECT:
		&quot;Use a simple scaling policy to dynamically scale the Auto Scaling group&quot; is incorrect as target
		tracking is a better way to keep the aggregate CPU usage at around 40% INCORRECT: &quot;Use an AWS Lambda
		function to update the desired Auto Scaling group capacity&quot; is incorrect as this can be done
		automatically.<br /><br />INCORRECT: &quot;Use scheduled scaling actions to scale up and scale down the Auto
		Scaling group&quot; is incorrect as dynamic scaling is required to respond to changes in
		utilization.<br /><br />References:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling > User Guide > Target
		tracking scaling policies for Amazon <a href="#EC2">EC2</a> Auto Scaling</div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 148<br />A company runs an internal
		browser&#8211;based application. The application runs on Amazon <a href="#EC2">EC2</a> instances behind an
		Application Load Balancer. The instances run in an Amazon <a href="#EC2">EC2</a> Auto Scaling group across
		multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down
		to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although
		it runs well by mid&#8211;morning.<br /><br />How should the scaling be changed to address the staff complaints
		and keep costs to a minimum?<br /><br />A. Implement a scheduled action that sets the desired capacity to 20
		shortly before the office opens.<br />B. Implement a step scaling action triggered at a lower CPU threshold, and
		decrease the cooldown period.<br />C. Implement a target tracking action triggered at a lower CPU threshold, and
		decrease the cooldown period.<br />D. Implement a scheduled action that sets the minimum and maximum capacity to
		20 shortly before the office opens.<br /><br /><b>Correct Answer:</b><br />A. Implement a scheduled action that
		sets the desired capacity to 20 shortly before the office opens.<br /><br />Answer Description:<br />Though this
		sounds like a good use case for scheduled actions, both answers using scheduled actions will have 20 instances
		running regardless of actual demand. A better option to be more cost effective is to use a target tracking
		action that triggers at a lower CPU threshold.<br /><br />With this solution the scaling will occur before the
		CPU utilization gets to a point where performance is affected. This will result in resolving the performance
		issues whilst minimizing costs. Using a reduced cooldown period will also more quickly terminate unneeded
		instances, further reducing costs.<br /><br />References:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling
		> User Guide > Target tracking scaling policies for Amazon <a href="#EC2">EC2</a> Auto Scaling</div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 149<br />A company hosts a static website
		on&#8211;premises and wants to migrate the website to AWS. The website should load as quickly as possible for
		users around the world. The company also wants the most cost&#8211;effective solution.<br /><br />What should a
		solutions architect do to accomplish this?<br /><br />A. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Replicate the <a
			href="#S3">S3</a> bucket to multiple AWS Regions.<br />B. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin.<br />C. Copy the
		website content to an Amazon EBS&#8211;backed Amazon <a href="#EC2">EC2</a> instance running Apache HTTP Server.
		Configure Amazon Route 53 geolocation routing policies to select the closest origin.<br />D. Copy the website
		content to multiple Amazon EBS&#8211;backed Amazon <a href="#EC2">EC2</a> instances running Apache HTTP Server
		in multiple AWS Regions. Configure Amazon <a href="#CloudFront">CloudFront</a> geolocation routing policies to
		select the closest origin.<br /><br /><b>Correct Answer:</b><br />B. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin.<br /><br />Answer
		Description:<br />What Is Amazon <a href="#CloudFront">CloudFront</a>?<br />Amazon <a
			href="#CloudFront">CloudFront</a> is a web service that speeds up distribution of your static and dynamic
		web content, such as .html, .css, .js, and image files, to your users. <a href="#CloudFront">CloudFront</a>
		delivers your content through a worldwide network of data centers called edge locations. When a user requests
		content that you&aposre serving with <a href="#CloudFront">CloudFront</a>, the user is routed to the edge
		location that provides the lowest latency (time delay), so that content is delivered with the best possible
		performance.<br /><br />Using Amazon <a href="#S3">S3</a> Buckets for Your Origin<br />When you use Amazon <a
			href="#S3">S3</a> as an origin for your distribution, you place any objects that you want <a
			href="#CloudFront">CloudFront</a> to deliver in an Amazon <a href="#S3">S3</a> bucket. You can use any
		method that is supported by Amazon <a href="#S3">S3</a> to get your objects into Amazon <a href="#S3">S3</a>,
		for example, the Amazon <a href="#S3">S3</a> console or API, or a third&#8211;party tool. You can create a
		hierarchy in your bucket to store the objects, just as you would with any other Amazon <a href="#S3">S3</a>
		bucket.<br /><br />Using an existing Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change the bucket in any way; you can still use
		it as you normally would to store and access Amazon <a href="#S3">S3</a> objects at the standard Amazon <a
			href="#S3">S3</a> price. You incur regular Amazon <a href="#S3">S3</a> charges for storing the objects in
		the bucket.<br /><br />The most cost&#8211;effective option is to migrate the website to an Amazon <a
			href="#S3">S3</a> bucket and configure that bucket for static website hosting. To enable good performance
		for global users the solutions architect should then configure a <a href="#CloudFront">CloudFront</a>
		distribution with the <a href="#S3">S3</a> bucket as the origin. This will cache the static content around the
		world closer to users.<br /><br />CORRECT: &quot;Copy the website content to an Amazon <a href="#S3">S3</a>
		bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Copy the website content to an Amazon <a href="#S3">S3</a> bucket. Configure
		the bucket to serve static webpage content. Replicate the <a href="#S3">S3</a> bucket to multiple AWS
		Regions&quot; is incorrect as there is no solution here for directing users to the closest region. This could be
		a more cost&#8211;effective (though less elegant) solution if AWS Route 53 latency records are
		created.<br /><br />INCORRECT: &quot;Copy the website content to an Amazon <a href="#EC2">EC2</a> instance.
		Configure Amazon Route 53 geolocation routing policies to select the closest origin&quot; is incorrect as using
		Amazon <a href="#EC2">EC2</a> instances is less cost&#8211;effective compared to hosting the website on <a
			href="#S3">S3</a>. Also, geolocation routing does not achieve anything with only a single
		record.<br /><br />INCORRECT: &quot;Copy the website content to multiple Amazon <a href="#EC2">EC2</a> instances
		in multiple AWS Regions. Configure AWS Route 53 geolocation routing policies to select the closest region&quot;
		is incorrect as using Amazon <a href="#EC2">EC2</a> instances is less cost&#8211;effective compared to hosting
		the website on <a href="#S3">S3</a>.<br /><br />References:<br /><br />How do I use <a
			href="#CloudFront">CloudFront</a> to serve a static website hosted on Amazon <a href="#S3">S3</a>?</div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 150<br />A company has deployed an API in a <a
			href="#VPC">VPC</a> behind an internet&#8211;facing Application Load Balancer (ALB). An application that
		consumes the API as a client is deployed in a second account in private subnets behind a NAT gateway. When
		requests to the client application increase, the NAT gateway costs are higher than expected. A solutions
		architect has configured the ALB to be internal.<br /><br />Which combination of architectural changes will
		reduce the NAT gateway costs? (Choose two.)<br /><br />A. Configure a <a href="#VPC">VPC</a> peering connection
		between the two <a href="#VPC">VPC</a>s. Access the API using the private address.<br />B. Configure an AWS
		Direct Connect connection between the two <a href="#VPC">VPC</a>s. Access the API using the private
		address.<br />C. Configure a ClassicLink connection for the API into the client <a href="#VPC">VPC</a>. Access
		the API using the ClassicLink address.<br />D. Configure a PrivateLink connection for the API into the client <a
			href="#VPC">VPC</a>. Access the API using the PrivateLink address.<br />E. Configure an AWS Resource Access
		Manager connection between the two accounts. Access the API using the private address.<br /><br /><b>Correct
			Answer:</b><br />A. Configure a <a href="#VPC">VPC</a> peering connection between the two <a
			href="#VPC">VPC</a>s. Access the API using the private address.<br />D. Configure a PrivateLink connection
		for the API into the client <a href="#VPC">VPC</a>. Access the API using the PrivateLink
		address.<br /><br />Answer Description:<br />PrivateLink makes it easy to connect services across different
		accounts and <a href="#VPC">VPC</a>s to significantly simplify the network architecture. There is no API listed
		in shareable resources for RAM.<br /><br />References:<br /><br />AWS Resource Access Manager > User Guide >
		Shareable AWS resources<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 151<br />A company has a two&#8211;tier
		application architecture that runs in public and private subnets. Amazon <a href="#EC2">EC2</a> instances
		running the web application are in the public subnet and a database runs on the private subnet.<br /><br />The
		web application instances and the database are running in a single Availability Zone (AZ).<br /><br />Which
		combination of steps should a solutions architect take to provide high availability for this architecture?
		(Choose two.)<br /><br />A. Create new public and private subnets in the same AZ for high availability.<br />B.
		Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group and Application Load Balancer spanning multiple
		AZs.<br />C. Add the existing web application instances to an Auto Scaling group behind an Application Load
		Balancer.<br />D. Create new public and private subnets in a new AZ. Create a database using Amazon <a
			href="#EC2">EC2</a> in one AZ.<br />E. Create new public and private subnets in the same <a
			href="#VPC">VPC</a>, each in a new AZ. Migrate the database to an Amazon RDS multi&#8211;AZ
		deployment.<br /><br /><b>Correct Answer:</b><br />B. Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group
		and Application Load Balancer spanning multiple AZs.<br />E. Create new public and private subnets in the same
		<a href="#VPC">VPC</a>, each in a new AZ. Migrate the database to an Amazon RDS multi&#8211;AZ
		deployment.<br /><br />Answer Description:<br /><br />You would the <a href="#EC2">EC2</a> instances to have
		high availability by placing them in multiple AZs.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 152<br />A marketing company is storing CSV files
		in an Amazon <a href="#S3">S3</a> bucket for statistical analysis. An application on an Amazon <a
			href="#EC2">EC2</a> instance needs permission to efficiently process the CSV data stored in the <a
			href="#S3">S3</a> bucket.<br /><br />Which action will MOST securely grant the <a href="#EC2">EC2</a>
		instance access to the <a href="#S3">S3</a> bucket?<br /><br />A. Attach a resource&#8211;based policy to the <a
			href="#S3">S3</a> bucket.<br />B. Create an <a href="#IAM">IAM</a> user for the application with specific
		permissions to the <a href="#S3">S3</a> bucket.<br />C. Associate an <a href="#IAM">IAM</a> role with least
		privilege permissions to the <a href="#EC2">EC2</a> instance profile.<br />D. Store AWS credentials directly on
		the <a href="#EC2">EC2</a> instance for applications on the instance to use for API calls.<br /><br /><b>Correct
			Answer:</b><br />C. Associate an <a href="#IAM">IAM</a> role with least privilege permissions to the <a
			href="#EC2">EC2</a> instance profile.<br /><br />Answer Description:<br />Keyword: Privilege Permission + <a
			href="#IAM">IAM</a> Role<br /><br />AWS Identity and Access Management (<a href="#IAM">IAM</a>) enables you
		to manage access to AWS services and resources securely. Using <a href="#IAM">IAM</a>, you can create and manage
		AWS users and groups, and use permissions to allow and deny their access to AWS resources.<br /><br /><a
			href="#IAM">IAM</a> is a feature of your AWS account offered at no additional charge. You will be charged
		only for use of other AWS services by your users.<br /><br /><a href="#IAM">IAM</a> roles for Amazon <a
			href="#EC2">EC2</a><br />Applications must sign their API requests with AWS credentials. Therefore, if you
		are an application developer, you need a strategy for managing credentials for your applications that run on <a
			href="#EC2">EC2</a> instances. For example, you can securely distribute your AWS credentials to the
		instances, enabling the applications on those instances to use your credentials to sign requests, while
		protecting your credentials from other users. However, it&aposs challenging to securely distribute credentials
		to each instance, especially those that AWS creates on your behalf, such as Spot Instances or instances in Auto
		Scaling groups. You must also be able to update the credentials on each instance when you rotate your AWS
		credentials.<br /><br />We designed <a href="#IAM">IAM</a> roles so that your applications can securely make API
		requests from your instances, without requiring you to manage the security credentials that the applications
		use.<br /><br />Instead of creating and distributing your AWS credentials, you can delegate permission to make
		API requests using <a href="#IAM">IAM</a> roles as follows:<br /><br />Create an <a href="#IAM">IAM</a>
		role.<br /><br />Define which accounts or AWS services can assume the role.<br /><br />Define which API actions
		and resources the application can use after assuming the role. Specify the role when you launch your instance,
		or attach the role to an existing instance. Have the application retrieve a set of temporary credentials and use
		them.<br /><br />For example, you can use <a href="#IAM">IAM</a> roles to grant permissions to applications
		running on your instances that need to use a bucket in Amazon <a href="#S3">S3</a>. You can specify permissions
		for <a href="#IAM">IAM</a> roles by creating a policy in JSON format. These are similar to the policies that you
		create for <a href="#IAM">IAM</a> users. If you change a role, the change is propagated to all
		instances.<br /><br />When creating <a href="#IAM">IAM</a> roles, associate least privilege <a
			href="#IAM">IAM</a> policies that restrict access to the specific API calls the application
		requires.<br /><br />References:<br /><br />AWS Identity and Access Management (<a href="#IAM">IAM</a>)
		FAQs<br />Amazon Elastic Compute Cloud > User Guide for Linux Instances > <a href="#IAM">IAM</a> roles for
		Amazon <a href="#EC2">EC2</a><br /><br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 153<br />A company hosts an application on
		multiple Amazon <a href="#EC2">EC2</a> instances. The application processes messages from an Amazon SQS queue,
		writes for an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found
		in the RDS table. The SQS queue does not contain any duplicate messages.<br /><br />What should a solutions
		architect do to ensure messages are being processed once only?<br /><br />A. Use the CreateQueue API call to
		create a new queue.<br />B. Use the AddPermission API call to add appropriate permissions.<br />C. Use the
		ReceiveMessage API call to set an appropriate wait time.<br />D. Use the ChangeMessageVisibility API call to
		increase the visibility timeout.<br /><br /><b>Correct Answer:</b><br />D. Use the ChangeMessageVisibility API
		call to increase the visibility timeout.<br /><br />Answer Description:<br />Keyword: SQS queue writes to an
		Amazon RDS<br /><br />From this, Option D best suite & other Options ruled out [Option A &#8212; You can&apost
		introduce one more Queue in the existing one; Option B &#8212; only Permission & Option C &#8212; Only Retrieves
		Messages]<br /><br />FIFO queues are designed to never introduce duplicate messages. However, your message
		producer might introduce duplicates in certain scenarios: for example, if the producer sends a message, does not
		receive a response, and then resends the same message. Amazon SQS APIs provide deduplication functionality that
		prevents your message producer from sending duplicates. Any duplicates introduced by the message producer are
		removed within a 5&#8211;minute deduplication interval.<br /><br />For standard queues, you might occasionally
		receive a duplicate copy of a message (at least once delivery). If you use a standard queue, you must design
		your applications to be idempotent (that is, they must not be affected adversely when processing the same
		message more than once).<br /><br />CreateQueue &#8212; You can&apost change the queue type after you create it
		and you can&apost convert an existing standard queue into a FIFO queue. You must either create a new FIFO queue
		for your application or delete your existing standard queue and recreate it as a FIFO
		queue.<br /><br />AddPermission &#8212; You create a queue, you have full control access rights for the queue.
		Only you, the owner of the queue, can grant or deny permissions to the queue.<br /><br />ReceiveMessage &#8212;
		Retrieves one or more messages (up to 10), from the specified queue.<br /><br />FIFO queues provide
		exactly&#8211;once processing, which means that each message is delivered once and remains available until a
		consumer processes it and deletes it.<br /><br />References:<br /><br />Amazon Simple Queue Service<br />Amazon
		SQS FAQs<br />Amazon Simple Queue Service > Developer Guide > What is Amazon Simple Queue Service?<br /><br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 154<br />A company has a three&#8211;tier
		image&#8211;sharing application. It uses an Amazon <a href="#EC2">EC2</a> instance for the front&#8211;end
		layer, another for the backend tier, and a third for the MySQL database. A solutions architect has been tasked
		with designing a solution that is highly available, and requires the least amount of changes to the
		application.<br /><br />Which solution meets these requirements?<br /><br />A. Use Amazon <a href="#S3">S3</a>
		to host the front&#8211;end layer and AWS Lambda functions for the backend layer. Move the database to an Amazon
		DynamoDB table and use Amazon <a href="#S3">S3</a> to store and serve users&apos images.<br />B. Use
		load&#8211;balanced Multi&#8211;AZ AWS Elastic Beanstalk environments for the front&#8211;end and backend
		layers. Move the database to an Amazon RDS instance with multiple read replicas to store and serve users&apos
		images.<br />C. Use Amazon <a href="#S3">S3</a> to host the front&#8211;end layer and a fleet of Amazon <a
			href="#EC2">EC2</a> instances in an Auto Scaling group for the backend layer. Move the database to a memory
		optimized instance type to store and serve users&apos images.<br />D. Use load&#8211;balanced Multi&#8211;AZ AWS
		Elastic Beanstalk environments for the front&#8211;end and backend layers. Move the database to an Amazon RDS
		instance with a Multi&#8211;AZ deployment. Use Amazon <a href="#S3">S3</a> to store and serve users&apos
		images.<br /><br /><b>Correct Answer:</b><br />D. Use load&#8211;balanced Multi&#8211;AZ AWS Elastic Beanstalk
		environments for the front&#8211;end and backend layers. Move the database to an Amazon RDS instance with a
		Multi&#8211;AZ deployment. Use Amazon <a href="#S3">S3</a> to store and serve users&apos
		images.<br /><br />Answer Description:<br />Keyword: Highly available + Least amount of changes to the
		application High Availability = Multi&#8211;AZ<br /><br />Least amount of changes to the application = Elastic
		Beanstalk Automatically handles the deployment, from capacity provisioning, Load Balancing, Auto Scaling to
		application health monitoring<br /><br />Option &#8212; D will be the right choice and Option &#8212; A; Option
		&#8212; B and Option &#8212; C out of race due to Cost & inter&#8211;operability.<br /><br />HA with Elastic
		Beanstalk and RDS<br /><br />AWS Elastic Beanstalk<br /><br />AWS Elastic Beanstalk is an
		easy&#8211;to&#8211;use service for deploying and scaling web applications and services developed with Java,
		.NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and
		IIS.<br /><br />You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from
		capacity provisioning, load balancing, auto&#8211;scaling to application health monitoring. At the same time,
		you retain full control over the AWS resources powering your application and can access the underlying resources
		at any time.<br /><br />There is no additional charge for Elastic Beanstalk &#8212; you pay only for the AWS
		resources needed to store and run your applications.<br /><br />AWS RDS<br />Amazon Relational Database Service
		(Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides
		cost&#8211;efficient and resizable capacity while automating time&#8211;consuming administration tasks such as
		hardware provisioning, database setup, patching and backups. It frees you to focus on your applications so you
		can give them the fast performance, high availability, security and compatibility they need.<br /><br />Amazon
		RDS is available on several database instance types &#8212; optimized for memory, performance or I/O &#8212; and
		provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL,
		MariaDB, Oracle Database, and SQL Server. You can use the AWS Database Migration Service to easily migrate or
		replicate your existing databases to Amazon RDS.<br /><br />AWS <a href="#S3">S3</a><br />Amazon Simple Storage
		Service (Amazon <a href="#S3">S3</a>) is an object storage service that offers industry&#8211;leading
		scalability, data availability, security, and performance. This means customers of all sizes and industries can
		use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications,
		backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon <a
			href="#S3">S3</a> provides easy&#8211;to&#8211;use management features so you can organize your data and
		configure finely&#8211;tuned access controls to meet your specific business, organizational, and compliance
		requirements. Amazon <a href="#S3">S3</a> is designed for 99.999999999% (11 9&aposs) of durability, and stores
		data for millions of applications for companies all around the world.<br /><br />References:<br /><br />AWS
		Elastic Beanstalk<br />Amazon Relational Database Service (RDS)<br />Amazon <a href="#S3">S3</a></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 155<br />A solutions architect is designing a
		system to analyze the performance of financial markets while the markets are closed. The system will run a
		series of compute&#8211;intensive jobs for 4 hours every night. The time to complete the compute jobs is
		expected to remain constant, and jobs cannot be interrupted once started. Once completed, the system is expected
		to run for a minimum of 1 year.<br /><br />Which type of Amazon <a href="#EC2">EC2</a> instances should be used
		to reduce the cost of the system?<br /><br />A. Spot Instances<br />B. On&#8211;Demand Instances<br />C.
		Standard Reserved Instances<br />D. Scheduled Reserved Instances<br /><br /><b>Correct Answer:</b><br />D.
		Scheduled Reserved Instances<br /><br />Answer Description:<br />Scheduled Reserved Instances (Scheduled
		Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a
		specified start time and duration, for a one&#8211;year term. You reserve the capacity in advance, so that you
		know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not
		use them.<br /><br />Scheduled Instances are a good choice for workloads that do not run continuously, but do
		run on a regular schedule. For example, you can use Scheduled Instances for an application that runs during
		business hours or for batch processing that runs at the end of the week.<br /><br />CORRECT: &quot;Scheduled
		Reserved Instances&quot; is the correct answer.<br /><br />INCORRECT: &quot;Standard Reserved Instances&quot; is
		incorrect as the workload only runs for 4 hours a day this would be more expensive.<br /><br />INCORRECT:
		&quot;On&#8211;Demand Instances&quot; is incorrect as this would be much more expensive as there is no discount
		applied.<br /><br />INCORRECT: &quot;Spot Instances&quot; is incorrect as the workload cannot be interrupted
		once started. With Spot instances workloads can be terminated if the Spot price changes or capacity is
		required.<br /><br />References:<br /><br />Amazon Elastic Compute Cloud > User Guide for Linux Instances >
		Scheduled Reserved Instances</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 156<br />A bicycle sharing company is developing a
		multi&#8211;tier architecture to track the location of its bicycles during peak operating hours. The company
		wants to use these data points in its existing analytics platform. A solutions architect must determine the most
		viable multi&#8211;tier option to support this architecture. The data points must be accessible from the REST
		API.<br /><br />Which action meets these requirements for storing and retrieving location data?<br /><br />A.
		Use Amazon Athena with Amazon <a href="#S3">S3</a>.<br />B. Use Amazon API Gateway with AWS Lambda.<br />C. Use
		Amazon QuickSight with Amazon Redshift.<br />D. Use Amazon API Gateway with Amazon Kinesis Data
		Analytics.<br /><br /><b>Correct Answer:</b><br />B. Use Amazon API Gateway with AWS Lambda.<br /><br />Answer
		Description:<br />Keyword: Data points in its existing analytics platform + Data points must be accessible from
		the REST API + Track the location of its bicycles during peak operating hours<br /><br />They already have an
		analytics platform, A (Athena) and D (Kinesis Data Analytics) are out of the race even though <a
			href="#S3">S3</a> & APT Gateway Support REST API. Now B and C are in Race. C will not support REST API. So
		answer should be B as per below details.<br /><br />Now if we talk about data type, we are talking about GEO
		location data for their bicycles. API Gateway will be support REST API. So, exact solution should be API Gateway
		with AWS Lambda along with Amazon Kinesis Data Analytics (Assume its used already).<br /><br />CORRECT:
		&quot;Use Amazon API Gateway with AWS Lambda&quot; is the correct answer. INCORRECT: &quot;Use Amazon Athena
		with Amazon <a href="#S3">S3</a>&quot; is incorrect as they already have analytics
		platform.<br /><br />INCORRECT: &quot;Use Amazon QuickSight with Amazon Redshift&quot; is incorrect. This is not
		support REST API.<br /><br />INCORRECT: &quot;Use Amazon API Gateway with Amazon Kinesis Data Analytics&quot; is
		incorrect as they already have analytics platform.<br /><br />References:<br /><br />Amazon API Gateway<br />AWS
		Lambda<br />Amazon Kinesis Data Analytics</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 157<br />An application requires a development
		environment (DEV) and production environment (PROD) for several years. The DEV instances will run for 10 hours
		each day during normal business hours, while the PROD instances will run 24 hours each day. A solutions
		architect needs to determine a compute instance purchase strategy to minimize costs.<br /><br />Which solution
		is the MOST cost&#8211;effective?<br /><br />A. DEV with Spot Instances and PROD with On&#8211;Demand
		Instances<br />B. DEV with On&#8211;Demand Instances and PROD with Spot Instances<br />C. DEV with Scheduled
		Reserved Instances and PROD with Reserved Instances<br />D. DEV with On&#8211;Demand Instances and PROD with
		Scheduled Reserved Instances<br /><br /><b>Correct Answer:</b><br />C. DEV with Scheduled Reserved Instances and
		PROD with Reserved Instances<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 158<br />A solutions architect observes that a
		nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon <a
			href="#EC2">EC2</a> capacity is reached. The peak capacity is the same every night and the batch jobs always
		start at 1 AM. The solutions architect needs to find a cost&#8211;effective solution that will allow for the
		desired <a href="#EC2">EC2</a> capacity to be reached quickly and allow the Auto Scaling group to scale down
		after the batch jobs are complete.<br /><br />What should the solutions architect do to meet these
		requirements?<br /><br />A. Increase the minimum capacity for the Auto Scaling group.<br />B. Increase the
		maximum capacity for the Auto Scaling group.<br />C. Configure scheduled scaling to scale up to the desired
		compute level.<br />D. Change the scaling policy to add more <a href="#EC2">EC2</a> instances during each
		scaling operation.<br /><br /><b>Correct Answer:</b><br />C. Configure scheduled scaling to scale up to the
		desired compute level.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 159<br />A Solutions Architect must design a web
		application that will be hosted on AWS, allowing users to purchase access to premium, shared content that is
		stored in an <a href="#S3">S3</a> bucket. Upon payment, content will be available for download for 14 days
		before the user is denied access.<br /><br />Which of the following would be the LEAST complicated
		implementation?<br /><br />A. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an origin
		access identity (OAI). Configure the distribution with an Amazon <a href="#S3">S3</a> origin to provide access
		to the file through signed URLs. Design a Lambda function to remove data that is older than 14 days.<br />B. Use
		an <a href="#S3">S3</a> bucket and provide direct access to the file. Design the application to track purchases
		in a DynamoDB table. Configure a Lambda function to remove data that is older than 14 days based on a query to
		Amazon DynamoDB.<br />C. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an OAI. Configure
		the distribution with an Amazon <a href="#S3">S3</a> origin to provide access to the file through signed URLs.
		Design the application to set an expiration of 14 days for the URL.<br />D. Use an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with an OAI. Configure the distribution with an Amazon <a
			href="#S3">S3</a> origin to provide access to the file through signed URLs. Design the application to set an
		expiration of 60 minutes for the URL and recreate the URL as necessary.<br /><br /><b>Correct
			Answer:</b><br />C. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an OAI. Configure
		the distribution with an Amazon <a href="#S3">S3</a> origin to provide access to the file through signed URLs.
		Design the application to set an expiration of 14 days for the URL.<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 160<br />A company wants to run a hybrid workload
		for data processing. The data needs to be accessed by on&#8211;premises applications for local data processing
		using an NFS protocol, and must also be accessible from the AWS Cloud for further analytics and batch
		processing.<br /><br />Which solution will meet these requirements?<br /><br />A. Use an AWS Storage Gateway
		file gateway to provide file storage to AWS, then perform analytics on this data in the AWS Cloud.<br />B. Use
		an AWS storage Gateway tape gateway to copy the backup of the local data to AWS, then perform analytics on this
		data in the AWS cloud.<br />C. Use an AWS Storage Gateway volume gateway in a stored volume configuration to
		regularly take snapshots of the local data, then copy the data to AWS.<br />D. Use an AWS Storage Gateway volume
		gateway in a cached volume configuration to back up all the local storage in the AWS cloud, then perform
		analytics on this data in the cloud.<br /><br /><b>Correct Answer:</b><br />A. Use an AWS Storage Gateway file
		gateway to provide file storage to AWS, then perform analytics on this data in the AWS
		Cloud.<br /><br />References:<br /><br />AWS Storage Gateway<br /><br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 161<br />A solutions architect is moving the
		static content from a public website hosted on Amazon <a href="#EC2">EC2</a> instances to an Amazon <a
			href="#S3">S3</a> bucket. An Amazon <a href="#CloudFront">CloudFront</a> distribution will be used to
		deliver the static assets. The security group used by the <a href="#EC2">EC2</a> instances restricts access to a
		limited set of IP ranges. Access to the static content should be similarly restricted.<br /><br />Which
		combination of steps will meet these requirements? (Choose two.)<br /><br />A. Create an origin access identity
		(OAI) and associate it with the distribution. Change the permissions in the bucket policy so that only the OAI
		can read the objects.<br />B. Create an AWS WAF web ACL that includes the same IP restrictions that exist in the
		<a href="#EC2">EC2</a> security group. Associate this new web ACL with the <a href="#CloudFront">CloudFront</a>
		distribution.<br />C. Create a new security group that includes the same IP restrictions that exist in the
		current <a href="#EC2">EC2</a> security group. Associate this new security group with the <a
			href="#CloudFront">CloudFront</a> distribution.<br />D. Create a new security group that includes the same
		IP restrictions that exist in the current <a href="#EC2">EC2</a> security group. Associate this new security
		group with the <a href="#S3">S3</a> bucket hosting the static content.<br />E. Create a new <a
			href="#IAM">IAM</a> role and associate the role with the distribution. Change the permissions either on the
		<a href="#S3">S3</a> bucket or on the files within the <a href="#S3">S3</a> bucket so that only the newly
		created <a href="#IAM">IAM</a> role has read and download permissions.<br /><br /><b>Correct Answer:</b><br />A.
		Create an origin access identity (OAI) and associate it with the distribution. Change the permissions in the
		bucket policy so that only the OAI can read the objects.<br />B. Create an AWS WAF web ACL that includes the
		same IP restrictions that exist in the <a href="#EC2">EC2</a> security group. Associate this new web ACL with
		the <a href="#CloudFront">CloudFront</a> distribution.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 162<br />A company recently launched its website
		to serve content to its global user base. The company wants to store and accelerate the delivery of static
		content to its users by leveraging Amazon <a href="#CloudFront">CloudFront</a> with an Amazon <a
			href="#EC2">EC2</a> instance attached as its origin.<br /><br />How should a solutions architect optimize
		high availability for the application?<br /><br />A. Use Lambda@Edge for <a
			href="#CloudFront">CloudFront</a>.<br />B. Use Amazon <a href="#S3">S3</a> Transfer Acceleration for <a
			href="#CloudFront">CloudFront</a>.<br />C. Configure another <a href="#EC2">EC2</a> instance in a different
		Availability Zone as part of the origin group.<br />D. Configure another <a href="#EC2">EC2</a> instance as part
		of the origin server cluster in the same Availability Zone.<br /><br /><b>Correct Answer:</b><br />A. Use
		Lambda@Edge for <a href="#CloudFront">CloudFront</a>.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 163<br />A company has created an isolated backup
		of its environment in another Region. The application is running in warm standby mode and is fronted by an
		Application Load Balancer (ALB). The current failover process is manual and requires updating a DNS alias record
		to point to the secondary ALB in another Region. What should a solutions architect do to automate the failover
		process?<br /><br />A. Enable an ALB health check<br />B. Enable an Amazon Route 53 health check.<br />C. Crate
		an CNAME record on Amazon Route 53 pointing to the ALB endpoint.<br />D. Create conditional forwarding rules on
		Amazon Route 53 pointing to an internal BIND DNS server.<br /><br /><b>Correct Answer:</b><br />C. Crate an
		CNAME record on Amazon Route 53 pointing to the ALB endpoint.<br /><br />References:<br /><br />How do I use
		Route 53 health checks for DNS failover?</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 164<br />A company relies on an application that
		needs at least 4 Amazon <a href="#EC2">EC2</a> instances during regular traffic and must scale up to 12 <a
			href="#EC2">EC2</a> instances during peak loads. The application is critical to the business and must be
		highly available.<br /><br />Which solution will meet these requirements?<br /><br />A. Deploy the <a
			href="#EC2">EC2</a> instances in an Auto Scaling group. Set the minimum to 4 and the maximum to 12, with 2
		in Availability Zone A and 2 in Availability Zone B.<br />B. Deploy the <a href="#EC2">EC2</a> instances in an
		Auto Scaling group. Set the minimum to 4 and the maximum to 12, with all 4 in Availability Zone A.<br />C.
		Deploy the <a href="#EC2">EC2</a> instances in an Auto Scaling group. Set the minimum to 8 and the maximum to
		12, with 4 in Availability Zone A and 4 in Availability Zone B.<br />D. Deploy the <a href="#EC2">EC2</a>
		instances in an Auto Scaling group. Set the minimum to 8 and the maximum to 12, with all 8 in Availability Zone
		A.<br /><br /><b>Correct Answer:</b><br />C. Deploy the <a href="#EC2">EC2</a> instances in an Auto Scaling
		group. Set the minimum to 8 and the maximum to 12, with 4 in Availability Zone A and 4 in Availability Zone
		B.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 165<br />A solutions architect must design a
		solution for a persistent database that is being migrated from on&#8211;premises to AWS. The database requires
		64,000 IOPS according to the database administrator. If possible, the database administrator wants to use a
		single Amazon Elastic Block Store (Amazon EBS) volume to host the database instance.<br /><br />Which solution
		effectively meets the database administrator&aposs criteria?<br /><br />A. Use an instance from the I3 I/O
		optimized family and leverage local ephemeral storage to achieve the IOPS requirement.<br />B. Create an
		Nitro&#8211;based Amazon <a href="#EC2">EC2</a> instance with an Amazon EBS Provisioned IOPS SSD (io1) volume
		attached. Configure the volume to have 64,000 IOPS.<br />C. Create and map an Amazon Elastic File System (Amazon
		EFS) volume to the database instance and use the volume to achieve the required IOPS for the database.<br />D.
		Provision two volumes and assign 32,000 IOPS to each. Create a logical volume at the operating system level that
		aggregates both volumes to achieve the IOPS requirements.<br /><br /><b>Correct Answer:</b><br />B. Create an
		Nitro&#8211;based Amazon <a href="#EC2">EC2</a> instance with an Amazon EBS Provisioned IOPS SSD (io1) volume
		attached. Configure the volume to have 64,000 IOPS.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 166<br />A company recently deployed a
		two&#8211;tier application in two Availability Zones in the us&#8211;east&#8211;1 Region. The databases are
		deployed in a private subnet while the web servers are deployed in a public subnet. An internet gateway is
		attached to the <a href="#VPC">VPC</a>. The application and database run on Amazon <a href="#EC2">EC2</a>
		instances. The database servers are unable to access patches on the internet. A solutions architect needs to
		design a solution that maintains database security with the least operational overhead.<br /><br />Which
		solution meets these requirements?<br /><br />A. Deploy a NAT gateway inside the public subnet for each
		Availability Zone and associate it with an Elastic IP address. Update the routing table of the private subnet to
		use it as the default route.<br />B. Deploy a NAT gateway inside the private subnet for each Availability Zone
		and associate it with an Elastic IP address. Update the routing table of the private subnet to use it as the
		default route.<br />C. Deploy two NAT instances inside the public subnet for each Availability Zone and
		associate them with Elastic IP addresses. Update the routing table of the private subnet to use it as the
		default route.<br />D. Deploy two NAT instances inside the private subnet for each Availability Zone and
		associate them with Elastic IP addresses. Update the routing table of the private subnet to use it as the
		default route.<br /><br /><b>Correct Answer:</b><br />A. Deploy a NAT gateway inside the public subnet for each
		Availability Zone and associate it with an Elastic IP address. Update the routing table of the private subnet to
		use it as the default route.<br /><br />Answer Description:<br /><a href="#VPC">VPC</a> with public and private
		subnets (NAT)<br /><br />The configuration for this scenario includes a virtual private cloud (<a
			href="#VPC">VPC</a>) with a public subnet and a private subnet. We recommend this scenario if you want to
		run a public&#8211;facing web application, while maintaining back&#8211;end servers that aren&apost publicly
		accessible. A common example is a multi&#8211;tier website, with the web servers in a public subnet and the
		database servers in a private subnet. You can set up security and routing so that the web servers can
		communicate with the database servers.<br /><br />The instances in the public subnet can send outbound traffic
		directly to the Internet, whereas the instances in the private subnet can&apost. Instead, the instances in the
		private subnet can access the Internet by using a network address translation (NAT) gateway that resides in the
		public subnet. The database servers can connect to the Internet for software updates using the NAT gateway, but
		the Internet cannot establish connections to the database servers.<br /><br />References:<br /><br />Amazon
		Virtual Private Cloud > User Guide > <a href="#VPC">VPC</a> with public and private subnets (NAT)</div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 167<br />A solutions architect needs to design a
		low&#8211;latency solution for a static single&#8211;page application accessed by users utilizing a custom
		domain name. The solution must be serverless, encrypted in transit, and cost&#8211;effective.<br /><br />Which
		combination of AWS services and features should the solutions architect use? (Choose two.)<br /><br />A. Amazon
		<a href="#S3">S3</a><br />B. Amazon <a href="#EC2">EC2</a><br />C. AWS Fargate<br />D. Amazon <a
			href="#CloudFront">CloudFront</a><br />E. Elastic Load Balancer<br /><br /><b>Correct Answer:</b><br />A.
		Amazon <a href="#S3">S3</a><br />D. Amazon <a href="#CloudFront">CloudFront</a><br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 168<br />A company has migrated an
		on&#8211;premises Oracle database to an Amazon RDS for Oracle Multi&#8211;AZ DB instance in the
		us&#8211;east&#8211;l Region. A solutions architect is designing a disaster recovery strategy to have the
		database provisioned in the us&#8211;west&#8211;2 Region in case the database becomes unavailable in the
		us&#8211;east&#8211;1 Region. The design must ensure the database is provisioned in the us&#8211;west&#8211;2
		Region in a maximum of 2 hours, with a data loss window of no more than 3 hours.<br /><br />How can these
		requirements be met?<br /><br />A. Edit the DB instance and create a read replica in us&#8211;west&#8211;2.
		Promote the read replica to master in us&#8211;west&#8211;2 in case the disaster recovery environment needs to
		be activated.<br />B. Select the multi&#8211;Region option to provision a standby instance in
		us&#8211;west&#8211;2. The standby instance will be automatically promoted to master in us&#8211;west&#8211;2 in
		case the disaster recovery environment needs to be created.<br />C. Take automated snapshots of the database
		instance and copy them to us&#8211;west&#8211;2 every 3 hours. Restore the latest snapshot to provision another
		database instance in us&#8211;west&#8211;2 in case the disaster recovery environment needs to be
		activated.<br />D. Create a multimaster read/write instances across multiple AWS Regions Select <a
			href="#VPC">VPC</a>s in us&#8211;east&#8211;1 and us&#8211;west&#8211;2 to make that deployment. Keep the
		master read/write instance in us&#8211;west&#8211;2 available to avoid having to activate a disaster recovery
		environment.<br /><br /><b>Correct Answer:</b><br />A. Edit the DB instance and create a read replica in
		us&#8211;west&#8211;2. Promote the read replica to master in us&#8211;west&#8211;2 in case the disaster recovery
		environment needs to be activated.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 169<br />A monolithic application was recently
		migrated to AWS and is now running on a single Amazon <a href="#EC2">EC2</a> instance. Due to application
		limitations, it is not possible to use automatic scaling to scale out the application. The chief technology
		officer (CTO) wants an automated solution to restore the <a href="#EC2">EC2</a> instance in the unlikely event
		the underlying hardware fails.<br /><br />What would allow for automatic recovery of the <a href="#EC2">EC2</a>
		instance as quickly as possible?<br /><br />A. Configure an Amazon CloudWatch alarm that triggers the recovery
		of the <a href="#EC2">EC2</a> instance if it becomes impaired.<br />B. Configure an Amazon CloudWatch alarm to
		trigger an SNS message that alerts the CTO when the <a href="#EC2">EC2</a> instance is impaired.<br />C.
		Configure AWS CloudTrail to monitor the health of the <a href="#EC2">EC2</a> instance, and if it becomes
		impaired, trigger instance recovery.<br />D. Configure an Amazon EventBridge event to trigger an AWS Lambda
		function once an hour that checks the health of the <a href="#EC2">EC2</a> instance and triggers instance
		recovery if the <a href="#EC2">EC2</a> instance is unhealthy.<br /><br /><b>Correct Answer:</b><br />A.
		Configure an Amazon CloudWatch alarm that triggers the recovery of the <a href="#EC2">EC2</a> instance if it
		becomes impaired.<br /><br />References:<br /><br />Amazon Elastic Compute Cloud > User Guide for Linux
		Instances > Recover your instance</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 170<br />Application developers have noticed that
		a production application is very slow when business reporting users run large production reports against the
		Amazon RDS instance backing the application. The CPU and memory utilization metrics for the RDS instance do not
		exceed 60% while the reporting queries are running.<br /><br />The business reporting users must be able to
		generate reports without affecting the application&aposs performance.<br /><br />Which action will accomplish
		this?<br /><br />A. Increase the size of the RDS instance.<br />B. Create a read replica and connect the
		application to it.<br />C. Enable multiple Availability Zones on the RDS instance.<br />D. Create a read replica
		and connect the business reports to it.<br /><br /><b>Correct Answer:</b><br />D. Create a read replica and
		connect the business reports to it.<br /><br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 171<br />A company is using a tape backup solution
		to store its key application data offsite. The daily data volume is around 50 TB. The company needs to retain
		the backups for 7 years for regulatory purposes. The backups are rarely accessed, and a week&aposs notice is
		typically given if a backup needs to be restored.<br /><br />The company is now considering a cloud&#8211;based
		option to reduce the storage costs and operational burden of managing tapes. The company also wants to make sure
		that the transition from tape backups to the cloud minimizes disruptions.<br /><br />Which storage solution is
		MOST cost&#8211;effective?<br /><br />A. Use Amazon Storage Gateway to back up to Amazon Glacier Deep
		Archive.<br />B. Use AWS Snowball Edge to directly integrate the backups with Amazon <a href="#S3">S3</a>
		Glacier.<br />C. Copy the backup data to Amazon <a href="#S3">S3</a> and create a lifecycle policy to move the
		data to Amazon <a href="#S3">S3</a> Glacier.<br />D. Use Amazon Storage Gateway to back up to Amazon <a
			href="#S3">S3</a> and create a lifecycle policy to move the backup to Amazon <a href="#S3">S3</a>
		Glacier.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon Storage Gateway to back up to Amazon Glacier Deep
		Archive.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 172<br />A company collects temperature, humidity,
		and atmospheric pressure data in cities across multiple continents. The average volume of data collected per
		site each day is 500 GB. Each site has a high&#8211;speed internet connection. The company&aposs weather
		forecasting applications are based in a single Region and analyze the data daily.<br /><br />What is the FASTEST
		way to aggregate data from all of these global sites?<br /><br />A. Enable Amazon <a href="#S3">S3</a> Transfer
		Acceleration on the destination bucket. Use multipart uploads to directly upload site data to the destination
		bucket.<br />B. Upload site data to an Amazon <a href="#S3">S3</a> bucket in the closest AWS Region. Use <a
			href="#S3">S3</a> cross&#8211;Region replication to copy objects to the destination bucket.<br />C. Schedule
		AWS Snowball jobs daily to transfer data to the closest AWS Region. Use <a href="#S3">S3</a> cross&#8211;Region
		replication to copy objects to the destination bucket.<br />D. Upload the data to an Amazon <a
			href="#EC2">EC2</a> instance in the closest Region. Store the data in an Amazon EBS volume. Once a day take
		an EBS snapshot and copy it to the centralized Region. Restore the EBS volume in the centralized Region and run
		an analysis on the data daily.<br /><br /><b>Correct Answer:</b><br />B. Upload site data to an Amazon <a
			href="#S3">S3</a> bucket in the closest AWS Region. Use <a href="#S3">S3</a> cross&#8211;Region replication
		to copy objects to the destination bucket.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 173<br />A company has a web server running on an
		Amazon <a href="#EC2">EC2</a> instance in a public subnet with an Elastic IP address. The default security group
		is assigned to the <a href="#EC2">EC2</a> instance. The default network ACL has been modified to block all
		traffic. A solutions architect needs to make the web server accessible from everywhere on port
		443.<br /><br />Which combination of steps will accomplish this task? (Choose two.)<br /><br />A. Create a
		security group with a rule to allow TCP port 443 from source 0.0.0.0/0.<br />B. Create a security group with a
		rule to allow TCP port 443 to destination 0.0.0.0/0.<br />C. Update the network ACL to allow TCP port 443 from
		source 0.0.0.0/0.<br />D. Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0
		and to destination 0.0.0.0/0.<br />E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0
		and outbound TCP port 32768&#8211;65535 to destination 0.0.0.0/0.<br /><br /><b>Correct Answer:</b><br />A.
		Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.<br />B. Create a security group
		with a rule to allow TCP port 443 to destination 0.0.0.0/0.<br /></div><a href="#All">All(200)</a> <a href="#"
		<i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 174<br />A database is on an Amazon RDS MySQL 5.6
		Multi&#8211;AZ DB instance that experiences highly dynamic reads.<br /><br />Application developers notice a
		significant slowdown when testing read performance from a secondary AWS Region. The developers want a solution
		that provides less than 1 second of read replication latency.<br /><br />What should the solutions architect
		recommend?<br /><br />A. Install MySQL on Amazon <a href="#EC2">EC2</a> in the secondary Region.<br />B. Migrate
		the database to Amazon Aurora with cross&#8211;Region replicas.<br />C. Create another RDS for MySQL read
		replica in the secondary Region.<br />D. Implement Amazon ElastiCache to improve database query
		performance.<br /><br /><b>Correct Answer:</b><br />B. Migrate the database to Amazon Aurora with
		cross&#8211;Region replicas.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 175<br />A company hosts its core network
		services, including directory services and DNS, in its on&#8211;premises data center. The data center is
		connected to the AWS Cloud using AWS Direct Connect (DX). Additional AWS accounts are planned that will require
		quick, cost&#8211;effective, and consistent access to these network services.<br /><br />What should a solutions
		architect implement to meet these requirements with the LEAST amount of operational overhead?<br /><br />A.
		Create a DX connection in each new account. Route the network traffic to the on&#8211;premises servers.<br />B.
		Configure <a href="#VPC">VPC</a> endpoints in the DX <a href="#VPC">VPC</a> for all required services. Route the
		network traffic to the on&#8211;premises servers.<br />C. Create a VPN connection between each new account and
		the DX <a href="#VPC">VPC</a>. Route the network traffic to the on&#8211;premises servers.<br />D. Configure AWS
		Transit Gateway between the accounts. Assign DX to the transit gateway and route network traffic to the
		on&#8211;premises servers.<br /><br /><b>Correct Answer:</b><br />D. Configure AWS Transit Gateway between the
		accounts. Assign DX to the transit gateway and route network traffic to the on&#8211;premises servers.<br />
	</div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 176<br />A solutions architect is designing a
		solution where users will be directed to a backup static error page if the primary website is unavailable. The
		primary website&aposs DNS records are hosted in Amazon Route 53 where their domain is pointing to an Application
		Load Balancer (ALB).<br /><br />Which configuration should the solutions architect use to meet the company&aposs
		needs while minimizing changes and infrastructure overhead?<br /><br />A. Point a Route 53 alias record to an
		Amazon <a href="#CloudFront">CloudFront</a> distribution with the ALB as one of its origins. Then, create custom
		error pages for the distribution.<br />B. Set up a Route 53 active&#8211;passive failover configuration. Direct
		traffic to a static error page hosted within an Amazon <a href="#S3">S3</a> bucket when Route 53 health checks
		determine that the ALB endpoint is unhealthy.<br />C. Update the Route 53 record to use a latency&#8211;based
		routing policy. Add the backup static error page hosted within an Amazon <a href="#S3">S3</a> bucket to the
		record so the traffic is sent to the most responsive endpoints.<br />D. Set up a Route 53 active&#8211;active
		configuration with the ALB and an Amazon <a href="#EC2">EC2</a> instance hosting a static error page as
		endpoints. Route 53 will only send requests to the instance if the health checks fail for the
		ALB.<br /><br /><b>Correct Answer:</b><br />B. Set up a Route 53 active&#8211;passive failover configuration.
		Direct traffic to a static error page hosted within an Amazon <a href="#S3">S3</a> bucket when Route 53 health
		checks determine that the ALB endpoint is unhealthy.<br /><br />Answer Description:<br />Active&#8211;passive
		failover<br />Use an active&#8211;passive failover configuration when you want a primary resource or group of
		resources to be available the majority of the time and you want a secondary resource or group of resources to be
		on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes
		only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only
		the healthy secondary resources in response to DNS queries.<br /><br />To create an active&#8211;passive
		failover configuration with one primary record and one secondary record, you just create the records and specify
		Failover for the routing policy. When the primary resource is healthy, Route 53 responds to DNS queries using
		the primary record. When the primary resource is unhealthy, Route 53 responds to DNS queries using the secondary
		record.<br /><br />How Amazon Route 53 averts cascading failures<br />As the first defense against cascading
		failures, each request routing algorithm (such as weighted and failover) has a mode of last resort. In this
		special mode, when all records are considered unhealthy, the Route 53 algorithm reverts to considering all
		records healthy.<br /><br />For example, if all instances of an application, on several hosts, are rejecting
		health check requests, Route 53 DNS servers will choose an answer anyway and return it rather than returning no
		DNS answer or returning an NXDOMAIN (non&#8211;existent domain) response. An application can respond to users
		but still fail health checks, so this provides some protection against misconfiguration.<br /><br />Similarly,
		if an application is overloaded, and one out of three endpoints fails its health checks, so that it&aposs
		excluded from Route 53 DNS responses, Route 53 distributes responses between the two remaining endpoints. If the
		remaining endpoints are unable to handle the additional load and they fail, Route 53 reverts to distributing
		requests to all three endpoints.<br /><br />Using Amazon <a href="#CloudFront">CloudFront</a> as the
		front&#8211;end provides the option to specify a custom message instead of the default message. To specify the
		specific file that you want to return and the errors for which the file should be returned, you update your <a
			href="#CloudFront">CloudFront</a> distribution to specify those values.<br /><br />For example, the
		following is a customized error message:<br /><br />The <a href="#CloudFront">CloudFront</a> distribution can
		use the ALB as the origin, which will cause the website content to be cached on the <a
			href="#CloudFront">CloudFront</a> edge caches.<br /><br />This solution represents the most operationally
		efficient choice as no action is required in the event of an issue, other than troubleshooting the root
		cause.<br /><br />References:<br /><br />Amazon <a href="#CloudFront">CloudFront</a> > Developer Guide > What is
		Amazon <a href="#CloudFront">CloudFront</a>?</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 177<br />An application running on AWS uses an
		Amazon Aurora Multi&#8211;AZ deployment for its database. When evaluating performance metrics, a solutions
		architect discovered that the database reads are causing high I/O and adding latency to the write requests
		against the database.<br /><br />What should the solutions architect do to separate the read requests from the
		write requests?<br /><br />A. Enable read&#8211;through caching on the Amazon Aurora database.<br />B. Update
		the application to read from the Multi&#8211;AZ standby instance.<br />C. Create a read replica and modify the
		application to use the appropriate endpoint.<br />D. Create a second Amazon Aurora database and link it to the
		primary database as a read replica.<br /><br /><b>Correct Answer:</b><br />C. Create a read replica and modify
		the application to use the appropriate endpoint.<br /><br />Answer Description:<br />Amazon RDS Read Replicas
		provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically
		scale out beyond the capacity constraints of a single DB instance for read&#8211;heavy database workloads. You
		can create one or more replicas of a given source DB Instance and serve high&#8211;volume application read
		traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also
		be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL,
		MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora.<br /><br />For the MySQL, MariaDB,
		PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of
		the source DB instance. It then uses the engines&apos native asynchronous replication to update the read replica
		whenever there is a change to the source DB instance. The read replica operates as a DB instance that allows
		only read&#8211;only connections; applications can connect to a read replica just as they would to any DB
		instance. Amazon RDS replicates all databases in the source DB instance.<br /><br />Amazon Aurora further
		extends the benefits of read replicas by employing an SSD&#8211;backed virtualized storage layer
		purpose&#8211;built for database workloads. Amazon Aurora replicas share the same underlying storage as the
		source instance, lowering costs and avoiding the need to copy data to the replica nodes. For more information
		about replication with Amazon Aurora, see the online documentation.<br /><br />Amazon Aurora<br /><br />Aurora
		Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing
		availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans
		within an AWS Region.<br /><br />The DB cluster volume is made up of multiple copies of the data for the DB
		cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary
		instance and to Aurora Replicas in the DB cluster.<br /><br />As well as providing scaling for reads, Aurora
		Replicas are also targets for multi&#8211;AZ. In this case the solutions architect can update the application to
		read from the Multi&#8211;AZ standby instance.<br /><br />References:<br /><br />Amazon Aurora > User Guide for
		Aurora > Replication with Amazon Aurora</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 178<br />A recently acquired company is required
		to build its own infrastructure on AWS and migrate multiple applications to the cloud within a month. Each
		application has approximately 50 TB of data to be transferred. After the migration is complete, this company and
		its parent company will both require secure network connectivity with consistent throughput from their data
		centers to the applications. A solutions architect must ensure one&#8211;time data migration and ongoing network
		connectivity.<br /><br />Which solution will meet these requirements?<br /><br />A. AWS Direct Connect for both
		the initial transfer and ongoing connectivity.<br />B. AWS Site&#8211;to&#8211;Site VPN for both the initial
		transfer and ongoing connectivity.<br />C. AWS Snowball for the initial transfer and AWS Direct Connect for
		ongoing connectivity.<br />D. AWS Snowball for the initial transfer and AWS Site&#8211;to&#8211;Site VPN for
		ongoing connectivity.<br /><br /><b>Correct Answer:</b><br />C. AWS Snowball for the initial transfer and AWS
		Direct Connect for ongoing connectivity.<br /><br />Answer Description:<br />&quot;Each application has
		approximately 50 TB of data to be transferred&quot; = AWS Snowball; &quot;secure network connectivity with
		consistent throughput from their data centers to the applications&quot;<br /><br />What are the benefits of
		using AWS Direct Connect and private network connections? In many circumstances, private network connections can
		reduce costs, increase bandwidth, and provide a more consistent network experience than Internet&#8211;based
		connections. &quot;more consistent network experience&quot;, hence AWS Direct Connect.<br /><br />Direct Connect
		is better than VPN; reduced cost+increased bandwith+(remain connection or consistent network) = direct
		connect<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 179<br />A company serves content to its
		subscribers across the world using an application running on AWS. The application has several Amazon <a
			href="#EC2">EC2</a> instances in a private subnet behind an Application Load Balancer (ALB). Due to a recent
		change in copyright restrictions, the chief information officer (CIO) wants to block access for certain
		countries.<br /><br />Which action will meet these requirements?<br /><br />A. Modify the ALB security group to
		deny incoming traffic from blocked countries.<br />B. Modify the security group for <a href="#EC2">EC2</a>
		instances to deny incoming traffic from blocked countries.<br />C. Use Amazon <a
			href="#CloudFront">CloudFront</a> to serve the application and deny access to blocked countries.<br />D. Use
		ALB listener rules to return access denied responses to incoming traffic from blocked
		countries.<br /><br /><b>Correct Answer:</b><br />C. Use Amazon <a href="#CloudFront">CloudFront</a> to serve
		the application and deny access to blocked countries.<br /><br />Answer Description:<br />&quot;block access for
		certain countries.&quot; You can use geo restriction, also known as geo blocking, to prevent users in specific
		geographic locations from accessing content that you&aposre distributing through a <a
			href="#CloudFront">CloudFront</a> web distribution.<br /><br />When a user requests your content, <a
			href="#CloudFront">CloudFront</a> typically serves the requested content regardless of where the user is
		located. If you need to prevent users in specific countries from accessing your content, you can use the <a
			href="#CloudFront">CloudFront</a> geo restriction feature to do one of the following:<br /><br /><a
			href="#All">All</a>ow your users to access your content only if they&aposre in one of the countries on a
		whitelist of approved countries.<br /><br />Prevent your users from accessing your content if they&aposre in one
		of the countries on a blacklist of banned countries.<br /><br />For example, if a request comes from a country
		where, for copyright reasons, you are not authorized to distribute your content, you can use <a
			href="#CloudFront">CloudFront</a> geo restriction to block the request. This is the easiest and most
		effective way to implement a geographic restriction for the delivery of content.<br /><br />CORRECT: &quot;Use
		Amazon <a href="#CloudFront">CloudFront</a> to serve the application and deny access to blocked countries&quot;
		is the correct answer.<br /><br />INCORRECT: &quot;Use a Network ACL to block the IP address ranges associated
		with the specific countries&quot; is incorrect as this would be extremely difficult to
		manage.<br /><br />INCORRECT: &quot;Modify the ALB security group to deny incoming traffic from blocked
		countries&quot; is incorrect as security groups cannot block traffic by country.<br /><br />INCORRECT:
		&quot;Modify the security group for <a href="#EC2">EC2</a> instances to deny incoming traffic from blocked
		countries&quot; is incorrect as security groups cannot block traffic by
		country.<br /><br />References:<br /><br />Amazon <a href="#CloudFront">CloudFront</a> > Developer Guide >
		Restricting the geographic distribution of your content</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 180<br />A company wants to migrate a high
		performance computing (HPC) application and data from on&#8211;premises to the AWS Cloud. The company uses
		tiered storage on&#8211;premises with hot high&#8211;performance parallel storage to support the application
		during periodic runs of the application, and more economical cold storage to hold the data when the application
		is not actively running.<br /><br />Which combination of solutions should a solutions architect recommend to
		support the storage needs of the application? (Choose two.)<br /><br />A. Amazon <a href="#S3">S3</a> for cold
		data storage<br />B. Amazon EFS for cold data storage<br />C. Amazon <a href="#S3">S3</a> for
		high&#8211;performance parallel storage<br />D. Amazon FSx for Lustre for high&#8211;performance parallel
		storage<br />E. Amazon FSx for Windows for high&#8211;performance parallel storage<br /><br /><b>Correct
			Answer:</b><br />A. Amazon <a href="#S3">S3</a> for cold data storage<br />D. Amazon FSx for Lustre for
		high&#8211;performance parallel storage<br /><br />Answer Description:<br />Amazon FSx for Lustre makes it easy
		and cost effective to launch and run the world&aposs most popular high&#8211;performance file system. Use it for
		workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and
		financial modeling.<br /><br />Amazon FSx for Lustre provides a high&#8211;performance file system optimized for
		fast processing of workloads such as machine learning, high&#8211;performance computing (HPC), video processing,
		financial modeling, and electronic design automation (EDA).<br /><br />These workloads commonly require data to
		be presented via a fast and scalable file system interface, and typically have data sets stored on
		long&#8211;term data stores like Amazon <a href="#S3">S3</a>.<br /><br />Amazon FSx works natively with Amazon
		<a href="#S3">S3</a>, making it easy to access your <a href="#S3">S3</a> data to run data processing workloads.
		Your <a href="#S3">S3</a> objects are presented as files in your file system, and you can write your results
		back to <a href="#S3">S3</a>. This lets you run data processing workloads on FSx for Lustre and store your
		long&#8211;term data on <a href="#S3">S3</a> or on&#8211;premises data stores.<br /><br />Therefore, the best
		combination for this scenario is to use <a href="#S3">S3</a> for cold data and FSx for Lustre for the parallel
		HPC job.<br /><br />CORRECT: &quot;Amazon <a href="#S3">S3</a> for cold data storage&quot; is the correct
		answer.<br /><br />CORRECT: &quot;Amazon FSx for Lustre for high&#8211;performance parallel storage&quot; is the
		correct answer. INCORRECT: &quot;Amazon EFS for cold data storage&quot; is incorrect as FSx works natively with
		<a href="#S3">S3</a> which is also more economical.<br /><br />INCORRECT: &quot;Amazon <a href="#S3">S3</a> for
		high&#8211;performance parallel storage&quot; is incorrect as <a href="#S3">S3</a> is not suitable for running
		high&#8211;performance computing jobs.<br /><br />INCORRECT: &quot;Amazon FSx for Windows for
		high&#8211;performance parallel storage&quot; is incorrect as FSx for Lustre should be used for HPC use cases
		and use cases that require storing data on <a href="#S3">S3</a>.<br /><br />References:<br /><br />Amazon FSx
		for Lustre<br /><br /><br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 181<br />A company&aposs legacy application is
		currently relying on a single&#8211;instance Amazon RDS MySQL database without encryption. Due to new compliance
		requirements, all existing and new data in this database must be encrypted.<br /><br />How should this be
		accomplished?<br /><br />A. Create an Amazon <a href="#S3">S3</a> bucket with server&#8211;side encryption
		enabled. Move all the data to Amazon <a href="#S3">S3</a>. Delete the RDS instance.<br />B. Enable RDS
		Multi&#8211;AZ mode with encryption at rest enabled. Perform a failover to the standby instance to delete the
		original instance.<br />C. Take a Snapshot of the RDS instance. Create an encrypted copy of the snapshot.
		Restore the RDS instance from the encrypted snapshot.<br />D. Create an RDS read replica with encryption at rest
		enabled. Promote the read replica to master and switch the over to the new master. Delete the old RDS
		instance.<br /><br /><b>Correct Answer:</b><br />C. Take a Snapshot of the RDS instance. Create an encrypted
		copy of the snapshot. Restore the RDS instance from the encrypted snapshot.<br /><br />Answer
		Description:<br />How do I encrypt Amazon RDS snapshots?<br />The following steps are applicable to Amazon RDS
		for MySQL, Oracle, SQL Server, PostgreSQL, or MariaDB.<br /><br />Important: If you use Amazon Aurora, you can
		restore an unencrypted Aurora DB cluster snapshot to an encrypted Aurora DB cluster if you specify an AWS Key
		Management Service (AWS KMS) encryption key when you restore from the unencrypted DB cluster snapshot. For more
		information, see Limitations of Amazon RDS Encrypted DB Instances.<br /><br />Open the Amazon RDS console, and
		then choose Snapshots from the navigation pane.<br /><br />Select the snapshot that you want to
		encrypt.<br /><br />Under Snapshot Actions, choose Copy Snapshot.<br /><br />Choose your Destination Region, and
		then enter your New DB Snapshot Identifier.<br /><br />Change Enable Encryption to Yes.<br /><br />Select your
		Master Key from the list, and then choose Copy Snapshot.<br /><br />After the snapshot status is available, the
		Encrypted field will be True to indicate that the snapshot is encrypted.<br /><br />You now have an encrypted
		snapshot of your DB. You can use this encrypted DB snapshot to restore the DB instance from the DB
		snapshot.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 182<br />A solutions architect at an eCommerce
		company wants to back up application log data to Amazon <a href="#S3">S3</a>. The solutions architect is unsure
		how frequently the logs will be accessed or which logs will be accessed the most. The company wants to keep
		costs as low as possible by using the appropriate <a href="#S3">S3</a> storage class.<br /><br />Which <a
			href="#S3">S3</a> storage class should be implemented to meet these requirements?<br /><br />A. <a
			href="#S3">S3</a> Glacier<br />B. <a href="#S3">S3</a> Intelligent&#8211;Tiering<br />C. <a
			href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a> Standard&#8211;IA)<br />D. <a
			href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />B. <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br /><br />Answer Description:<br /><a href="#S3">S3</a> Intelligent&#8211;Tiering is
		a new Amazon <a href="#S3">S3</a> storage class designed for customers who want to optimize storage costs
		automatically when data access patterns change, without performance impact or operational overhead. <a
			href="#S3">S3</a> Intelligent&#8211;Tiering is the first cloud object storage class that delivers automatic
		cost savings by moving data between two access tiers &#8212; frequent access and infrequent access &#8212; when
		access patterns change, and is ideal for data with unknown or changing access patterns.<br /><br /><a
			href="#S3">S3</a> Intelligent&#8211;Tiering stores objects in two access tiers: one tier that is optimized
		for frequent access and another lower&#8211;cost tier that is optimized for infrequent access. For a small
		monthly monitoring and automation fee per object, <a href="#S3">S3</a> Intelligent&#8211;Tiering monitors access
		patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access
		tier.<br /><br />There are no retrieval fees in <a href="#S3">S3</a> Intelligent&#8211;Tiering. If an object in
		the infrequent access tier is accessed later, it is automatically moved back to the frequent access tier. No
		additional tiering fees apply when objects are moved between access tiers within the <a href="#S3">S3</a>
		Intelligent&#8211;Tiering storage class. <a href="#S3">S3</a> Intelligent&#8211;Tiering is designed for 99.9%
		availability and 99.999999999% durability, and offers the same low latency and high throughput performance of <a
			href="#S3">S3</a> Standard.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 183<br />A solutions architect is designing
		storage for a high performance computing (HPC) environment based on Amazon Linux. The workload stores and
		processes a large amount of engineering drawings that require shared storage and heavy
		computing.<br /><br />Which storage option would be the optimal solution?<br /><br />A. Amazon Elastic File
		System (Amazon EFS)<br />B. Amazon FSx for Lustre<br />C. Amazon <a href="#EC2">EC2</a> instance store<br />D.
		Amazon EBS Provisioned IOPS SSD (io1)<br /><br /><b>Correct Answer:</b><br />B. Amazon FSx for
		Lustre<br /><br />Answer Description:<br />Amazon FSx for Lustre is a new, fully managed service provided by AWS
		based on the Lustre file system.<br /><br />Amazon FSx for Lustre provides a high&#8211;performance file system
		optimized for fast processing of workloads such as machine learning, high performance computing (HPC), video
		processing, financial modeling, and electronic design automation (EDA).<br /><br />FSx for Lustre allows
		customers to create a Lustre filesystem on demand and associate it to an Amazon <a href="#S3">S3</a> bucket. As
		part of the filesystem creation, Lustre reads the objects in the buckets and adds that to the file system
		metadata. Any Lustre client in your <a href="#VPC">VPC</a> is then able to access the data, which gets cached on
		the high&#8211;speed Lustre filesystem. This is ideal for HPC workloads, because you can get the speed of an
		optimized Lustre file system without having to manage the complexity of deploying, optimizing, and managing the
		Lustre cluster.<br /><br />Additionally, having the filesystem work natively with Amazon <a href="#S3">S3</a>
		means you can shut down the Lustre filesystem when you don&apost need it but still access objects in Amazon <a
			href="#S3">S3</a> via other AWS Services. FSx for Lustre also allows you to also write the output of your
		HPC job back to Amazon <a href="#S3">S3</a>.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 184<br />A company is running an eCommerce
		application on Amazon <a href="#EC2">EC2</a>. The application consists of a stateless web tier that requires a
		minimum of 10 instances, and a peak of 250 instances to support the application&aposs usage. The application
		requires 50 instances 80% of the time.<br /><br />Which solution should be used to minimize costs?<br /><br />A.
		Purchase Reserved Instances to cover 250 instances.<br />B. Purchase Reserved Instances to cover 80 instances.
		Use Spot Instances to cover the remaining instances.<br />C. Purchase On&#8211;Demand Instances to cover 40
		instances. Use Spot Instances to cover the remaining instances.<br />D. Purchase Reserved Instances to cover 50
		instances. Use On&#8211;Demand and Spot Instances to cover the remaining instances.<br /><br /><b>Correct
			Answer:</b><br />D. Purchase Reserved Instances to cover 50 instances. Use On&#8211;Demand and Spot
		Instances to cover the remaining instances.<br /><br />Answer Description:<br />Reserved Instances<br />Having
		50 <a href="#EC2">EC2</a> RIs provide a discounted hourly rate and an optional capacity reservation for <a
			href="#EC2">EC2</a> instances. AWS Billing automatically applies your RI&aposs discounted rate when
		attributes of <a href="#EC2">EC2</a> instance usage match attributes of an active RI.<br /><br />If an
		Availability Zone is specified, <a href="#EC2">EC2</a> reserves capacity matching the attributes of the RI. The
		capacity reservation of an RI is automatically utilized by running instances matching these
		attributes.<br /><br />You can also choose to forego the capacity reservation and purchase an RI that is scoped
		to a region. RIs that are scoped to a region automatically apply the RI&aposs discount to instance usage across
		AZs and instance sizes in a region, making it easier for you to take advantage of the RI&aposs discounted
		rate.<br /><br />On&#8211;Demand Instance<br />On&#8211;Demand instances let you pay for compute capacity by the
		hour or second (minimum of 60 seconds) with no long&#8211;term commitments. This frees you from the costs and
		complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed
		costs into much smaller variable costs.<br /><br />The pricing below includes the cost to run private and public
		AMIs on the specified operating system (&quot;Windows Usage&quot; prices apply to Windows Server 2003 R2, 2008,
		2008 R2, 2012, 2012 R2, 2016, and 2019). Amazon also provides you with additional instances for Amazon <a
			href="#EC2">EC2</a> running Microsoft Windows with SQL Server, Amazon <a href="#EC2">EC2</a> running SUSE
		Linux Enterprise Server, Amazon <a href="#EC2">EC2</a> running Red Hat Enterprise Linux and Amazon <a
			href="#EC2">EC2</a> running IBM that are priced differently.<br /><br />Spot Instances<br />A Spot Instance
		is an unused <a href="#EC2">EC2</a> instance that is available for less than the On&#8211;Demand price. Because
		Spot Instances enable you to request unused <a href="#EC2">EC2</a> instances at steep discounts, you can lower
		your Amazon <a href="#EC2">EC2</a> costs significantly. The hourly price for a Spot Instance is called a Spot
		price. The Spot price of each instance type in each Availability Zone is set by Amazon <a href="#EC2">EC2</a>,
		and adjusted gradually based on the long&#8211;term supply of and demand for Spot Instances. Your Spot Instance
		runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot
		price.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 185<br />A solutions architect is implementing a
		document review application using an Amazon <a href="#S3">S3</a> bucket for storage. The solution must prevent
		an accidental deletion of the documents and ensure that all versions of the documents are available. Users must
		be able to download, modify, and upload documents.<br /><br />Which combination of actions should be taken to
		meet these requirements? (Choose two.)<br /><br />A. Enable a read&#8211;only bucket ACL.<br />B. Enable
		versioning on the bucket.<br />C. Attach an <a href="#IAM">IAM</a> policy to the bucket.<br />D. Enable MFA
		Delete on the bucket.<br />E. Encrypt the bucket using AWS KMS.<br /><br /><b>Correct Answer:</b><br />B. Enable
		versioning on the bucket.<br />D. Enable MFA Delete on the bucket.<br /><br />Answer Description:<br />Object
		Versioning<br />Use Amazon <a href="#S3">S3</a> Versioning to keep multiple versions of an object in one bucket.
		For example, you could store my&#8211;image.jpg (version 111111) and my&#8211;image.jpg (version 222222) in a
		single bucket. <a href="#S3">S3</a> Versioning protects you from the consequences of unintended overwrites and
		deletions. You can also use it to archive objects so that you have access to previous versions.<br /><br />To
		customize your data retention approach and control storage costs, use object versioning with Object lifecycle
		management. For information about creating <a href="#S3">S3</a> Lifecycle policies using the AWS Management
		Console, see How Do I Create a Lifecycle Policy for an <a href="#S3">S3</a> Bucket? in the Amazon Simple Storage
		Service Console User Guide.<br /><br />If you have an object expiration lifecycle policy in your
		non&#8211;versioned bucket and you want to maintain the same permanent delete behavior when you enable
		versioning, you must add a noncurrent expiration policy. The noncurrent expiration lifecycle policy will manage
		the deletes of the noncurrent object versions in the version&#8211;enabled bucket. (A version&#8211;enabled
		bucket maintains one current and zero or more noncurrent object versions.)<br /><br />You must explicitly enable
		<a href="#S3">S3</a> Versioning on your bucket. By default, <a href="#S3">S3</a> Versioning is disabled.
		Regardless of whether you have enabled Versioning, each object in your bucket has a version ID. If you have not
		enabled Versioning, Amazon <a href="#S3">S3</a> sets the value of the version ID to null. If <a
			href="#S3">S3</a> Versioning is enabled, Amazon <a href="#S3">S3</a> assigns a version ID value for the
		object. This value distinguishes it from other versions of the same key.<br /><br />Enabling and suspending
		versioning is done at the bucket level. When you enable versioning on an existing bucket, objects that are
		already stored in the bucket are unchanged. The version IDs (null), contents, and permissions remain the same.
		After you enable <a href="#S3">S3</a> Versioning for a bucket, each object that is added to the bucket gets a
		version ID, which distinguishes it from other versions of the same key.<br /><br />Only Amazon <a
			href="#S3">S3</a> generates version IDs, and they can&apost be edited. Version IDs are Unicode, UTF&#8211;8
		encoded, URL&#8211;ready, opaque strings that are no more than 1,024 bytes long. The following is an example:
		3/L4kqtJlcpXroDTDmJ+rmSpXd3dIbrHY+MTRCxf3vjVBH40Nr8X8gdRQBpUMLUo.<br /><br />Using MFA delete<br />If a
		bucket&aposs versioning configuration is MFA Delete&#8212;enabled, the bucket owner must include the
		x&#8211;amz&#8211;mfa request header in requests to permanently delete an object version or change the
		versioning state of the bucket. Requests that include x&#8211;amz&#8211;mfa must use HTTPS. The header&aposs
		value is the concatenation of your authentication device&aposs serial number, a space, and the authentication
		code displayed on it. If you do not include this request header, the request fails.<br /><br />None of the
		options present a good solution for specifying permissions required to write and modify objects so that
		requirement needs to be taken care of separately. The other requirements are to prevent accidental deletion and
		the ensure that all versions of the document are available. The two solutions for these requirements are
		versioning and MFA delete. Versioning will retain a copy of each version of the document and multi&#8211;factor
		authentication delete (MFA delete) will prevent any accidental deletion as you need to supply a second factor
		when attempting a delete. CORRECT: &quot;Enable versioning on the bucket&quot; is a correct
		answer.<br /><br />CORRECT: &quot;Enable MFA Delete on the bucket&quot; is also a correct
		answer.<br /><br />INCORRECT: &quot;Set read&#8211;only permissions on the bucket&quot; is incorrect as this
		will also prevent any writing to the bucket which is not desired.<br /><br />INCORRECT: &quot;Attach an <a
			href="#IAM">IAM</a> policy to the bucket&quot; is incorrect as users need to modify documents which will
		also allow delete. Therefore, a method must be implemented to just control deletes.<br /><br />INCORRECT:
		&quot;Encrypt the bucket using AWS SSE&#8211;<a href="#S3">S3</a>&quot; is incorrect as encryption doesn&apost
		stop you from deleting an object.<br /><br />References:<br /><br />Amazon Simple Storage Service > User Guide >
		Using versioning in <a href="#S3">S3</a> buckets<br />Amazon Simple Storage Service > User Guide > Deleting an
		object from an MFA delete&#8211;enabled bucket</div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 186<br />A solutions architect is designing a
		two&#8211;tier web application. The application consists of a public&#8211;facing web tier hosted on Amazon <a
			href="#EC2">EC2</a> in public subnets. The database tier consists of Microsoft SQL Server running on Amazon
		<a href="#EC2">EC2</a> in a private subnet. Security is a high priority for the company.<br /><br />How should
		security groups be configured in this situation? (Choose two.)<br /><br />A. Configure the security group for
		the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.<br />B. Configure the security group for the
		web tier to allow outbound traffic on port 443 from 0.0.0.0/0.<br />C. Configure the security group for the
		database tier to allow inbound traffic on port 1433 from the security group for the web tier.<br />D. Configure
		the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group
		for the web tier.<br />E. Configure the security group for the database tier to allow inbound traffic on ports
		443 and 1433 from the security group for the web tier.<br /><br /><b>Correct Answer:</b><br />A. Configure the
		security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.<br />C. Configure the
		security group for the database tier to allow inbound traffic on port 1433 from the security group for the web
		tier.<br /><br />Answer Description:<br />In this scenario an inbound rule is required to allow traffic from any
		internet client to the web front end on SSL/TLS port 443. The source should therefore be set to 0.0.0.0/0 to
		allow any inbound traffic.<br /><br />To secure the connection from the web frontend to the database tier, an
		outbound rule should be created from the public <a href="#EC2">EC2</a> security group with a destination of the
		private <a href="#EC2">EC2</a> security group. The port should be set to 1433 for MySQL. The private <a
			href="#EC2">EC2</a> security group will also need to allow inbound traffic on 1433 from the public <a
			href="#EC2">EC2</a> security group.<br /><br />This configuration can be seen in the
		diagram:<br /><br />CORRECT: &quot;Configure the security group for the web tier to allow inbound traffic on
		port 443 from 0.0.0.0/0&quot; is a correct answer.<br /><br />CORRECT: &quot;Configure the security group for
		the database tier to allow inbound traffic on port 1433 from the security group for the web tier&quot; is also a
		correct answer.<br /><br />INCORRECT: &quot;Configure the security group for the web tier to allow outbound
		traffic on port 443 from 0.0.0.0/0&quot; is incorrect as this is configured backwards.<br /><br />INCORRECT:
		&quot;Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the
		security group for the web tier&quot; is incorrect as the MySQL database instance does not need to send outbound
		traffic on either of these ports.<br /><br />INCORRECT: &quot;Configure the security group for the database tier
		to allow inbound traffic on ports 443 and 1433 from the security group for the web tier&quot; is incorrect as
		the database tier does not need to allow inbound traffic on port 443.<br /><br />References:<br />Amazon Virtual
		Private Cloud > User Guide > Security groups for your <a href="#VPC">VPC</a><br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 187<br />A company hosts its product information
		webpages on AWS. The existing solution uses multiple Amazon C2 instances behind an Application Load Balancer in
		an Auto Scaling group. The website also uses a custom DNS name and communicates with HTTPS only using a
		dedicated SSL certificate. The company is planning a new product launch and wants to be sure that users from
		around the world have the best possible experience on the new website.<br /><br />What should a solutions
		architect do to meet these requirements?<br /><br />A. Redesign the application to use Amazon <a
			href="#CloudFront">CloudFront</a>.<br />B. Redesign the application to use AWS Elastic Beanstalk.<br />C.
		Redesign the application to use a Network Load Balancer.<br />D. Redesign the application to use Amazon <a
			href="#S3">S3</a> static website hosting.<br /><br /><b>Correct Answer:</b><br />A. Redesign the application
		to use Amazon <a href="#CloudFront">CloudFront</a>.<br /><br />Answer Description:<br />What Is Amazon <a
			href="#CloudFront">CloudFront</a>?<br />Amazon <a href="#CloudFront">CloudFront</a> is a web service that
		speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to
		your users. <a href="#CloudFront">CloudFront</a> delivers your content through a worldwide network of data
		centers called edge locations. When a user requests content that you&aposre serving with <a
			href="#CloudFront">CloudFront</a>, the user is routed to the edge location that provides the lowest latency
		(time delay), so that content is delivered with the best possible performance.<br /><br />If the content is
		already in the edge location with the lowest latency, <a href="#CloudFront">CloudFront</a> delivers it
		immediately.<br /><br />If the content is not in that edge location, <a href="#CloudFront">CloudFront</a>
		retrieves it from an origin that you&aposve defined &#8212; such as an Amazon <a href="#S3">S3</a> bucket, a
		MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for
		the definitive version of your content.<br /><br />As an example, suppose that you&aposre serving an image from
		a traditional web server, not from <a href="#CloudFront">CloudFront</a>. For example, you might serve an image,
		sunsetphoto.png, using the URL http://example.com/sunsetphoto.png.<br /><br />Your users can easily navigate to
		this URL and see the image. But they probably don&apost know that their request was routed from one network to
		another &#8212; through the complex collection of interconnected networks that comprise the internet &#8212;
		until the image was found.<br /><br /><a href="#CloudFront">CloudFront</a> speeds up the distribution of your
		content by routing each user request through the AWS backbone network to the edge location that can best serve
		your content. Typically, this is a <a href="#CloudFront">CloudFront</a> edge server that provides the fastest
		delivery to the viewer. Using the AWS network dramatically reduces the number of networks that your users&apos
		requests must pass through, which improves performance. Users get lower latency &#8212; the time it takes to
		load the first byte of the file &#8212; and higher data transfer rates.<br /><br />You also get increased
		reliability and availability because copies of your files (also known as objects) are now held (or cached) in
		multiple edge locations around the world.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 188<br />A solutions architect is designing the
		cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding
		and removing application nodes as needed based on the number of jobs to be processed. The processor application
		is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are
		durably stored.<br /><br />Which design should the solutions architect use?<br /><br />A. Create an Amazon SNS
		topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the
		processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the
		launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU
		usage.<br />B. Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine
		Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create
		an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add
		and remove nodes based on network usage.<br />C. Create an Amazon SQS queue to hold the jobs that need to be
		processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch
		template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for
		the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.<br />D. Create an
		Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists
		of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the
		launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number
		of messages published to the SNS topic.<br /><br /><b>Correct Answer:</b><br />C. Create an Amazon SQS queue to
		hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor
		application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template.
		Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the
		SQS queue.<br /><br />Answer Description:<br />Amazon Simple Queue Service (SQS) is a fully managed message
		queuing service that enables you to decouple and scale microservices, distributed systems, and serverless
		applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented
		middleware, and empowers developers to focus on differentiating work. Using SQS, you can send, store, and
		receive messages between software components at any volume, without losing messages or requiring other services
		to be available. Get started with SQS in minutes using the AWS console, Command Line Interface or SDK of your
		choice, and three simple commands.<br /><br />SQS offers two types of message queues. Standard queues offer
		maximum throughput, best&#8211;effort ordering, and at&#8211;least&#8211;once delivery. SQS FIFO queues are
		designed to guarantee that messages are processed exactly once, in the exact order that they are
		sent.<br /><br />Scaling Based on Amazon SQS<br />There are some scenarios where you might think about scaling
		in response to activity in an Amazon SQS queue. For example, suppose that you have a web app that lets users
		upload images and use them online. In this scenario, each image requires resizing and encoding before it can be
		published. The app runs on <a href="#EC2">EC2</a> instances in an Auto Scaling group, and it&aposs configured to
		handle your typical upload rates. Unhealthy instances are terminated and replaced to maintain current instance
		levels at all times. The app places the raw bitmap data of the images in an SQS queue for processing. It
		processes the images and then publishes the processed images where they can be viewed by users. The architecture
		for this scenario works well if the number of image uploads doesn&apost vary over time. But if the number of
		uploads changes over time, you might consider using dynamic scaling to scale the capacity of your Auto Scaling
		group.<br /><br />In this case we need to find a durable and loosely coupled solution for storing jobs. Amazon
		SQS is ideal for this use case and can be configured to use dynamic scaling based on the number of jobs waiting
		in the queue.<br /><br />To configure this scaling you can use the backlog per instance metric with the target
		value being the acceptable backlog per instance to maintain. You can calculate these numbers as follows: Backlog
		per instance: To calculate your backlog per instance, start with the ApproximateNumberOfMessages queue attribute
		to determine the length of the SQS queue (number of messages available for retrieval from the queue). Divide
		that number by the fleet&aposs running capacity, which for an Auto Scaling group is the number of instances in
		the InService state, to get the backlog per instance.<br /><br />Acceptable backlog per instance: To calculate
		your target value, first determine what your application can accept in terms of latency. Then, take the
		acceptable latency value and divide it by the average time that an <a href="#EC2">EC2</a> instance takes to
		process a message.<br /><br />This solution will scale <a href="#EC2">EC2</a> instances using Auto Scaling based
		on the number of jobs waiting in the SQS queue.<br /><br />CORRECT: &quot;Create an Amazon SQS queue to hold the
		jobs that needs to be processed. Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group for the compute
		application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of
		items in the SQS queue&quot; is the correct answer.<br /><br />INCORRECT: &quot;Create an Amazon SQS queue to
		hold the jobs that need to be processed. Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group for the
		compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network
		usage&quot; is incorrect as scaling on network usage does not relate to the number of jobs waiting to be
		processed.<br /><br />INCORRECT: &quot;Create an Amazon SNS topic to send the jobs that need to be processed.
		Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group for the compute application. Set the scaling policy
		for the<br /><br />Auto Scaling group to add and remove nodes based on CPU usage&quot; is incorrect. Amazon SNS
		is a notification service so it delivers notifications to subscribers. It does store data durably but is less
		suitable than SQS for this use case. Scaling on CPU usage is not the best solution as it does not relate to the
		number of jobs waiting to be processed.<br /><br />INCORRECT: &quot;Create an Amazon SNS topic to send the jobs
		that need to be processed. Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group for the compute
		application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of
		messages published to the SNS topic&quot; is incorrect. Amazon SNS is a notification service so it delivers
		notifications to subscribers. It does store data durably but is less suitable than SQS for this use case.
		Scaling on the number of notifications in SNS is not possible.<br /><br />References:<br /><br />Amazon <a
			href="#EC2">EC2</a> Auto Scaling > User Guide > Scaling based on Amazon SQS</div><a href="#All">All(200)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 189<br />A company has an application that calls
		AWS Lambda functions. A recent code review found database credentials stored in the source code. The database
		credentials need to be removed from the Lambda source code. The credentials must then be securely stored and
		rotated on an ongoing basis to meet security policy requirements.<br /><br />What should a solutions architect
		recommend to meet these requirements?<br /><br />A. Store the password in AWS CloudHSM. Associate the Lambda
		function with a role that can retrieve the password from CloudHSM given its key ID.<br />B. Store the password
		in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the password from Secrets
		Manager given its secret ID.<br />C. Move the database password to an environment variable associated with the
		Lambda function. Retrieve the password from the environment variable upon execution.<br />D. Store the password
		in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the
		password from AWS KMS given its key ID.<br /><br /><b>Correct Answer:</b><br />B. Store the password in AWS
		Secrets Manager. Associate the Lambda function with a role that can retrieve the password from Secrets Manager
		given its secret ID.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 190<br />A company built an application that lets
		users check in to places they visit, rank the places, and add reviews about their experiences. The application
		is successful with a rapid increase in the number of users every month.<br /><br />The chief technology officer
		fears the database supporting the current Infrastructure may not handle the new load the following month because
		the single Amazon RDS for MySQL instance has triggered alarms related to resource exhaustion due to read
		requests.<br /><br />What can a solutions architect recommend to prevent service Interruptions at the database
		layer with minimal changes to code?<br /><br />A. Create RDS read replicas and redirect read&#8211;only traffic
		to the read replica endpoints. Enable a Multi&#8211;AZ deployment.<br />B. Create an Amazon EMR cluster and
		migrate the data to a Hadoop Distributed File System (HDFS) with a replication factor of 3.<br />C. Create an
		Amazon ElastiCache cluster and redirect all read&#8211;only traffic to the cluster. Set up the cluster to be
		deployed in three Availability Zones.<br />D. Create an Amazon DynamoDB table to replace the RDS instance and
		redirect all read&#8211;only traffic to the DynamoDB table. Enable DynamoDB Accelerator to offload traffic from
		the main table.<br /><br /><b>Correct Answer:</b><br />A. Create RDS read replicas and redirect read&#8211;only
		traffic to the read replica endpoints. Enable a Multi&#8211;AZ deployment.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 191<br />A company is looking for a solution that
		can store video archives in AWS from old news footage. The company needs to minimize costs and will rarely need
		to restore these files. When the files are needed, they must be available in a maximum of five
		minutes.<br /><br />What is the MOST cost&#8211;effective solution?<br /><br />A. Store the video archives in
		Amazon <a href="#S3">S3</a> Glacier and use Expedited retrievals.<br />B. Store the video archives in Amazon <a
			href="#S3">S3</a> Glacier and use Standard retrievals.<br />C. Store the video archives in Amazon <a
			href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a> Standard&#8211;IA).<br />D. Store
		the video archives in Amazon <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA).<br /><br /><b>Correct Answer:</b><br />A. Store the video archives in Amazon <a
			href="#S3">S3</a> Glacier and use Expedited retrievals.<br /></div><a href="#All">All(200)</a> <a href="#"
		<i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 192<br />A healthcare company stores highly
		sensitive patient records. Compliance requires that multiple copies be stored in different locations. Each
		record must be stored for 7 years. The company has a service level agreement (SLA) to provide records to
		government agencies immediately for the first 30 days and then within 4 hours of a request
		thereafter.<br /><br />What should a solutions architect recommend?<br /><br />A. Use Amazon <a
			href="#S3">S3</a> with cross&#8211;Region replication enabled. After 30 days, transition the data to Amazon
		<a href="#S3">S3</a> Glacier using lifecycle policy.<br />B. Use Amazon <a href="#S3">S3</a> with
		cross&#8211;origin resource sharing (CORS) enabled. After 30 days, transition the data to Amazon <a
			href="#S3">S3</a> Glacier using a lifecycle policy.<br />C. Use Amazon <a href="#S3">S3</a> with
		cross&#8211;Region replication enabled. After 30 days, transition the data to Amazon <a href="#S3">S3</a>
		Glacier Deep Achieve using a lifecycle policy.<br />D. Use Amazon <a href="#S3">S3</a> with cross&#8211;origin
		resource sharing (CORS) enabled. After 30 days, transition the data to Amazon <a href="#S3">S3</a> Glacier Deep
		Archive using a lifecycle policy.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon <a href="#S3">S3</a> with
		cross&#8211;Region replication enabled. After 30 days, transition the data to Amazon <a href="#S3">S3</a>
		Glacier using lifecycle policy.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 193<br />A public&#8211;facing web application
		queries a database hosted on an Amazon <a href="#EC2">EC2</a> instance in a private subnet.<br /><br />A large
		number of queries involve multiple table joins, and the application performance has been degrading due to an
		increase in complex queries. The application team will be performing updates to improve
		performance.<br /><br />What should a solutions architect recommend to the application team? (Choose
		two.)<br /><br />A. Cache query data in Amazon SQS<br />B. Create a read replica to offload queries<br />C.
		Migrate the database to Amazon Athena<br />D. Implement Amazon DynamoDB Accelerator to cache data.<br />E.
		Migrate the database to Amazon RDS<br /><br /><b>Correct Answer:</b><br />B. Create a read replica to offload
		queries<br />E. Migrate the database to Amazon RDS<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 194<br />A company runs multiple Amazon <a
			href="#EC2">EC2</a> Linux instances in a <a href="#VPC">VPC</a> with applications that use a hierarchical
		directory structure. The applications need to rapidly and concurrently read and write to shared
		storage.<br /><br />How can this be achieved?<br /><br />A. Create an Amazon EFS file system and mount it from
		each <a href="#EC2">EC2</a> instance.<br />B. Create an Amazon <a href="#S3">S3</a> bucket and permit access
		from all the <a href="#EC2">EC2</a> instances in the <a href="#VPC">VPC</a>.<br />C. Create a file system on an
		Amazon EBS Provisioned IOPS SSD (io1) volume. Attach the volume to all the <a href="#EC2">EC2</a>
		instances.<br />D. Create file systems on Amazon EBS volumes attached to each <a href="#EC2">EC2</a> instance.
		Synchronize the Amazon EBS volumes across the different <a href="#EC2">EC2</a> instances.<br /><br /><b>Correct
			Answer:</b><br />A. Create an Amazon EFS file system and mount it from each <a href="#EC2">EC2</a>
		instance.<br /></div><a href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 195<br />An ecommerce company is running a
		multi&#8211;tier application on AWS. The front&#8211;end and backend tiers both run on Amazon <a
			href="#EC2">EC2</a>, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the
		RDS instance. There are frequent calls to return identical datasets from the database that are causing
		performance slowdowns.<br /><br />Which action should be taken to improve the performance of the
		backend?<br /><br />A. Implement Amazon SNS to store the database calls.<br />B. Implement Amazon ElastiCache to
		cache the large datasets.<br />C. Implement an RDS for MySQL read replica to cache database calls.<br />D.
		Implement Amazon Kinesis Data Firehose to stream the calls to the database.<br /><br /><b>Correct
			Answer:</b><br />B. Implement Amazon ElastiCache to cache the large datasets.<br /></div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 196<br />A company currently stores symmetric
		encryption keys in a hardware security module (HSM). A solutions architect must design a solution to migrate key
		management to AWS. The solution should allow for key rotation and support the use of customer provided
		keys.<br /><br />Where should the key material be stored to meet these requirements?<br /><br />A. Amazon <a
			href="#S3">S3</a><br />B. AWS Secrets Manager<br />C. AWS Systems Manager Parameter store<br />D. AWS Key
		Management Service (AWS KMS)<br /><br /><b>Correct Answer:</b><br />B. AWS Secrets Manager<br /><br />Answer
		Description:<br />AWS Secrets Manager helps you protect secrets needed to access your applications, services,
		and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys,
		and other secrets throughout their lifecycle.<br /><br />References:<br /><br />AWS Secrets Manager</div><a
		href="#All">All(200)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 197<br />A recent analysis of a company&aposs IT
		expenses highlights the need to reduce backup costs. The company&aposs chief information officer wants to
		simplify the on&#8211;premises backup infrastructure and reduce costs by eliminating the use of physical backup
		tapes. The company must preserve the existing investment in the on&#8211;premises backup applications and
		workflows.<br /><br />What should a solutions architect recommend?<br /><br />A. Set up AWS Storage Gateway to
		connect with the backup applications using the NFS interface.<br />B. Set up an Amazon EFS file system that
		connects with the backup applications using the NFS interface.<br />C. Set up an Amazon EFS file system that
		connects with the backup applications using the iSCSI interface.<br />D. Set up AWS Storage Gateway to connect
		with the backup applications using the iSCSI&#8211;virtual tape library (VTL) interface.<br /><br /><b>Correct
			Answer:</b><br />D. Set up AWS Storage Gateway to connect with the backup applications using the
		iSCSI&#8211;virtual tape library (VTL) interface.<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 198<br />A company hosts an application on an
		Amazon <a href="#EC2">EC2</a> instance that requires a maximum of 200 GB storage space. The application is used
		infrequently, with peaks during mornings and evenings. Disk I/O varies, but peaks at 3,000 IOPS. The chief
		financial officer of the company is concerned about costs and has asked a solutions architect to recommend the
		most cost&#8211;effective storage option that does not sacrifice performance.<br /><br />Which solution should
		the solutions architect recommend?<br /><br />A. Amazon EBS Cold HDD (sc1)<br />B. Amazon EBS General Purpose
		SSD (gp2)<br />C. Amazon EBS Provisioned IOPS SSD (io1)<br />D. Amazon EBS Throughput Optimized HDD
		(st1)<br /><br /><b>Correct Answer:</b><br />B. Amazon EBS General Purpose SSD (gp2)<br /><br />Answer
		Description:<br />General Purpose SSD (gp2) volumes offer cost&#8211;effective storage that is ideal for a broad
		range of workloads. These volumes deliver single&#8211;digit millisecond latencies and the ability to burst to
		3,000 IOPS for extended periods of time.<br /><br />Between a minimum of 100 IOPS (at 33.33 GiB and below) and a
		maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of
		volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can
		range in size from 1 GiB to 16 TiB.<br /><br />In this case the volume would have a baseline performance of 3 x
		200 = 600 IOPS. The volume could also burst to 3,000 IOPS for extended periods. As the I/O varies, this should
		be suitable. CORRECT: &quot;Amazon EBS General Purpose SSD (gp2)&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Amazon EBS Provisioned IOPS SSD (io1) &quot; is incorrect as this would be a
		more expensive option and is not required for the performance characteristics of this
		workload.<br /><br />INCORRECT: &quot;Amazon EBS Cold HDD (sc1)&quot; is incorrect as there is no IOPS SLA for
		HDD volumes and they would likely not perform well enough for this workload.<br /><br />INCORRECT: &quot;Amazon
		EBS Throughput Optimized HDD (st1)&quot; is incorrect as there is no IOPS SLA for HDD volumes and they would
		likely not perform well enough for this workload.<br /><br />References:<br />Amazon Elastic Compute Cloud >
		User Guide for Linux Instances > Amazon EBS volume types<br /></div><a href="#All">All(200)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 199<br />A company is using Amazon <a
			href="#EC2">EC2</a> to run its big data analytics workloads. These variable workloads run each night, and it
		is critical they finish by the start of business the following day. A solutions architect has been tasked with
		designing the MOST cost&#8211;effective solution.<br /><br />Which solution will accomplish this?<br /><br />A.
		Spot Fleet<br />B. Spot Instances<br />C. Reserved Instances<br />D. On&#8211;Demand
		Instances<br /><br /><b>Correct Answer:</b><br />A. Spot Fleet<br /></div><a href="#All">All(200)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a><a id=S3>
		<h2>S3</h2>
	</a> - 44 Questions <br><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 9<br />Organizers for a global event want to put
		daily reports online as static HTML pages. The pages are expected to generate millions of views from users
		around the world. The files are stored in an Amazon <a href="#S3">S3</a> bucket. A solutions architect has been
		asked to design an efficient and effective solution.<br /><br />Which action should the solutions architect take
		to accomplish this?<br /><br />A. Generate presigned URLs for the files.<br />B. Use cross&#8211;Region
		replication to all Regions.<br />C. Use the geoproximity feature of Amazon Route 53.<br />D. Use Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as its origin.<br /><br /><b>Correct
			Answer:</b><br />D. Use Amazon <a href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as
		its origin.<br /><br />Answer Description:<br />Using Amazon <a href="#S3">S3</a> Origins, MediaPackage
		Channels, and Custom Origins for Web Distributions<br /><br />Using Amazon <a href="#S3">S3</a> Buckets for Your
		Origin<br />When you use Amazon <a href="#S3">S3</a> as an origin for your distribution, you place any objects
		that you want <a href="#CloudFront">CloudFront</a> to deliver in an Amazon <a href="#S3">S3</a> bucket. You can
		use any method that is supported by Amazon <a href="#S3">S3</a> to get your objects into Amazon <a
			href="#S3">S3</a>, for example, the Amazon <a href="#S3">S3</a> console or API, or a third&#8211;party tool.
		You can create a hierarchy in your bucket to store the objects, just as you would with any other Amazon <a
			href="#S3">S3</a> bucket.<br /><br />Using an existing Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change the bucket in any way; you can still use
		it as you normally would to store and access Amazon <a href="#S3">S3</a> objects at the standard Amazon <a
			href="#S3">S3</a> price. You incur regular Amazon <a href="#S3">S3</a> charges for storing the objects in
		the bucket.<br /><br />Using Amazon <a href="#S3">S3</a> Buckets Configured as Website Endpoints for Your
		Origin<br />You can set up an Amazon <a href="#S3">S3</a> bucket that is configured as a website endpoint as
		custom origin with <a href="#CloudFront">CloudFront</a>.<br /><br />When you configure your <a
			href="#CloudFront">CloudFront</a> distribution, for the origin, enter the Amazon <a href="#S3">S3</a> static
		website hosting endpoint for your bucket. This value appears in the Amazon <a href="#S3">S3</a> console, on the
		Properties tab, in the Static website hosting pane. For example:
		http://bucket&#8211;name.s3&#8211;website&#8211;region.amazonaws.com<br /><br />For more information about
		specifying Amazon <a href="#S3">S3</a> static website endpoints, see Website endpoints in the Amazon Simple
		Storage Service Developer Guide.<br /><br />When you specify the bucket name in this format as your origin, you
		can use Amazon <a href="#S3">S3</a> redirects and Amazon <a href="#S3">S3</a> custom error documents. For more
		information about Amazon <a href="#S3">S3</a> features, see the Amazon <a href="#S3">S3</a>
		documentation.<br /><br />Using an Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change it in any way. You can still use it as
		you normally would and you incur regular Amazon <a href="#S3">S3</a> charges.<br /><br />Amazon <a
			href="#CloudFront">CloudFront</a> can be used to cache the files in edge locations around the world and this
		will improve the performance of the webpages.<br /><br />To serve a static website hosted on Amazon <a
			href="#S3">S3</a>, you can deploy a <a href="#CloudFront">CloudFront</a> distribution using one of these
		configurations:<br /><br />Using a REST API endpoint as the origin with access restricted by an origin access
		identity (OAI) Using a website endpoint as the origin with anonymous (public) access allowed<br /><br />Using a
		website endpoint as the origin with access restricted by a Referer header CORRECT: &quot;Use Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as its origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Generate presigned URLs for the files&quot; is incorrect as this is used to
		restrict access which is not a requirement.<br /><br />INCORRECT: &quot;Use cross&#8211;Region replication to
		all Regions&quot; is incorrect as this does not provide a mechanism for directing users to the closest copy of
		the static webpages.<br /><br />INCORRECT: &quot;Use the geoproximity feature of Amazon Route 53&quot; is
		incorrect as this does not include a solution for having multiple copies of the data in different geographic
		locations.<br /><br />References:<br /><br />How do I use <a href="#CloudFront">CloudFront</a> to serve a static
		website hosted on Amazon <a href="#S3">S3</a>?</div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 11<br />A solutions architect is designing a
		solution to access a catalog of images and provide users with the ability to submit requests to customize
		images. Image customization parameters will be in any request sent to an AWS API Gateway API. The customized
		image will be generated on demand, and users will receive a link they can click to view or download their
		customized image. The solution must be highly available for viewing and customizing images.<br /><br />What is
		the MOST cost&#8211;effective solution to meet these requirements?<br /><br />A. Use Amazon <a
			href="#EC2">EC2</a> instances to manipulate the original image into the requested customization. Store the
		original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an Elastic Load Balancer in front of
		the <a href="#EC2">EC2</a> instances.<br />B. Use AWS Lambda to manipulate the original image to the requested
		customization. Store the original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the origin.<br />C.
		Use AWS Lambda to manipulate the original image to the requested customization. Store the original images in
		Amazon <a href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in
		front of the Amazon <a href="#EC2">EC2</a> instances.<br />D. Use Amazon <a href="#EC2">EC2</a> instances to
		manipulate the original image into the requested customization. Store the original images in Amazon <a
			href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the
		origin.<br /><br /><b>Correct Answer:</b><br />B. Use AWS Lambda to manipulate the original image to the
		requested customization. Store the original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an
		Amazon <a href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the
		origin.<br /><br />Answer Description:<br />AWS Lambda is a compute service that lets you run code without
		provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from
		a few requests per day to thousands per second. You pay only for the compute time you consume &#8212; there is
		no charge when your code is not running. With AWS Lambda, you can run code for virtually any type of application
		or backend service &#8212; all with zero administration. AWS Lambda runs your code on a high&#8211;availability
		compute infrastructure and performs all of the administration of the compute resources, including server and
		operating system maintenance, capacity provisioning and automatic scaling, code monitoring, and
		logging.<br /><br /><a href="#All">All</a> you need to do is supply your code in one of the languages that AWS
		Lambda supports.<br /><br />Storing your static content with <a href="#S3">S3</a> provides a lot of advantages.
		But to help optimize your application&aposs performance and security while effectively managing cost, we
		recommend that you also set up Amazon <a href="#CloudFront">CloudFront</a> to work with your <a
			href="#S3">S3</a> bucket to serve and protect the content. <a href="#CloudFront">CloudFront</a> is a content
		delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the
		world, securely and at scale. By design, delivering data out of <a href="#CloudFront">CloudFront</a> can be more
		cost effective than delivering it from <a href="#S3">S3</a> directly to your users.<br /><br /><a
			href="#CloudFront">CloudFront</a> serves content through a worldwide network of data centers called Edge
		Locations. Using edge servers to cache and serve content improves performance by providing content closer to
		where viewers are located. <a href="#CloudFront">CloudFront</a> has edge servers in locations all around the
		world.<br /><br /><a href="#All">All</a> solutions presented are highly available. The key requirement that must
		be satisfied is that the solution should be cost&#8211;effective and you must choose the most
		cost&#8211;effective option.<br /><br />Therefore, it&aposs best to eliminate services such as Amazon <a
			href="#EC2">EC2</a> and ELB as these require ongoing costs even when they&aposre not used. Instead, a fully
		serverless solution should be used. AWS Lambda, Amazon <a href="#S3">S3</a> and <a
			href="#CloudFront">CloudFront</a> are the best services to use for these requirements.<br /><br />CORRECT:
		&quot;Use AWS Lambda to manipulate the original images to the requested customization. Store the original and
		manipulated images in Amazon <a href="#S3">S3</a>. Configure an Amazon <a href="#CloudFront">CloudFront</a>
		distribution with the <a href="#S3">S3</a> bucket as the origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Use Amazon <a href="#EC2">EC2</a> instances to manipulate the original
		images into the requested customization. Store the original and manipulated images in Amazon <a
			href="#S3">S3</a>. Configure an Elastic Load Balancer in front of the <a href="#EC2">EC2</a> instances&quot;
		is incorrect. This is not the most cost&#8211;effective option as the ELB and <a href="#EC2">EC2</a> instances
		will incur costs even when not used.<br /><br />INCORRECT: &quot;Use AWS Lambda to manipulate the original
		images to the requested customization. Store the original images in Amazon <a href="#S3">S3</a> and the
		manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in front of the Amazon <a
			href="#EC2">EC2</a> instances&quot; is incorrect. This is not the most cost&#8211;effective option as the
		ELB will incur costs even when not used. Also, Amazon DynamoDB will incur RCU/WCUs when running and is not the
		best choice for storing images.<br /><br />INCORRECT: &quot;Use Amazon <a href="#EC2">EC2</a> instances to
		manipulate the original images into the requested customization. Store the original images in Amazon <a
			href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the origin&quot; is
		incorrect. This is not the most cost&#8211;effective option as the <a href="#EC2">EC2</a> instances will incur
		costs even when not used.<br /><br />References:<br /><br />Serverless on AWS</div><a href="#S3">S3(44)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 12<br />A company is planning to migrate a
		business&#8211;critical dataset to Amazon <a href="#S3">S3</a>. The current solution design uses a single <a
			href="#S3">S3</a> bucket in the us&#8211;east&#8211;1 Region with versioning enabled to store the dataset.
		The company&aposs disaster recovery policy states that all data multiple AWS Regions.<br /><br />How should a
		solutions architect design the <a href="#S3">S3</a> solution?<br /><br />A. Create an additional <a
			href="#S3">S3</a> bucket in another Region and configure cross&#8211;Region replication.<br />B. Create an
		additional <a href="#S3">S3</a> bucket in another Region and configure cross&#8211;origin resource sharing
		(CORS).<br />C. Create an additional <a href="#S3">S3</a> bucket with versioning in another Region and configure
		cross&#8211;Region replication.<br />D. Create an additional <a href="#S3">S3</a> bucket with versioning in
		another Region and configure cross&#8211;origin resource (CORS).<br /><br /><b>Correct Answer:</b><br />C.
		Create an additional <a href="#S3">S3</a> bucket with versioning in another Region and configure
		cross&#8211;Region replication.<br /><br />Answer Description:<br />Replication enables automatic, asynchronous
		copying of objects across Amazon <a href="#S3">S3</a> buckets. Buckets that are configured for object
		replication can be owned by the same AWS account or by different accounts. You can copy objects between
		different AWS Regions or within the same Region. Both source and destination buckets must have versioning
		enabled.<br /><br />CORRECT: &quot;Create an additional <a href="#S3">S3</a> bucket with versioning in another
		Region and configure cross&#8211;Region replication&quot; is the correct answer.<br /><br />INCORRECT:
		&quot;Create an additional <a href="#S3">S3</a> bucket in another Region and configure cross&#8211;Region
		replication&quot; is incorrect as the destination bucket must also have versioning
		enabled.<br /><br />INCORRECT: &quot;Create an additional <a href="#S3">S3</a> bucket in another Region and
		configure cross&#8211;origin resource sharing (CORS)&quot; is incorrect as CORS is not related to
		replication.<br /><br />INCORRECT: &quot;Create an additional <a href="#S3">S3</a> bucket with versioning in
		another Region and configure cross&#8211;origin resource sharing (CORS)&quot; is incorrect as CORS is not
		related to replication.<br /><br />References:<br />Amazon Simple Storage Service > User Guide > Replicating
		objects<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 13<br />A company has application running on
		Amazon <a href="#EC2">EC2</a> instances in a <a href="#VPC">VPC</a>. One of the applications needs to call an
		Amazon <a href="#S3">S3</a> API to store and read objects. The company&aposs security policies restrict any
		internet&#8211;bound traffic from the applications.<br /><br />Which action will fulfill these requirements and
		maintain security?<br /><br />A. Configure an <a href="#S3">S3</a> interface endpoint.<br />B. Configure an <a
			href="#S3">S3</a> gateway endpoint.<br />C. Create an <a href="#S3">S3</a> bucket in a private
		subnet.<br />D. Create an <a href="#S3">S3</a> bucket in the same Region as the <a href="#EC2">EC2</a>
		instance.<br /><br /><b>Correct Answer:</b><br />B. Configure an <a href="#S3">S3</a> gateway
		endpoint.<br /><br />Answer Description:<br /><a href="#VPC">VPC</a> endpoints: A <a href="#VPC">VPC</a>
		endpoint enables you to privately connect your <a href="#VPC">VPC</a> to supported AWS services and <a
			href="#VPC">VPC</a> endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT
		device, VPN connection, or AWS Direct Connect connection. Instances in your <a href="#VPC">VPC</a> do not
		require public IP addresses to communicate with resources in the service. Traffic between your <a
			href="#VPC">VPC</a> and the other service does not leave the Amazon network.<br /><br />An interface
		endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that
		serves as an entry point for traffic destined to a supported service. Interface endpoints are powered by AWS
		PrivateLink, a technology that enables you to privately access services by using private IP addresses. AWS
		PrivateLink restricts all network traffic between your <a href="#VPC">VPC</a> and services to the Amazon
		network. You do not need an internet gateway, a NAT device, or a virtual private
		gateway.<br /><br />References:<br /><br />Amazon Virtual Private Cloud > AWS PrivateLink > Endpoints for Amazon
		<a href="#S3">S3</a><br />Amazon Virtual Private Cloud > AWS PrivateLink > Gateway <a href="#VPC">VPC</a>
		endpoints</div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 25<br />A company hosts a static website within an
		Amazon <a href="#S3">S3</a> bucket. A solutions architect needs to ensure that data can be recovered in case of
		accidental deletion.<br /><br />Which action will accomplish this?<br /><br />A. Enable Amazon <a
			href="#S3">S3</a> versioning.<br />B. Enable Amazon <a href="#S3">S3</a> Intelligent&#8211;Tiering.<br />C.
		Enable an Amazon <a href="#S3">S3</a> lifecycle policy.<br />D. Enable Amazon <a href="#S3">S3</a>
		cross&#8211;Region replication.<br /><br /><b>Correct Answer:</b><br />A. Enable Amazon <a href="#S3">S3</a>
		versioning.<br /><br />Answer Description:<br />Data can be recover if versioning enable, also it provide a
		extra protection like file delete, MFA delete. MFA. Delete only works for CLI or API interaction, not in the AWS
		Management Console. Also, you cannot make version DELETE actions with MFA using <a href="#IAM">IAM</a> user
		credentials. You must use your root AWS account.<br /><br />Object Versioning: Use Amazon <a href="#S3">S3</a>
		Versioning to keep multiple versions of an object in one bucket. For example, you could store my&#8211;image.jpg
		(version 111111) and my&#8211;image.jpg (version 222222) in a single bucket. <a href="#S3">S3</a> Versioning
		protects you from the consequences of unintended overwrites and deletions. You can also use it to archive
		objects so that you have access to previous versions.<br /><br />You must explicitly enable <a href="#S3">S3</a>
		Versioning on your bucket. By default, <a href="#S3">S3</a> Versioning is disabled. Regardless of whether you
		have enabled Versioning, each object in your bucket has a version ID. If you have not enabled Versioning, Amazon
		<a href="#S3">S3</a> sets the value of the version ID to null. If <a href="#S3">S3</a> Versioning is enabled,
		Amazon <a href="#S3">S3</a> assigns a version ID value for the object. This value distinguishes it from other
		versions of the same key.<br /><br />Object versioning is a means of keeping multiple variants of an object in
		the same Amazon <a href="#S3">S3</a> bucket. Versioning provides the ability to recover from both unintended
		user actions and application failures. You can use versioning to preserve, retrieve, and restore every version
		of every object stored in your Amazon <a href="#S3">S3</a> bucket.<br /><br />CORRECT: &quot;Enable Amazon <a
			href="#S3">S3</a> versioning&quot; is the correct answer.<br /><br />INCORRECT: &quot;Enable Amazon <a
			href="#S3">S3</a> Intelligent&#8211;Tiering&quot; is incorrect. This is a storage class that automatically
		moves data between frequent access and infrequent access classes based on usage patterns.<br /><br />INCORRECT:
		&quot;Enable an Amazon <a href="#S3">S3</a> lifecycle policy&quot; is incorrect. An <a href="#S3">S3</a>
		lifecycle policy is a set of rules that define actions that apply to groups of <a href="#S3">S3</a> objects such
		as transitioning objects to another storage class.<br /><br />INCORRECT: &quot;Enable Amazon <a
			href="#S3">S3</a> cross&#8211;Region replication&quot; is incorrect as this is used to copy objects to
		different regions. CRR relies on versioning which is the feature that is required for protecting against
		accidental deletion.<br /><br />References:<br /><br />Protecting Amazon <a href="#S3">S3</a> Against Object
		Deletion</div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 27<br />A company runs an application in a branch
		office within a small data closet with no virtualized compute resources. The application data is stored on an
		NFS volume. Compliance standards require a daily offsite backup of the NFS volume.<br /><br />Which solution
		meet these requirements?<br /><br />A. Install an AWS Storage Gateway file gateway on premises to replicate the
		data to Amazon <a href="#S3">S3</a>.<br />B. Install an AWS Storage Gateway file gateway hardware appliance on
		premises to replicate the data to Amazon <a href="#S3">S3</a>.<br />C. Install an AWS Storage Gateway volume
		gateway with stored volumes on premises to replicate the data to Amazon <a href="#S3">S3</a>.<br />D. Install an
		AWS Storage Gateway volume gateway with cached volumes on premises to replicate the data to Amazon <a
			href="#S3">S3</a>.<br /><br /><b>Correct Answer:</b><br />B. Install an AWS Storage Gateway file gateway
		hardware appliance on premises to replicate the data to Amazon <a href="#S3">S3</a>.<br /><br />Answer
		Description:<br />AWS Storage Gateway Hardware Appliance<br />Hardware Appliance: Storage Gateway is available
		as a hardware appliance, adding to the existing support for VMware ESXi, Microsoft Hyper&#8211;V, and Amazon <a
			href="#EC2">EC2</a>. This means that you can now make use of Storage Gateway in situations where you do not
		have a virtualized environment, server&#8211;class hardware or IT staff with the specialized skills that are
		needed to manage them. You can order appliances from Amazon.com for delivery to branch offices, warehouses, and
		&quot;outpost&quot; offices that lack dedicated IT resources. Setup (as you will see in a minute) is quick and
		easy, and gives you access to three storage solutions:<br /><br />File Gateway: A file interface to Amazon <a
			href="#S3">S3</a>, accessible via NFS or SMB. The files are stored as <a href="#S3">S3</a> objects, allowing
		you to make use of specialized <a href="#S3">S3</a> features such as lifecycle management and cross region
		replication. You can trigger AWS Lambda functions, run Amazon Athena queries, and use Amazon Macie to discover
		and classify sensitive data.<br /><br />Keyword: NFS + Compliance<br /><br />File gateway provides a virtual
		on&#8211;premises file server, which enables you to store and retrieve files as objects in Amazon <a
			href="#S3">S3</a>. It can be used for on&#8211;premises applications, and for Amazon <a
			href="#EC2">EC2</a>&#8211; resident applications that need file storage in <a href="#S3">S3</a> for object
		based workloads. Used for flat files only, stored directly on <a href="#S3">S3</a>. File gateway offers SMB or
		NFS&#8211;based access to data in Amazon <a href="#S3">S3</a> with local caching.<br /><br />WS Storage Gateway
		&#8212; File Gateway<br /><br />The table below shows the different gateways available and the interfaces and
		use cases:<br /><br />Storage Gateway Overview<br /><br />CORRECT: &quot;Install an AWS Storage Gateway file
		gateway hardware appliance on premises to replicate the data to Amazon <a href="#S3">S3</a>&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Install an AWS Storage Gateway file gateway on premises to replicate the
		data to Amazon <a href="#S3">S3</a>&quot; is incorrect.<br /><br />INCORRECT: &quot;Install an AWS Storage
		Gateway volume gateway with stored volumes on premises to replicate the data to Amazon <a
			href="#S3">S3</a>&quot; is incorrect as unsupported NFS. INCORRECT: &quot;Install an AWS Storage Gateway
		volume gateway with cached volumes on premises to replicate the data to Amazon <a href="#S3">S3</a>&quot; is
		incorrect as unsupported NFS.<br /><br />References:<br /><br />AWS News Blog > File Interface to AWS Storage
		Gateway</div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 29<br />A data science team requires storage for
		nightly log processing. The size and number of logs is unknown and will persist for 24 hours
		only.<br /><br />What is the MOST cost&#8211;effective solution?<br /><br />A. Amazon <a href="#S3">S3</a>
		Glacier<br />B. Amazon <a href="#S3">S3</a> Standard<br />C. Amazon <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br />D. Amazon <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a
			href="#S3">S3</a> One Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />B. Amazon <a href="#S3">S3</a>
		Standard<br /><br />Answer Description:<br />The <a href="#S3">S3</a> Intelligent&#8211;Tiering storage class is
		designed to optimize costs by automatically moving data to the most cost&#8211;effective access tier, without
		performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is
		optimized for frequent access and another lower&#8211;cost tier that is optimized for infrequent access. This is
		an ideal use case for intelligent&#8211;tiering as the access patterns for the log files are not
		known.<br /><br />CORRECT: &quot;<a href="#S3">S3</a> Intelligent&#8211;Tiering&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;<a href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a>
		Standard&#8211;IA)&quot; is incorrect as if the data is accessed often retrieval fees could become
		expensive.<br /><br />INCORRECT: &quot;<a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a
			href="#S3">S3</a> One Zone&#8211;IA)&quot; is incorrect as if the data is accessed often retrieval fees
		could become expensive.<br /><br />INCORRECT: &quot;<a href="#S3">S3</a> Glacier&quot; is incorrect as if the
		data is accessed often retrieval fees could become expensive. Glacier also requires more work in retrieving the
		data from the archive and quick access requirements can add further
		costs.<br /><br />References:<br /><br />Unknown or changing access</div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 32<br />A solutions architect is tasked with
		transferring 750 TB of data from a network&#8211;attached file system located at a branch office Amazon <a
			href="#S3">S3</a> Glacier. The solution must avoid saturating the branch office&aposs low&#8211;bandwidth
		internet connection.<br /><br />What is the MOST cost&#8211;effective solution?<br /><br />A. Create a
		site&#8211;to&#8211;site VPN tunnel to an Amazon <a href="#S3">S3</a> bucket and transfer the files directly.
		Create a bucket policy to enforce a <a href="#VPC">VPC</a> endpoint.<br />B. Order 10 AWS Snowball appliances
		and select an <a href="#S3">S3</a> Glacier vault as the destination. Create a bucket policy to enforce a <a
			href="#VPC">VPC</a> endpoint.<br />C. Mount the network&#8211;attached file system to Amazon <a
			href="#S3">S3</a> and copy the files directly. Create a lifecycle policy to transition the <a
			href="#S3">S3</a> objects to Amazon <a href="#S3">S3</a> Glacier.<br />D. Order 10 AWS Snowball appliances
		and select an Amazon <a href="#S3">S3</a> bucket as the destination. Create a lifecycle policy to transition the
		<a href="#S3">S3</a> objects to Amazon <a href="#S3">S3</a> Glacier.<br /><br /><b>Correct Answer:</b><br />D.
		Order 10 AWS Snowball appliances and select an Amazon <a href="#S3">S3</a> bucket as the destination. Create a
		lifecycle policy to transition the <a href="#S3">S3</a> objects to Amazon <a href="#S3">S3</a>
		Glacier.<br /><br />Answer Description:<br />Regional Limitations for AWS Snowball<br />The AWS Snowball service
		has two device types, the standard Snowball and the Snowball Edge. The following table highlights which of these
		devices are available in which regions.<br /><br />The following table highlights which of these devices are
		available in which regions.<br /><br />The following table highlights which of these devices are available in
		which regions.<br /><br />Limitations on Jobs in AWS Snowball<br /><br />The following limitations exist for
		creating jobs in AWS Snowball:<br /><br />For security purposes, data transfers must be completed within 90 days
		of the Snowball being prepared.<br /><br />Currently, AWS Snowball Edge device doesn&apost support
		server&#8211;side encryption with customer&#8211;provided keys (SSE&#8211;C). AWS Snowball Edge device does
		support server&#8211;side encryption with Amazon <a href="#S3">S3</a>&#8212;managed encryption keys
		(SSE&#8211;<a href="#S3">S3</a>) and server&#8211;side encryption with AWS Key Management Service &#8212;
		managed keys (SSE&#8211;KMS). For more information, see Protecting Data Using Server&#8211;Side Encryption in
		the Amazon Simple Storage Service Developer Guide.<br /><br />In the US regions, Snowballs come in two sizes: 50
		TB and 80 TB. <a href="#All">All</a> other regions have the 80 TB Snowballs only. If you&aposre using Snowball
		to import data, and you need to transfer more data than will fit on a single Snowball, create additional jobs.
		Each export job can use multiple Snowballs.<br /><br />The default service limit for the number of Snowballs you
		can have at one time is 1. If you want to increase your service limit, contact AWS Support.<br /><br /><a
			href="#All">All</a> objects transferred to the Snowball have their metadata changed. The only metadata that
		remains the same is filename and filesize. <a href="#All">All</a> other metadata is set as in the following
		example: &#8211;rw&#8211;rw&#8211;r&#8212; 1 root root [filesize] Dec 31 1969 [path/filename].<br /><br />Object
		lifecycle management<br />To manage your objects so that they are stored cost effectively throughout their
		lifecycle, configure their Amazon <a href="#S3">S3</a> Lifecycle. An <a href="#S3">S3</a> Lifecycle
		configuration is a set of rules that define actions that Amazon <a href="#S3">S3</a> applies to a group of
		objects. There are two types of actions:<br /><br />Transition actions &#8212; Define when objects transition to
		another storage class. For example, you might choose to transition objects to the <a href="#S3">S3</a>
		Standard&#8211;IA storage class 30 days after you created them, or archive objects to the <a href="#S3">S3</a>
		Glacier storage class one year after creating them.<br /><br />Expiration actions &#8212; Define when objects
		expire. Amazon <a href="#S3">S3</a> deletes expired objects on your behalf. The lifecycle expiration costs
		depend on when you choose to expire objects.<br /><br />As the company&aposs internet link is
		low&#8211;bandwidth uploading directly to Amazon <a href="#S3">S3</a> (ready for transition to Glacier) would
		saturate the link. The best alternative is to use AWS Snowball appliances. The Snowball Edge appliance can hold
		up to 75 TB of data so 10 devices would be required to migrate 750 TB of data.<br /><br />Snowball moves data
		into AWS using a hardware device and the data is then copied into an Amazon <a href="#S3">S3</a> bucket of your
		choice. From there, lifecycle policies can transition the <a href="#S3">S3</a> objects to Amazon <a
			href="#S3">S3</a> Glacier.<br /><br />CORRECT: &quot;Order 10 AWS Snowball appliances and select an Amazon
		<a href="#S3">S3</a> bucket as the destination. Create a lifecycle policy to transition the <a href="#S3">S3</a>
		objects to Amazon <a href="#S3">S3</a> Glacier&quot; is the correct answer.<br /><br />INCORRECT: &quot;Order 10
		AWS Snowball appliances and select an <a href="#S3">S3</a> Glacier vault as the destination. Create a bucket
		policy to enforce a <a href="#VPC">VPC</a> endpoint&quot; is incorrect as you cannot set a Glacier vault as the
		destination, it must be an <a href="#S3">S3</a> bucket. You also can&apost enforce a <a href="#VPC">VPC</a>
		endpoint using a bucket policy.<br /><br />INCORRECT: &quot;Create an AWS Direct Connect connection and migrate
		the data straight into Amazon Glacier&quot; is incorrect as this is not the most cost&#8211;effective option and
		takes time to setup. INCORRECT: &quot;Use AWS Global Accelerator to accelerate upload and optimize usage of the
		available bandwidth&quot; is incorrect as this service is not used for accelerating or optimizing the upload of
		data from on&#8211;premises networks.<br /><br />References:<br /><br />AWS Snowball Edge Developer Guide > AWS
		Snowball Edge Specifications</div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 36<br />A company runs an application on a group
		of Amazon Linux <a href="#EC2">EC2</a> instances. The application writes log files using standard API calls. For
		compliance reasons, all log files must be retained indefinitely and will be analyzed by a reporting tool that
		must access all files concurrently.<br /><br />Which storage service should a solutions architect use to provide
		the MOST cost&#8211;effective solution?<br /><br />A. Amazon EBS<br />B. Amazon EFS<br />C. Amazon <a
			href="#EC2">EC2</a> instance store<br />D. Amazon <a href="#S3">S3</a><br /><br /><b>Correct
			Answer:</b><br />D. Amazon <a href="#S3">S3</a><br /><br />Answer Description:<br />Amazon <a
			href="#S3">S3</a>: Requests to Amazon <a href="#S3">S3</a> can be authenticated or anonymous. Authenticated
		access requires credentials that AWS can use to authenticate your requests. When making REST API calls directly
		from your code, you create a signature using valid credentials and include the signature in your request. Amazon
		Simple Storage Service (Amazon <a href="#S3">S3</a>) is an object storage service that offers
		industry&#8211;leading scalability, data availability, security, and performance. This means customers of all
		sizes and industries can use it to store and protect any amount of data for a range of use cases, such as
		websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data
		analytics. Amazon <a href="#S3">S3</a> provides easy&#8211;to&#8211;use management features so you can organize
		your data and configure finely&#8211;tuned access controls to meet your specific business, organizational, and
		compliance requirements. Amazon <a href="#S3">S3</a> is designed for 99.999999999% (11 9&aposs) of durability,
		and stores data for millions of applications for companies all around the world.<br /><br />The application is
		writing the files using API calls which means it will be compatible with Amazon <a href="#S3">S3</a> which uses
		a REST API. <a href="#S3">S3</a> is a massively scalable key&#8211;based object store that is well&#8211;suited
		to allowing concurrent access to the files from many instances.<br /><br />Amazon <a href="#S3">S3</a> will also
		be the most cost&#8211;effective choice. A rough calculation using the AWS pricing calculator shows the cost
		differences between 1TB of storage on EBS, EFS, and <a href="#S3">S3</a> Standard.<br /><br />CORRECT:
		&quot;Amazon <a href="#S3">S3</a>&quot; is the correct answer.<br /><br />INCORRECT: &quot;Amazon EFS&quot; is
		incorrect as though this does offer concurrent access from many <a href="#EC2">EC2</a> Linux instances, it is
		not the most cost&#8211;effective solution.<br /><br />INCORRECT: &quot;Amazon EBS&quot; is incorrect. The
		Elastic Block Store (EBS) is not a good solution for concurrent access from many <a href="#EC2">EC2</a>
		instances and is not the most cost&#8211;effective option either. EBS volumes are mounted to a single instance
		except when using multi&#8211;attach which is a new feature and has several constraints.<br /><br />INCORRECT:
		&quot;Amazon <a href="#EC2">EC2</a> instance store&quot; is incorrect as this is an ephemeral storage solution
		which means the data is lost when powered down.<br /><br />Therefore, this is not an option for long&#8211;term
		data storage.<br /><br />References:<br /><br />Amazon Simple Storage Service > User Guide > Best practices
		design patterns: optimizing Amazon <a href="#S3">S3</a> performance</div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 42<br />A company built a food ordering
		application that captures user data and stores it for future analysis. The application&aposs static front end is
		deployed on an Amazon <a href="#EC2">EC2</a> instance. The front&#8211;end application sends the requests to the
		backend application running on separate <a href="#EC2">EC2</a> instance. The backend application then stores the
		data in Amazon RDS.<br /><br />What should a solutions architect do to decouple the architecture and make it
		scalable?<br /><br />A. Use Amazon <a href="#S3">S3</a> to serve the front&#8211;end application, which sends
		requests to Amazon <a href="#EC2">EC2</a> to execute the backend application. The backend application will
		process and store the data in Amazon RDS.<br />B. Use Amazon <a href="#S3">S3</a> to serve the front&#8211;end
		application and write requests to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon <a
			href="#EC2">EC2</a> instances to the HTTP/HTTPS endpoint of the topic, and process and store the data in
		Amazon RDS.<br />C. Use an <a href="#EC2">EC2</a> instance to serve the front end and write requests to an
		Amazon SQS queue. Place the backend instance in an Auto Scaling group, and scale based on the queue depth to
		process and store the data in Amazon RDS.<br />D. Use Amazon <a href="#S3">S3</a> to serve the static
		front&#8211;end application and send requests to Amazon API Gateway, which writes the requests to an Amazon SQS
		queue. Place the backend instances in an Auto Scaling group, and scale based on the queue depth to process and
		store the data in Amazon RDS.<br /><br /><b>Correct Answer:</b><br />D. Use Amazon <a href="#S3">S3</a> to serve
		the static front&#8211;end application and send requests to Amazon API Gateway, which writes the requests to an
		Amazon SQS queue. Place the backend instances in an Auto Scaling group, and scale based on the queue depth to
		process and store the data in Amazon RDS.<br /><br />Answer Description:<br />Keyword: Static + Decouple +
		Scalable Static=<a href="#S3">S3</a><br /><br />Decouple=SQS Queue Scalable=ASG<br /><br />Option B will not be
		there in the race due to Auto&#8211;Scaling unavailability. Option A will not be there in the race due to
		Decouple unavailability.<br /><br />Option C & D will be in the race and Option D will be correct answers due to
		all 3 combination matches [Static=<a href="#S3">S3</a>; Decouple=SQS Queue; Scalable=ASG] & Option C will loose
		due to Static option unavailability<br /></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 44<br />A company is managing health records
		on&#8211;premises. The company must keep these records indefinitely, disable any modifications to the records
		once they are stored, and granularly audit access at all levels. The chief technology officer (CTO) is concerned
		because there are already millions of records not being used by any application, and the current infrastructure
		is running out of space. The CTO has requested a solutions architect design a solution to move existing data and
		support future records.<br /><br />Which services can the solutions architect recommend to meet these
		requirements?<br /><br />A. Use AWS DataSync to move existing data to AWS. Use Amazon <a href="#S3">S3</a> to
		store existing and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable AWS CloudTrail with data
		events.<br />B. Use AWS Storage Gateway to move existing data to AWS. Use Amazon <a href="#S3">S3</a> to store
		existing and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable AWS CloudTrail with management
		events.<br />C. Use AWS DataSync to move existing data to AWS. Use Amazon <a href="#S3">S3</a> to store existing
		and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable AWS CloudTrail with management
		events.<br />D. Use AWS Storage Gateway to move existing data to AWS. Use Amazon Elastic Block Store (Amazon
		EBS) to store existing and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable Amazon <a
			href="#S3">S3</a> server access logging.<br /><br /><b>Correct Answer:</b><br />D. Use AWS Storage Gateway
		to move existing data to AWS. Use Amazon Elastic Block Store (Amazon EBS) to store existing and new data. Enable
		Amazon <a href="#S3">S3</a> object lock and enable Amazon <a href="#S3">S3</a> server access
		logging.<br /><br />Answer Description:<br />Keyword: Move existing data and support future records + Granular
		audit access at all levels<br /><br />Use AWS DataSync to migrate existing data to Amazon <a href="#S3">S3</a>,
		and then use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for
		ongoing updates from your on&#8211;premises file&#8211;based applications.<br /><br />Need a solution to move
		existing data and support future records = AWS DataSync should be used for migration.<br /><br />Need granular
		audit access at all levels = Data Events should be used in CloudTrail, Management Events is enabled by
		default.<br /><br />CORRECT: &quot;Use AWS DataSync to move existing data to AWS. Use Amazon <a
			href="#S3">S3</a> to store existing and new data. Enable Amazon <a href="#S3">S3</a> object lock and enable
		AWS CloudTrail with data events&quot; is the correct answer.<br /><br />INCORRECT: &quot;Use AWS Storage Gateway
		to move existing data to AWS. Use Amazon <a href="#S3">S3</a> to store existing and new data. Enable Amazon <a
			href="#S3">S3</a> object lock and enable AWS CloudTrail with management events&quot; is incorrect as
		&quot;current infrastructure is running out of space&quot; INCORRECT: &quot;Use AWS DataSync to move existing
		data to AWS. Use Amazon <a href="#S3">S3</a> to store existing and new data. Enable Amazon <a href="#S3">S3</a>
		object lock and enable AWS CloudTrail with management events.&quot; is incorrect as &quot;Management Events is
		enabled by default&quot; INCORRECT: &quot;Use AWS Storage Gateway to move existing data to AWS. Use Amazon
		Elastic Block Store (Amazon EBS) to store existing and new data. Enable Amazon <a href="#S3">S3</a> object lock
		and enable Amazon <a href="#S3">S3</a> server access logging.&quot; is incorrect as &quot;current infrastructure
		is running out of space&quot;<br /><br />References:<br /><br />AWS DataSync<br />AWS CloudTrail<br />AWS
		Storage Gateway</div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 45<br />A company wants to use Amazon <a
			href="#S3">S3</a> for the secondary copy of its on&#8211;premises dataset. The company would rarely need to
		access this copy. The storage solution&aposs cost should be minimal.<br /><br />Which storage solution meets
		these requirements?<br /><br />A. <a href="#S3">S3</a> Standard<br />B. <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br />C. <a href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a>
		Standard&#8211;IA)<br />D. <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />D. <a href="#S3">S3</a> One Zone&#8211;Infrequent Access
		(<a href="#S3">S3</a> One Zone&#8211;IA)<br /></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 61<br />A company has an on&#8211;premises data
		center that is running out of storage capacity. The company wants to migrate its storage infrastructure to AWS
		while minimizing bandwidth costs. The solution must allow for immediate retrieval of data at no additional
		cost.<br /><br />How can these requirements be met?<br /><br />A. Deploy Amazon <a href="#S3">S3</a> Glacier
		Vault and enable expedited retrieval. Enable provisioned retrieval capacity for the workload.<br />B. Deploy AWS
		Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon <a href="#S3">S3</a> while
		retaining copies of frequently accessed data subsets locally.<br />C. Deploy AWS Storage Gateway using stored
		volumes to store data locally. Use Storage Gateway to asynchronously back up point&#8211;in&#8211;time snapshots
		of the data to Amazon <a href="#S3">S3</a>.<br />D. Deploy AWS Direct Connect to connect with the
		on&#8211;premises data center. Configure AWS Storage Gateway to store data locally. Use Storage Gateway to
		asynchronously back up point&#8211;in&#8211;time snapshots of the data to Amazon <a
			href="#S3">S3</a>.<br /><br /><b>Correct Answer:</b><br />C. Deploy AWS Storage Gateway using stored volumes
		to store data locally. Use Storage Gateway to asynchronously back up point&#8211;in&#8211;time snapshots of the
		data to Amazon <a href="#S3">S3</a>.<br /><br />Answer Description:<br />Volume Gateway provides an iSCSI
		target, which enables you to create block storage volumes and mount them as iSCSI devices from your
		on&#8211;premises or <a href="#EC2">EC2</a> application servers. The Volume Gateway runs in either a cached or
		stored mode:<br /><br />In the cached mode, your primary data is written to <a href="#S3">S3</a>, while
		retaining your frequently accessed data locally in a cache for low&#8211;latency access.<br /><br />In the
		stored mode, your primary data is stored locally and your entire dataset is available for low&#8211;latency
		access while asynchronously backed up to AWS.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 65<br />A company&aposs application hosted on
		Amazon <a href="#EC2">EC2</a> instances needs to access an Amazon <a href="#S3">S3</a> bucket. Due to data
		sensitivity, traffic cannot traverse the internet.<br /><br />How should a solutions architect configure
		access?<br /><br />A. Create a private hosted zone using Amazon Route 53.<br />B. Configure a <a
			href="#VPC">VPC</a> gateway endpoint for Amazon <a href="#S3">S3</a> in the <a href="#VPC">VPC</a>.<br />C.
		Configure AWS PrivateLink between the <a href="#EC2">EC2</a> instance and the <a href="#S3">S3</a>
		bucket.<br />D. Set up a site&#8211;to&#8211;site VPN connection between the <a href="#VPC">VPC</a> and the <a
			href="#S3">S3</a> bucket.<br /><br /><b>Correct Answer:</b><br />B. Configure a <a href="#VPC">VPC</a>
		gateway endpoint for Amazon <a href="#S3">S3</a> in the <a href="#VPC">VPC</a>.<br /></div><a
		href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 70<br />A company has enabled AWS CloudTrail logs
		to deliver log files to an Amazon <a href="#S3">S3</a> bucket for each of its developer accounts. The company
		has created a central AWS account for streamlining management and audit reviews. An internal auditor needs to
		access the CloudTrail logs, yet access needs to be restricted for all developer account users. The solution must
		be secure and optimized.<br /><br />How should a solutions architect meet these requirements?<br /><br />A.
		Configure an AWS Lambda function in each developer account to copy the log files to the central account. Create
		an <a href="#IAM">IAM</a> role in the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy
		providing read only permissions to the bucket.<br />B. Configure CloudTrail from each developer account to
		deliver the log files to an <a href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a>
		user in the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing full permissions
		to the bucket.<br />C. Configure CloudTrail from each developer account to deliver the log files to an <a
			href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a> role in the central
		account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing read only permissions to the
		bucket.<br />D. Configure an AWS Lambda function in the central account to copy the log files from the <a
			href="#S3">S3</a> bucket in each developer account. Create an <a href="#IAM">IAM</a> user in the central
		account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing full permissions to the
		bucket.<br /><br /><b>Correct Answer:</b><br />C. Configure CloudTrail from each developer account to deliver
		the log files to an <a href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a> role in
		the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing read only permissions to
		the bucket.<br /><br /> Go to dashboard</div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 73<br />A company runs a website on Amazon <a
			href="#EC2">EC2</a> instances behind an ELB Application Load Balancer. Amazon Route 53 is used for the DNS.
		The company wants to set up a backup website with a message including a phone number and email address that
		users can reach if the primary website is down.<br /><br />How should the company deploy this
		solution?<br /><br />A. Use Amazon <a href="#S3">S3</a> website hosting for the backup website and Route 53
		failover routing policy.<br />B. Use Amazon <a href="#S3">S3</a> website hosting for the backup website and
		Route 53 latency routing policy.<br />C. Deploy the application in another AWS Region and use ELB health checks
		for failover routing.<br />D. Deploy the application in another AWS Region and use server&#8211;side redirection
		on the primary website.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon <a href="#S3">S3</a> website
		hosting for the backup website and Route 53 failover routing policy.<br /></div><a href="#S3">S3(44)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 74<br />A media company is evaluating the
		possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the maximum
		possible I/O performance for video processing, 300 TB of very durable storage for storing media content, and 900
		TB of storage to meet requirements for archival media that is not in use anymore.<br /><br />Which set of
		services should a solutions architect recommend to meet these requirements?<br /><br />A. Amazon EBS for maximum
		performance, Amazon <a href="#S3">S3</a> for durable data storage, and Amazon <a href="#S3">S3</a> Glacier for
		archival storage<br />B. Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon <a
			href="#S3">S3</a> Glacier for archival storage<br />C. Amazon <a href="#EC2">EC2</a> instance store for
		maximum performance, Amazon EFS for durable data storage, and Amazon <a href="#S3">S3</a> for archival
		storage<br />D. Amazon <a href="#EC2">EC2</a> instance store for maximum performance, Amazon <a
			href="#S3">S3</a> for durable data storage, and Amazon <a href="#S3">S3</a> Glacier for archival
		storage<br /><br /><b>Correct Answer:</b><br />A. Amazon EBS for maximum performance, Amazon <a
			href="#S3">S3</a> for durable data storage, and Amazon <a href="#S3">S3</a> Glacier for archival
		storage<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 75<br />An application is running on Amazon <a
			href="#EC2">EC2</a> instances. Sensitive information required for the application is stored in an Amazon <a
			href="#S3">S3</a> bucket. The bucket needs to be protected from internet access while only allowing services
		within the <a href="#VPC">VPC</a> access to the bucket.<br /><br />Which combination of actions should solutions
		archived take to accomplish this? (Choose two.)<br /><br />A. Create a <a href="#VPC">VPC</a> endpoint for
		Amazon <a href="#S3">S3</a>.<br />B. Enable server access logging on the bucket.<br />C. Apply a bucket policy
		to restrict access to the <a href="#S3">S3</a> endpoint.<br />D. Add an <a href="#S3">S3</a> ACL to the bucket
		that has sensitive information.<br />E. Restrict users using the <a href="#IAM">IAM</a> policy to use the
		specific bucket.<br /><br /><b>Correct Answer:</b><br />A. Create a <a href="#VPC">VPC</a> endpoint for Amazon
		<a href="#S3">S3</a>.<br />C. Apply a bucket policy to restrict access to the <a href="#S3">S3</a>
		endpoint.<br /><br />Answer Description:<br />ACL is a property at object level not at bucket level. Also by
		just adding ACL you cant let the services in <a href="#VPC">VPC</a> allow access to the bucket.<br /></div><a
		href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 86<br />A company is running a two&#8211;tier
		eCommerce website using services. The current architect uses a public facing Elastic Load Balancer that sends
		traffic to Amazon <a href="#EC2">EC2</a> instances in a private subnet. The static content is hosted on <a
			href="#EC2">EC2</a> instances, and the dynamic content is retrieved from a MYSQL database. The application
		is running in the United States. The company recently started selling to users in Europe and Australia. A
		solutions architect needs to design solution so their international users have an improved browsing
		experience.<br /><br />Which solution is MOST cost&#8211;effective?<br /><br />A. Host the entire website on
		Amazon <a href="#S3">S3</a>.<br />B. Use Amazon <a href="#CloudFront">CloudFront</a> and Amazon <a
			href="#S3">S3</a> to host static images.<br />C. Increase the number of public load balancers and <a
			href="#EC2">EC2</a> instances.<br />D. Deploy the two&#8211;tier website in AWS Regions in Europe and
		Australia.<br /><br /><b>Correct Answer:</b><br />B. Use Amazon <a href="#CloudFront">CloudFront</a> and Amazon
		<a href="#S3">S3</a> to host static images.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 87<br />A company&aposs website provides users
		with downloadable historical performance reports. The website needs a solution that will scale to meet the
		company&aposs website demands globally. The solution should be cost effective, limit the provisioning of
		infrastructure resources, and provide the fastest possible response time.<br /><br />Which combination should a
		solutions architect recommend to meet these requirements?<br /><br />A. Amazon <a
			href="#CloudFront">CloudFront</a> and Amazon <a href="#S3">S3</a><br />B. AWS Lambda and Amazon
		DynamoDB<br />C. Application Load Balancer with Amazon <a href="#EC2">EC2</a> Auto Scaling<br />D. Amazon Route
		53 with internal Application Load Balancers<br /><br /><b>Correct Answer:</b><br />A. Amazon <a
			href="#CloudFront">CloudFront</a> and Amazon <a href="#S3">S3</a><br /></div><a href="#S3">S3(44)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 92<br />A company requires a durable backup
		storage solution for its on&#8211;premises database servers while ensuring on&#8211;premises applications
		maintain access to these backups for quick recovery. The company will use AWS storage services as the
		destination for these backups. A solutions architect is designing a solution with minimal operational
		overhead.<br /><br />Which solution should the solutions architect implement?<br /><br />A. Deploy an AWS
		Storage Gateway file gateway on&#8211;premises and associate it with an Amazon <a href="#S3">S3</a>
		bucket.<br />B. Back up the databases to an AWS Storage Gateway volume gateway and access it using the Amazon <a
			href="#S3">S3</a> API.<br />C. Transfer the database backup files to an Amazon Elastic Block Store (Amazon
		EBS) volume attached to an Amazon <a href="#EC2">EC2</a> instance.<br />D. Back up the database directly to an
		AWS Snowball device and use lifecycle rules to move the data to Amazon <a href="#S3">S3</a> Glacier Deep
		Archive.<br /><br /><b>Correct Answer:</b><br />A. Deploy an AWS Storage Gateway file gateway on&#8211;premises
		and associate it with an Amazon <a href="#S3">S3</a> bucket.<br /><br />Answer Description:<br />Network Load
		Balancer overview<br /><br />A Network Load Balancer functions at the fourth layer of the Open Systems
		Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a
		connection request, it selects a target from the target group for the default rule. It attempts to open a TCP
		connection to the selected target on the port specified in the listener configuration.<br /><br />When you
		enable an Availability Zone for the load balancer, Elastic Load Balancing creates a load balancer node in the
		Availability Zone. By default, each load balancer node distributes traffic across the registered targets in its
		Availability Zone only. If you enable cross&#8211;zone load balancing, each load balancer node distributes
		traffic across the registered targets in all enabled Availability Zones. For more information, see Availability
		Zones.<br /><br />If you enable multiple Availability Zones for your load balancer and ensure that each target
		group has at least one target in each enabled Availability Zone, this increases the fault tolerance of your
		applications. For example, if one or more target groups does not have a healthy target in an Availability Zone,
		we remove the IP address for the corresponding subnet from DNS, but the load balancer nodes in the other
		Availability Zones are still available to route traffic. If a client doesn&apost honor the
		time&#8211;to&#8211;live (TTL) and sends requests to the IP address after it is removed from DNS, the requests
		fail.<br /><br />For TCP traffic, the load balancer selects a target using a flow hash algorithm based on the
		protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number. The
		TCP connections from a client have different source ports and sequence numbers, and can be routed to different
		targets. Each individual TCP connection is routed to a single target for the life of the
		connection.<br /><br />For UDP traffic, the load balancer selects a target using a flow hash algorithm based on
		the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the
		same source and destination, so it is consistently routed to a single target throughout its lifetime. Different
		UDP flows have different source IP addresses and ports, so they can be routed to different
		targets.<br /><br />An Auto Scaling group contains a collection of Amazon <a href="#EC2">EC2</a> instances that
		are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group
		also enables you to use Amazon <a href="#EC2">EC2</a> Auto Scaling features such as health check replacements
		and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling
		are the core functionality of the Amazon <a href="#EC2">EC2</a> Auto Scaling service.<br /><br />The size of an
		Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its
		size to meet demand, either manually or by using automatic scaling.<br /><br />An Auto Scaling group starts by
		launching enough instances to meet its desired capacity. It maintains this number of instances by performing
		periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed
		number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group
		terminates the unhealthy instance and launches another instance to replace it.<br /></div><a
		href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 101<br />A solutions architect is using Amazon <a
			href="#S3">S3</a> to design the storage architecture of a new digital media application. The media files
		must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are
		rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and
		retrieving the media files.<br /><br />Which storage option meets these requirements?<br /><br />A. <a
			href="#S3">S3</a> Standard<br />B. <a href="#S3">S3</a> Intelligent&#8211;Tiering<br />C. <a
			href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a> Standard&#8211;IA)<br />D. <a
			href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />B. <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br /><br />Answer Description:<br /><a href="#S3">S3</a> Intelligent&#8211;Tiering is
		a new Amazon <a href="#S3">S3</a> storage class designed for customers who want to optimize storage costs
		automatically when data access patterns change, without performance impact or operational overhead. <a
			href="#S3">S3</a> Intelligent&#8211;Tiering is the first cloud object storage class that delivers automatic
		cost savings by moving data between two access tiers &#8212; frequent access and infrequent access &#8212; when
		access patterns change, and is ideal for data with unknown or changing access patterns.<br /><br /><a
			href="#S3">S3</a> Intelligent&#8211;Tiering stores objects in two access tiers: one tier that is optimized
		for frequent access and another lower&#8211;cost tier that is optimized for infrequent access. For a small
		monthly monitoring and automation fee per object, <a href="#S3">S3</a> Intelligent&#8211;Tiering monitors access
		patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access tier.
		There are no retrieval fees in <a href="#S3">S3</a> Intelligent&#8211;Tiering. If an object in the infrequent
		access tier is accessed later, it is automatically moved back to the frequent access tier. No additional tiering
		fees apply when objects are moved between access tiers within the <a href="#S3">S3</a> Intelligent&#8211;Tiering
		storage class. <a href="#S3">S3</a> Intelligent&#8211;Tiering is designed for 99.9% availability and
		99.999999999% durability, and offers the same low latency and high throughput performance of <a
			href="#S3">S3</a> Standard.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 103<br />A solutions architect is designing the
		cloud architecture for a new application being deployed to AWS. The application allows users to interactively
		download and upload files. Files older than 2 years will be accessed less frequently. The solutions architect
		needs to ensure that the application can scale to any number of files while maintaining high availability and
		durability.<br /><br />Which scalable solutions should the solutions architect recommend? (Choose
		two.)<br /><br />A. Store the files on Amazon <a href="#S3">S3</a> with a lifecycle policy that moves objects
		older than 2 years to <a href="#S3">S3</a> Glacier.<br />B. Store the files on Amazon <a href="#S3">S3</a> with
		a lifecycle policy that moves objects older than 2 years to <a href="#S3">S3</a> Standard&#8211;Infrequent
		Access (<a href="#S3">S3</a> Standard&#8211;IA)<br />C. Store the files on Amazon Elastic File System (Amazon
		EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent Access (EFS IA).<br />D.
		Store the files in Amazon Elastic Block Store (Amazon EBS) volumes. Schedule snapshots of the volumes. Use the
		snapshots to archive data older than 2 years.<br />E. Store the files in RAID&#8211;striped Amazon Elastic Block
		Store (Amazon EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2
		years.<br /><br /><b>Correct Answer:</b><br />A. Store the files on Amazon <a href="#S3">S3</a> with a lifecycle
		policy that moves objects older than 2 years to <a href="#S3">S3</a> Glacier.<br />C. Store the files on Amazon
		Elastic File System (Amazon EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent
		Access (EFS IA).<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 106<br />A company is hosting multiple websites
		for several lines of business under its registered parent domain.<br /><br />Users accessing these websites will
		be routed to appropriate backend Amazon <a href="#EC2">EC2</a> instances based on the subdomain. The websites
		host static webpages, images, and server&#8211;side scripts like PHP and JavaScript. Some of the websites
		experience peak access during the first two hours of business with constant usage throughout the rest of the
		day. A solutions architect needs to design a solution that will automatically adjust capacity to these traffic
		patterns while keeping costs low.<br /><br />Which combination of AWS services or features will meet these
		requirements? (Choose two.)<br /><br />A. AWS Batch<br />B. Network Load Balancer<br />C. Application Load
		Balancer<br />D. Amazon <a href="#EC2">EC2</a> Auto Scaling<br />E. Amazon <a href="#S3">S3</a> website
		hosting<br /><br /><b>Correct Answer:</b><br />C. Application Load Balancer<br />D. Amazon <a
			href="#EC2">EC2</a> Auto Scaling<br /><br />References:<br /><br />Amazon Simple Storage Service > User
		Guide > Hosting a static website using Amazon <a href="#S3">S3</a></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 107<br />A company uses an Amazon <a
			href="#S3">S3</a> bucket to store static images for its website. The company configured permissions to allow
		access to Amazon <a href="#S3">S3</a> objects by privileged users only.<br /><br />What should a solutions
		architect do to protect against data loss? (Choose two.)<br /><br />A. Enable versioning on the <a
			href="#S3">S3</a> bucket.<br />B. Enable access logging on the <a href="#S3">S3</a> bucket.<br />C. Enable
		server&#8211;side encryption on the <a href="#S3">S3</a> bucket.<br />D. Configure an <a href="#S3">S3</a>
		lifecycle rule to transition objects to Amazon <a href="#S3">S3</a> Glacier.<br />E. Use MFA Delete to require
		multi&#8211;factor authentication to delete an object.<br /><br /><b>Correct Answer:</b><br />A. Enable
		versioning on the <a href="#S3">S3</a> bucket.<br />E. Use MFA Delete to require multi&#8211;factor
		authentication to delete an object.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 111<br />A company has multiple AWS accounts for
		various departments. One of the departments wants to share an Amazon <a href="#S3">S3</a> bucket with all other
		departments.<br /><br />Which solution will require the LEAST amount of effort?<br /><br />A. Enable
		cross&#8211;account <a href="#S3">S3</a> replication for the bucket.<br />B. Create a pre&#8211;signed URL for
		the bucket and share it with other departments.<br />C. Set the <a href="#S3">S3</a> bucket policy to allow
		cross&#8211;account access to other departments.<br />D. Create <a href="#IAM">IAM</a> users for each of the
		departments and configure a read&#8211;only <a href="#IAM">IAM</a> policy.<br /><br /><b>Correct
			Answer:</b><br />C. Set the <a href="#S3">S3</a> bucket policy to allow cross&#8211;account access to other
		departments.<br /><br />Answer Description:<br /><a href="#S3">S3</a> standard is the best choice in this
		scenario for a short term storage solution. In this case the size and number of logs is unknown and it would be
		difficult to fully assess the access patterns at this stage. Therefore, using <a href="#S3">S3</a> standard is
		best as it is cost&#8211;effective, provides immediate access, and there are no retrieval fees or minimum
		capacity charge per object.<br /><br />CORRECT: &quot;Amazon <a href="#S3">S3</a> Standard&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Amazon <a href="#S3">S3</a> Intelligent&#8211;Tiering&quot; is incorrect as
		there is an additional fee for using this service and for a short&#8211;term requirement it may not be
		beneficial.<br /><br />INCORRECT: &quot;Amazon <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a
			href="#S3">S3</a> One Zone&#8211;IA)&quot; is incorrect as this storage class has a minimum capacity charge
		per object (128 KB) and a per GB retrieval fee. INCORRECT: &quot;Amazon <a href="#S3">S3</a> Glacier Deep
		Archive&quot; is incorrect as this storage class is used for archiving data. There are retrieval fees and it
		take hours to retrieve data from an archive.<br /><br />References:<br /><br />Amazon <a href="#S3">S3</a>
		Storage Classes</div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 113<br />A company has a custom application
		running on an Amazon EC instance that:<br /><br />Reads a large amount of data from Amazon <a
			href="#S3">S3</a><br />Performs a multi&#8211;stage analysis<br />Writes the results to Amazon
		DynamoDB<br />The application writes a significant number of large, temporary files during the multi&#8211;stage
		analysis. The process performance depends on the temporary storage performance.<br /><br />What would be the
		fastest storage option for holding the temporary files?<br /><br />A. Multiple Amazon <a href="#S3">S3</a>
		buckets with Transfer Acceleration for storage.<br />B. Multiple Amazon EBS drives with Provisioned IOPS and EBS
		optimization.<br />C. Multiple Amazon EFS volumes using the Network File System version 4.1 (NFSv4.1)
		protocol.<br />D. Multiple instance store volumes with software RAID 0.<br /><br /><b>Correct
			Answer:</b><br />A. Multiple Amazon <a href="#S3">S3</a> buckets with Transfer Acceleration for
		storage.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 114<br />A leasing company generates and emails
		PDF statements every month for all its customers. Each statement is about 400 KB in size.<br /><br />Customers
		can download their statements from the website for up to 30 days from when the statements were generated. At the
		end of their 3&#8211;year lease, the customers are emailed a ZIP file that contains all the
		statements.<br /><br />What is the MOST cost&#8211;effective storage solution for this situation?<br /><br />A.
		Store the statements using the Amazon <a href="#S3">S3</a> Standard storage class. Create a lifecycle policy to
		move the statements to Amazon <a href="#S3">S3</a> Glacier storage after 1 day.<br />B. Store the statements
		using the Amazon <a href="#S3">S3</a> Glacier storage class. Create a lifecycle policy to move the statements to
		Amazon <a href="#S3">S3</a> Glacier Deep Archive storage after 30 days.<br />C. Store the statements using the
		Amazon <a href="#S3">S3</a> Standard storage class. Create a lifecycle policy to move the statements to Amazon
		<a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One Zone&#8211;IA) storage after 30
		days.<br />D. Store the statements using the Amazon <a href="#S3">S3</a> Standard&#8211;Infrequent Access (<a
			href="#S3">S3</a> Standard&#8211;IA) storage class. Create a lifecycle policy to move the statements to
		Amazon <a href="#S3">S3</a> Glacier storage after 30 days.<br /><br /><b>Correct Answer:</b><br />B. Store the
		statements using the Amazon <a href="#S3">S3</a> Glacier storage class. Create a lifecycle policy to move the
		statements to Amazon <a href="#S3">S3</a> Glacier Deep Archive storage after 30 days.<br /></div><a
		href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 120<br />A company is planning to deploy an Amazon
		RDS DB instance running Amazon Aurora. The company has a backup retention policy requirement of 90 days. Which
		solution should a solutions architect recommend?<br /><br />A. Set the backup retention period to 90 days when
		creating the RDS DB instance.<br />B. Configure RDS to copy automated snapshots to a user&#8211;managed Amazon
		<a href="#S3">S3</a> bucket with a lifecycle policy set to delete after 90 days.<br />C. Create an AWS Backup
		plan to perform a daily snapshot of the RDS database with the retention set to 90 days. Create an AWS Backup job
		to schedule the execution of the backup plan daily.<br />D. Use a daily scheduled event with Amazon CloudWatch
		Events to execute a custom AWS Lambda function that makes a copy of the RDS automated snapshot. Purge snapshots
		older than 90 days.<br /><br /><b>Correct Answer:</b><br />B. Configure RDS to copy automated snapshots to a
		user&#8211;managed Amazon <a href="#S3">S3</a> bucket with a lifecycle policy set to delete after 90 days.<br />
	</div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 121<br />A company currently has 250 TB of backup
		files stored in Amazon <a href="#S3">S3</a> in a vendor&aposs proprietary format.<br /><br />Using a
		Linux&#8211;based software application provided by the vendor, the company wants to retrieve files from Amazon
		<a href="#S3">S3</a>, transform the files to an industry&#8211;standard format, and re&#8211;upload them to
		Amazon <a href="#S3">S3</a>. The company wants to minimize the data transfer charges associated with this
		conversation.<br /><br />What should a solutions architect do to accomplish this?<br /><br />A. Install the
		conversion software as an Amazon <a href="#S3">S3</a> batch operation so the data is transformed without leaving
		Amazon <a href="#S3">S3</a>.<br />B. Install the conversion software onto an on&#8211;premises virtual machine.
		Perform the transformation and reupload the files to Amazon <a href="#S3">S3</a> from the virtual
		machine.<br />C. Use AWS Snowball Edge devices to export the data and install the conversion software onto the
		devices. Perform the data transformation and re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the
		Snowball Edge devices.<br />D. Launch an Amazon <a href="#EC2">EC2</a> instance in the same Region as Amazon <a
			href="#S3">S3</a> and install the conversion software onto the instance. Perform the transformation and
		re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the <a href="#EC2">EC2</a>
		instance.<br /><br /><b>Correct Answer:</b><br />D. Launch an Amazon <a href="#EC2">EC2</a> instance in the same
		Region as Amazon <a href="#S3">S3</a> and install the conversion software onto the instance. Perform the
		transformation and re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the <a href="#EC2">EC2</a>
		instance.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 129<br />A company has a 143 TB MySQL database
		that it wants to migrate to AWS. The plan is to use Amazon Aurora MySQL as the platform going forward. The
		company has a 100 Mbps AWS Direct Connect connection to Amazon <a href="#VPC">VPC</a>.<br /><br />Which solution
		meets the company&aposs needs and takes the LEAST amount of time?<br /><br />A. Use a gateway endpoint for
		Amazon <a href="#S3">S3</a>. Migrate the data to Amazon <a href="#S3">S3</a>. Import the data into
		Aurora.<br />B. Upgrade the Direct Connect link to 500 Mbps. Copy the data to Amazon <a href="#S3">S3</a>.
		Import the data into Aurora.<br />C. Order an AWS Snowmobile and copy the database backup to it. Have AWS import
		the data into Amazon <a href="#S3">S3</a>. Import the backup into Aurora.<br />D. Order four 50&#8211;TB AWS
		Snowball devices and copy the database backup onto them. Have AWS import the data into Amazon <a
			href="#S3">S3</a>. Import the data into Aurora.<br /><br /><b>Correct Answer:</b><br />D. Order four
		50&#8211;TB AWS Snowball devices and copy the database backup onto them. Have AWS import the data into Amazon <a
			href="#S3">S3</a>. Import the data into Aurora.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 138<br />A company stores user data in AWS. The
		data is used continuously with peak usage during business hours. Access patterns vary, with some data not being
		used for months at a time. A solutions architect must choose a cost&#8211;effective solution that maintains the
		highest level of durability while maintaining high availability.<br /><br />Which storage solution meets these
		requirements?<br /><br />A. Amazon <a href="#S3">S3</a> Standard<br />B. Amazon <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br />C. Amazon <a href="#S3">S3</a> Glacier Deep Archive<br />D. Amazon <a
			href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />B. Amazon <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 139<br />A company receives inconsistent service
		from its data center provider because the company is headquartered in an area affected by natural disasters. The
		company is not ready to fully migrate to the AWS Cloud, but it wants a failure environment on AWS in case the
		on&#8211;premises data center fails.<br /><br />The company runs web servers that connect to external vendors.
		The data available on AWS and on&#8211;premises must be uniform.<br /><br />Which solution should a solutions
		architect recommend that has the LEAST amount of downtime?<br /><br />A. Configure an Amazon Route 53 failover
		record. Run application servers on Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer
		in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon <a
			href="#S3">S3</a>.<br />B. Configure an Amazon Route 53 failover record. Execute an AWS CloudFormation
		template from a script to create Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer.
		Set up AWS Storage Gateway with stored volumes to back up data to Amazon <a href="#S3">S3</a>.<br />C. Configure
		an Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a <a href="#VPC">VPC</a> and
		the data center. Run application servers on Amazon <a href="#EC2">EC2</a> in an Auto Scaling group. Run an AWS
		Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer.<br />D.
		Configure an Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation
		template to launch two Amazon <a href="#EC2">EC2</a> instances. Set up AWS Storage Gateway with stored volumes
		to back up data to Amazon <a href="#S3">S3</a>. Set up an AWS Direct Connect connection between a <a
			href="#VPC">VPC</a> and the data center.<br /><br /><b>Correct Answer:</b><br />A. Configure an Amazon Route
		53 failover record. Run application servers on Amazon <a href="#EC2">EC2</a> instances behind an Application
		Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon
		<a href="#S3">S3</a>.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 145<br />A manufacturing company wants to
		implement predictive maintenance on its machinery equipment. The company will install thousands of IoT sensors
		that will send data to AWS in real time. A solutions architect is tasked with implementing a solution that will
		receive events in an ordered manner for each machinery asset and ensure that data is saved for further
		processing at a later time.<br /><br />Which solution would be MOST efficient?<br /><br />A. Use Amazon Kinesis
		Data Streams for real&#8211;time events with a partition for each equipment asset. Use Amazon Kinesis Data
		Firehose to save data to Amazon <a href="#S3">S3</a>.<br />B. Use Amazon Kinesis Data Streams for
		real&#8211;time events with a shard for each equipment asset. Use Amazon Kinesis Data Firehose to save data to
		Amazon EBS.<br />C. Use an Amazon SQS FIFO queue for real&#8211;time events with one queue for each equipment
		asset. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS.<br />D. Use an Amazon SQS
		standard queue for real&#8211;time events with one queue for each equipment asset. Trigger an AWS Lambda
		function from the SQS queue to save data to Amazon <a href="#S3">S3</a>.<br /><br /><b>Correct
			Answer:</b><br />A. Use Amazon Kinesis Data Streams for real&#8211;time events with a partition for each
		equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon <a href="#S3">S3</a>.<br /><br />Answer
		Description:<br />Amazon SQS Introduces FIFO Queues with Exactly&#8211;Once Processing and Lower Prices for
		Standard Queues<br /><br />You can now use Amazon Simple Queue Service (SQS) for applications that require
		messages to be processed in a strict sequence and exactly once using First&#8211;in, First&#8211;out (FIFO)
		queues. FIFO queues are designed to ensure that the order in which messages are sent and received is strictly
		preserved and that each message is processed exactly once.<br /><br />Amazon SQS is a reliable and
		highly&#8211;scalable managed message queue service for storing messages in transit between application
		components. FIFO queues complement the existing Amazon SQS standard queues, which offer high throughput,
		best&#8211;effort ordering, and at&#8211;least&#8211;once delivery. FIFO queues have essentially the same
		features as standard queues, but provide the added benefits of supporting ordering and exactly&#8211;once
		processing. FIFO queues provide additional features that help prevent unintentional duplicates from being sent
		by message producers or from being received by message consumers. Additionally, message groups allow multiple
		separate ordered message streams within the same queue.<br /><br />Amazon Kinesis Data Streams collect and
		process data in real time. A Kinesis data stream is a set of shards. Each shard has a sequence of data records.
		Each data record has a sequence number that is assigned by Kinesis Data Streams. A shard is a uniquely
		identified sequence of data records in a stream.<br /><br />A partition key is used to group data by shard
		within a stream. Kinesis Data Streams segregates the data records belonging to a stream into multiple shards. It
		uses the partition key that is associated with each data record to determine which shard a given data record
		belongs to.<br /><br />For this scenario, the solutions architect can use a partition key for each device. This
		will ensure the records for that device are grouped by shard and the shard will ensure ordering. Amazon <a
			href="#S3">S3</a> is a valid destination for saving the data records.<br /><br />CORRECT: &quot;Use Amazon
		Kinesis Data Streams for real&#8211;time events with a partition key for each device. Use Amazon Kinesis Data
		Firehose to save data to Amazon <a href="#S3">S3</a>&quot; is the correct answer.<br /><br />INCORRECT:
		&quot;Use Amazon Kinesis Data Streams for real&#8211;time events with a shard for each device. Use Amazon
		Kinesis Data Firehose to save data to Amazon EBS&quot; is incorrect as you cannot save data to EBS from
		Kinesis.<br /><br />INCORRECT: &quot;Use an Amazon SQS FIFO queue for real&#8211;time events with one queue for
		each device. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS&quot; is incorrect as
		SQS is not the most efficient service for streaming, real time data.<br /><br />INCORRECT: &quot;Use an Amazon
		SQS standard queue for real&#8211;time events with one queue for each device. Trigger an AWS Lambda function
		from the SQS queue to save data to Amazon <a href="#S3">S3</a>&quot; is incorrect as SQS is not the most
		efficient service for streaming, real time data.<br /><br />References:<br /><br />Amazon Kinesis Data Streams >
		Developer Guide > Amazon Kinesis Data Streams Terminology and Concepts</div><a href="#S3">S3(44)</a> <a href="#"
		<i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 149<br />A company hosts a static website
		on&#8211;premises and wants to migrate the website to AWS. The website should load as quickly as possible for
		users around the world. The company also wants the most cost&#8211;effective solution.<br /><br />What should a
		solutions architect do to accomplish this?<br /><br />A. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Replicate the <a
			href="#S3">S3</a> bucket to multiple AWS Regions.<br />B. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin.<br />C. Copy the
		website content to an Amazon EBS&#8211;backed Amazon <a href="#EC2">EC2</a> instance running Apache HTTP Server.
		Configure Amazon Route 53 geolocation routing policies to select the closest origin.<br />D. Copy the website
		content to multiple Amazon EBS&#8211;backed Amazon <a href="#EC2">EC2</a> instances running Apache HTTP Server
		in multiple AWS Regions. Configure Amazon <a href="#CloudFront">CloudFront</a> geolocation routing policies to
		select the closest origin.<br /><br /><b>Correct Answer:</b><br />B. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin.<br /><br />Answer
		Description:<br />What Is Amazon <a href="#CloudFront">CloudFront</a>?<br />Amazon <a
			href="#CloudFront">CloudFront</a> is a web service that speeds up distribution of your static and dynamic
		web content, such as .html, .css, .js, and image files, to your users. <a href="#CloudFront">CloudFront</a>
		delivers your content through a worldwide network of data centers called edge locations. When a user requests
		content that you&aposre serving with <a href="#CloudFront">CloudFront</a>, the user is routed to the edge
		location that provides the lowest latency (time delay), so that content is delivered with the best possible
		performance.<br /><br />Using Amazon <a href="#S3">S3</a> Buckets for Your Origin<br />When you use Amazon <a
			href="#S3">S3</a> as an origin for your distribution, you place any objects that you want <a
			href="#CloudFront">CloudFront</a> to deliver in an Amazon <a href="#S3">S3</a> bucket. You can use any
		method that is supported by Amazon <a href="#S3">S3</a> to get your objects into Amazon <a href="#S3">S3</a>,
		for example, the Amazon <a href="#S3">S3</a> console or API, or a third&#8211;party tool. You can create a
		hierarchy in your bucket to store the objects, just as you would with any other Amazon <a href="#S3">S3</a>
		bucket.<br /><br />Using an existing Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change the bucket in any way; you can still use
		it as you normally would to store and access Amazon <a href="#S3">S3</a> objects at the standard Amazon <a
			href="#S3">S3</a> price. You incur regular Amazon <a href="#S3">S3</a> charges for storing the objects in
		the bucket.<br /><br />The most cost&#8211;effective option is to migrate the website to an Amazon <a
			href="#S3">S3</a> bucket and configure that bucket for static website hosting. To enable good performance
		for global users the solutions architect should then configure a <a href="#CloudFront">CloudFront</a>
		distribution with the <a href="#S3">S3</a> bucket as the origin. This will cache the static content around the
		world closer to users.<br /><br />CORRECT: &quot;Copy the website content to an Amazon <a href="#S3">S3</a>
		bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Copy the website content to an Amazon <a href="#S3">S3</a> bucket. Configure
		the bucket to serve static webpage content. Replicate the <a href="#S3">S3</a> bucket to multiple AWS
		Regions&quot; is incorrect as there is no solution here for directing users to the closest region. This could be
		a more cost&#8211;effective (though less elegant) solution if AWS Route 53 latency records are
		created.<br /><br />INCORRECT: &quot;Copy the website content to an Amazon <a href="#EC2">EC2</a> instance.
		Configure Amazon Route 53 geolocation routing policies to select the closest origin&quot; is incorrect as using
		Amazon <a href="#EC2">EC2</a> instances is less cost&#8211;effective compared to hosting the website on <a
			href="#S3">S3</a>. Also, geolocation routing does not achieve anything with only a single
		record.<br /><br />INCORRECT: &quot;Copy the website content to multiple Amazon <a href="#EC2">EC2</a> instances
		in multiple AWS Regions. Configure AWS Route 53 geolocation routing policies to select the closest region&quot;
		is incorrect as using Amazon <a href="#EC2">EC2</a> instances is less cost&#8211;effective compared to hosting
		the website on <a href="#S3">S3</a>.<br /><br />References:<br /><br />How do I use <a
			href="#CloudFront">CloudFront</a> to serve a static website hosted on Amazon <a href="#S3">S3</a>?</div><a
		href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 154<br />A company has a three&#8211;tier
		image&#8211;sharing application. It uses an Amazon <a href="#EC2">EC2</a> instance for the front&#8211;end
		layer, another for the backend tier, and a third for the MySQL database. A solutions architect has been tasked
		with designing a solution that is highly available, and requires the least amount of changes to the
		application.<br /><br />Which solution meets these requirements?<br /><br />A. Use Amazon <a href="#S3">S3</a>
		to host the front&#8211;end layer and AWS Lambda functions for the backend layer. Move the database to an Amazon
		DynamoDB table and use Amazon <a href="#S3">S3</a> to store and serve users&apos images.<br />B. Use
		load&#8211;balanced Multi&#8211;AZ AWS Elastic Beanstalk environments for the front&#8211;end and backend
		layers. Move the database to an Amazon RDS instance with multiple read replicas to store and serve users&apos
		images.<br />C. Use Amazon <a href="#S3">S3</a> to host the front&#8211;end layer and a fleet of Amazon <a
			href="#EC2">EC2</a> instances in an Auto Scaling group for the backend layer. Move the database to a memory
		optimized instance type to store and serve users&apos images.<br />D. Use load&#8211;balanced Multi&#8211;AZ AWS
		Elastic Beanstalk environments for the front&#8211;end and backend layers. Move the database to an Amazon RDS
		instance with a Multi&#8211;AZ deployment. Use Amazon <a href="#S3">S3</a> to store and serve users&apos
		images.<br /><br /><b>Correct Answer:</b><br />D. Use load&#8211;balanced Multi&#8211;AZ AWS Elastic Beanstalk
		environments for the front&#8211;end and backend layers. Move the database to an Amazon RDS instance with a
		Multi&#8211;AZ deployment. Use Amazon <a href="#S3">S3</a> to store and serve users&apos
		images.<br /><br />Answer Description:<br />Keyword: Highly available + Least amount of changes to the
		application High Availability = Multi&#8211;AZ<br /><br />Least amount of changes to the application = Elastic
		Beanstalk Automatically handles the deployment, from capacity provisioning, Load Balancing, Auto Scaling to
		application health monitoring<br /><br />Option &#8212; D will be the right choice and Option &#8212; A; Option
		&#8212; B and Option &#8212; C out of race due to Cost & inter&#8211;operability.<br /><br />HA with Elastic
		Beanstalk and RDS<br /><br />AWS Elastic Beanstalk<br /><br />AWS Elastic Beanstalk is an
		easy&#8211;to&#8211;use service for deploying and scaling web applications and services developed with Java,
		.NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and
		IIS.<br /><br />You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from
		capacity provisioning, load balancing, auto&#8211;scaling to application health monitoring. At the same time,
		you retain full control over the AWS resources powering your application and can access the underlying resources
		at any time.<br /><br />There is no additional charge for Elastic Beanstalk &#8212; you pay only for the AWS
		resources needed to store and run your applications.<br /><br />AWS RDS<br />Amazon Relational Database Service
		(Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides
		cost&#8211;efficient and resizable capacity while automating time&#8211;consuming administration tasks such as
		hardware provisioning, database setup, patching and backups. It frees you to focus on your applications so you
		can give them the fast performance, high availability, security and compatibility they need.<br /><br />Amazon
		RDS is available on several database instance types &#8212; optimized for memory, performance or I/O &#8212; and
		provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL,
		MariaDB, Oracle Database, and SQL Server. You can use the AWS Database Migration Service to easily migrate or
		replicate your existing databases to Amazon RDS.<br /><br />AWS <a href="#S3">S3</a><br />Amazon Simple Storage
		Service (Amazon <a href="#S3">S3</a>) is an object storage service that offers industry&#8211;leading
		scalability, data availability, security, and performance. This means customers of all sizes and industries can
		use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications,
		backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon <a
			href="#S3">S3</a> provides easy&#8211;to&#8211;use management features so you can organize your data and
		configure finely&#8211;tuned access controls to meet your specific business, organizational, and compliance
		requirements. Amazon <a href="#S3">S3</a> is designed for 99.999999999% (11 9&aposs) of durability, and stores
		data for millions of applications for companies all around the world.<br /><br />References:<br /><br />AWS
		Elastic Beanstalk<br />Amazon Relational Database Service (RDS)<br />Amazon <a href="#S3">S3</a></div><a
		href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 159<br />A Solutions Architect must design a web
		application that will be hosted on AWS, allowing users to purchase access to premium, shared content that is
		stored in an <a href="#S3">S3</a> bucket. Upon payment, content will be available for download for 14 days
		before the user is denied access.<br /><br />Which of the following would be the LEAST complicated
		implementation?<br /><br />A. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an origin
		access identity (OAI). Configure the distribution with an Amazon <a href="#S3">S3</a> origin to provide access
		to the file through signed URLs. Design a Lambda function to remove data that is older than 14 days.<br />B. Use
		an <a href="#S3">S3</a> bucket and provide direct access to the file. Design the application to track purchases
		in a DynamoDB table. Configure a Lambda function to remove data that is older than 14 days based on a query to
		Amazon DynamoDB.<br />C. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an OAI. Configure
		the distribution with an Amazon <a href="#S3">S3</a> origin to provide access to the file through signed URLs.
		Design the application to set an expiration of 14 days for the URL.<br />D. Use an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with an OAI. Configure the distribution with an Amazon <a
			href="#S3">S3</a> origin to provide access to the file through signed URLs. Design the application to set an
		expiration of 60 minutes for the URL and recreate the URL as necessary.<br /><br /><b>Correct
			Answer:</b><br />C. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an OAI. Configure
		the distribution with an Amazon <a href="#S3">S3</a> origin to provide access to the file through signed URLs.
		Design the application to set an expiration of 14 days for the URL.<br /></div><a href="#S3">S3(44)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 167<br />A solutions architect needs to design a
		low&#8211;latency solution for a static single&#8211;page application accessed by users utilizing a custom
		domain name. The solution must be serverless, encrypted in transit, and cost&#8211;effective.<br /><br />Which
		combination of AWS services and features should the solutions architect use? (Choose two.)<br /><br />A. Amazon
		<a href="#S3">S3</a><br />B. Amazon <a href="#EC2">EC2</a><br />C. AWS Fargate<br />D. Amazon <a
			href="#CloudFront">CloudFront</a><br />E. Elastic Load Balancer<br /><br /><b>Correct Answer:</b><br />A.
		Amazon <a href="#S3">S3</a><br />D. Amazon <a href="#CloudFront">CloudFront</a><br /></div><a
		href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 172<br />A company collects temperature, humidity,
		and atmospheric pressure data in cities across multiple continents. The average volume of data collected per
		site each day is 500 GB. Each site has a high&#8211;speed internet connection. The company&aposs weather
		forecasting applications are based in a single Region and analyze the data daily.<br /><br />What is the FASTEST
		way to aggregate data from all of these global sites?<br /><br />A. Enable Amazon <a href="#S3">S3</a> Transfer
		Acceleration on the destination bucket. Use multipart uploads to directly upload site data to the destination
		bucket.<br />B. Upload site data to an Amazon <a href="#S3">S3</a> bucket in the closest AWS Region. Use <a
			href="#S3">S3</a> cross&#8211;Region replication to copy objects to the destination bucket.<br />C. Schedule
		AWS Snowball jobs daily to transfer data to the closest AWS Region. Use <a href="#S3">S3</a> cross&#8211;Region
		replication to copy objects to the destination bucket.<br />D. Upload the data to an Amazon <a
			href="#EC2">EC2</a> instance in the closest Region. Store the data in an Amazon EBS volume. Once a day take
		an EBS snapshot and copy it to the centralized Region. Restore the EBS volume in the centralized Region and run
		an analysis on the data daily.<br /><br /><b>Correct Answer:</b><br />B. Upload site data to an Amazon <a
			href="#S3">S3</a> bucket in the closest AWS Region. Use <a href="#S3">S3</a> cross&#8211;Region replication
		to copy objects to the destination bucket.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 176<br />A solutions architect is designing a
		solution where users will be directed to a backup static error page if the primary website is unavailable. The
		primary website&aposs DNS records are hosted in Amazon Route 53 where their domain is pointing to an Application
		Load Balancer (ALB).<br /><br />Which configuration should the solutions architect use to meet the company&aposs
		needs while minimizing changes and infrastructure overhead?<br /><br />A. Point a Route 53 alias record to an
		Amazon <a href="#CloudFront">CloudFront</a> distribution with the ALB as one of its origins. Then, create custom
		error pages for the distribution.<br />B. Set up a Route 53 active&#8211;passive failover configuration. Direct
		traffic to a static error page hosted within an Amazon <a href="#S3">S3</a> bucket when Route 53 health checks
		determine that the ALB endpoint is unhealthy.<br />C. Update the Route 53 record to use a latency&#8211;based
		routing policy. Add the backup static error page hosted within an Amazon <a href="#S3">S3</a> bucket to the
		record so the traffic is sent to the most responsive endpoints.<br />D. Set up a Route 53 active&#8211;active
		configuration with the ALB and an Amazon <a href="#EC2">EC2</a> instance hosting a static error page as
		endpoints. Route 53 will only send requests to the instance if the health checks fail for the
		ALB.<br /><br /><b>Correct Answer:</b><br />B. Set up a Route 53 active&#8211;passive failover configuration.
		Direct traffic to a static error page hosted within an Amazon <a href="#S3">S3</a> bucket when Route 53 health
		checks determine that the ALB endpoint is unhealthy.<br /><br />Answer Description:<br />Active&#8211;passive
		failover<br />Use an active&#8211;passive failover configuration when you want a primary resource or group of
		resources to be available the majority of the time and you want a secondary resource or group of resources to be
		on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes
		only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only
		the healthy secondary resources in response to DNS queries.<br /><br />To create an active&#8211;passive
		failover configuration with one primary record and one secondary record, you just create the records and specify
		Failover for the routing policy. When the primary resource is healthy, Route 53 responds to DNS queries using
		the primary record. When the primary resource is unhealthy, Route 53 responds to DNS queries using the secondary
		record.<br /><br />How Amazon Route 53 averts cascading failures<br />As the first defense against cascading
		failures, each request routing algorithm (such as weighted and failover) has a mode of last resort. In this
		special mode, when all records are considered unhealthy, the Route 53 algorithm reverts to considering all
		records healthy.<br /><br />For example, if all instances of an application, on several hosts, are rejecting
		health check requests, Route 53 DNS servers will choose an answer anyway and return it rather than returning no
		DNS answer or returning an NXDOMAIN (non&#8211;existent domain) response. An application can respond to users
		but still fail health checks, so this provides some protection against misconfiguration.<br /><br />Similarly,
		if an application is overloaded, and one out of three endpoints fails its health checks, so that it&aposs
		excluded from Route 53 DNS responses, Route 53 distributes responses between the two remaining endpoints. If the
		remaining endpoints are unable to handle the additional load and they fail, Route 53 reverts to distributing
		requests to all three endpoints.<br /><br />Using Amazon <a href="#CloudFront">CloudFront</a> as the
		front&#8211;end provides the option to specify a custom message instead of the default message. To specify the
		specific file that you want to return and the errors for which the file should be returned, you update your <a
			href="#CloudFront">CloudFront</a> distribution to specify those values.<br /><br />For example, the
		following is a customized error message:<br /><br />The <a href="#CloudFront">CloudFront</a> distribution can
		use the ALB as the origin, which will cause the website content to be cached on the <a
			href="#CloudFront">CloudFront</a> edge caches.<br /><br />This solution represents the most operationally
		efficient choice as no action is required in the event of an issue, other than troubleshooting the root
		cause.<br /><br />References:<br /><br />Amazon <a href="#CloudFront">CloudFront</a> > Developer Guide > What is
		Amazon <a href="#CloudFront">CloudFront</a>?</div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 180<br />A company wants to migrate a high
		performance computing (HPC) application and data from on&#8211;premises to the AWS Cloud. The company uses
		tiered storage on&#8211;premises with hot high&#8211;performance parallel storage to support the application
		during periodic runs of the application, and more economical cold storage to hold the data when the application
		is not actively running.<br /><br />Which combination of solutions should a solutions architect recommend to
		support the storage needs of the application? (Choose two.)<br /><br />A. Amazon <a href="#S3">S3</a> for cold
		data storage<br />B. Amazon EFS for cold data storage<br />C. Amazon <a href="#S3">S3</a> for
		high&#8211;performance parallel storage<br />D. Amazon FSx for Lustre for high&#8211;performance parallel
		storage<br />E. Amazon FSx for Windows for high&#8211;performance parallel storage<br /><br /><b>Correct
			Answer:</b><br />A. Amazon <a href="#S3">S3</a> for cold data storage<br />D. Amazon FSx for Lustre for
		high&#8211;performance parallel storage<br /><br />Answer Description:<br />Amazon FSx for Lustre makes it easy
		and cost effective to launch and run the world&aposs most popular high&#8211;performance file system. Use it for
		workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and
		financial modeling.<br /><br />Amazon FSx for Lustre provides a high&#8211;performance file system optimized for
		fast processing of workloads such as machine learning, high&#8211;performance computing (HPC), video processing,
		financial modeling, and electronic design automation (EDA).<br /><br />These workloads commonly require data to
		be presented via a fast and scalable file system interface, and typically have data sets stored on
		long&#8211;term data stores like Amazon <a href="#S3">S3</a>.<br /><br />Amazon FSx works natively with Amazon
		<a href="#S3">S3</a>, making it easy to access your <a href="#S3">S3</a> data to run data processing workloads.
		Your <a href="#S3">S3</a> objects are presented as files in your file system, and you can write your results
		back to <a href="#S3">S3</a>. This lets you run data processing workloads on FSx for Lustre and store your
		long&#8211;term data on <a href="#S3">S3</a> or on&#8211;premises data stores.<br /><br />Therefore, the best
		combination for this scenario is to use <a href="#S3">S3</a> for cold data and FSx for Lustre for the parallel
		HPC job.<br /><br />CORRECT: &quot;Amazon <a href="#S3">S3</a> for cold data storage&quot; is the correct
		answer.<br /><br />CORRECT: &quot;Amazon FSx for Lustre for high&#8211;performance parallel storage&quot; is the
		correct answer. INCORRECT: &quot;Amazon EFS for cold data storage&quot; is incorrect as FSx works natively with
		<a href="#S3">S3</a> which is also more economical.<br /><br />INCORRECT: &quot;Amazon <a href="#S3">S3</a> for
		high&#8211;performance parallel storage&quot; is incorrect as <a href="#S3">S3</a> is not suitable for running
		high&#8211;performance computing jobs.<br /><br />INCORRECT: &quot;Amazon FSx for Windows for
		high&#8211;performance parallel storage&quot; is incorrect as FSx for Lustre should be used for HPC use cases
		and use cases that require storing data on <a href="#S3">S3</a>.<br /><br />References:<br /><br />Amazon FSx
		for Lustre<br /><br /><br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 182<br />A solutions architect at an eCommerce
		company wants to back up application log data to Amazon <a href="#S3">S3</a>. The solutions architect is unsure
		how frequently the logs will be accessed or which logs will be accessed the most. The company wants to keep
		costs as low as possible by using the appropriate <a href="#S3">S3</a> storage class.<br /><br />Which <a
			href="#S3">S3</a> storage class should be implemented to meet these requirements?<br /><br />A. <a
			href="#S3">S3</a> Glacier<br />B. <a href="#S3">S3</a> Intelligent&#8211;Tiering<br />C. <a
			href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a> Standard&#8211;IA)<br />D. <a
			href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA)<br /><br /><b>Correct Answer:</b><br />B. <a href="#S3">S3</a>
		Intelligent&#8211;Tiering<br /><br />Answer Description:<br /><a href="#S3">S3</a> Intelligent&#8211;Tiering is
		a new Amazon <a href="#S3">S3</a> storage class designed for customers who want to optimize storage costs
		automatically when data access patterns change, without performance impact or operational overhead. <a
			href="#S3">S3</a> Intelligent&#8211;Tiering is the first cloud object storage class that delivers automatic
		cost savings by moving data between two access tiers &#8212; frequent access and infrequent access &#8212; when
		access patterns change, and is ideal for data with unknown or changing access patterns.<br /><br /><a
			href="#S3">S3</a> Intelligent&#8211;Tiering stores objects in two access tiers: one tier that is optimized
		for frequent access and another lower&#8211;cost tier that is optimized for infrequent access. For a small
		monthly monitoring and automation fee per object, <a href="#S3">S3</a> Intelligent&#8211;Tiering monitors access
		patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access
		tier.<br /><br />There are no retrieval fees in <a href="#S3">S3</a> Intelligent&#8211;Tiering. If an object in
		the infrequent access tier is accessed later, it is automatically moved back to the frequent access tier. No
		additional tiering fees apply when objects are moved between access tiers within the <a href="#S3">S3</a>
		Intelligent&#8211;Tiering storage class. <a href="#S3">S3</a> Intelligent&#8211;Tiering is designed for 99.9%
		availability and 99.999999999% durability, and offers the same low latency and high throughput performance of <a
			href="#S3">S3</a> Standard.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 191<br />A company is looking for a solution that
		can store video archives in AWS from old news footage. The company needs to minimize costs and will rarely need
		to restore these files. When the files are needed, they must be available in a maximum of five
		minutes.<br /><br />What is the MOST cost&#8211;effective solution?<br /><br />A. Store the video archives in
		Amazon <a href="#S3">S3</a> Glacier and use Expedited retrievals.<br />B. Store the video archives in Amazon <a
			href="#S3">S3</a> Glacier and use Standard retrievals.<br />C. Store the video archives in Amazon <a
			href="#S3">S3</a> Standard&#8211;Infrequent Access (<a href="#S3">S3</a> Standard&#8211;IA).<br />D. Store
		the video archives in Amazon <a href="#S3">S3</a> One Zone&#8211;Infrequent Access (<a href="#S3">S3</a> One
		Zone&#8211;IA).<br /><br /><b>Correct Answer:</b><br />A. Store the video archives in Amazon <a
			href="#S3">S3</a> Glacier and use Expedited retrievals.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 192<br />A healthcare company stores highly
		sensitive patient records. Compliance requires that multiple copies be stored in different locations. Each
		record must be stored for 7 years. The company has a service level agreement (SLA) to provide records to
		government agencies immediately for the first 30 days and then within 4 hours of a request
		thereafter.<br /><br />What should a solutions architect recommend?<br /><br />A. Use Amazon <a
			href="#S3">S3</a> with cross&#8211;Region replication enabled. After 30 days, transition the data to Amazon
		<a href="#S3">S3</a> Glacier using lifecycle policy.<br />B. Use Amazon <a href="#S3">S3</a> with
		cross&#8211;origin resource sharing (CORS) enabled. After 30 days, transition the data to Amazon <a
			href="#S3">S3</a> Glacier using a lifecycle policy.<br />C. Use Amazon <a href="#S3">S3</a> with
		cross&#8211;Region replication enabled. After 30 days, transition the data to Amazon <a href="#S3">S3</a>
		Glacier Deep Achieve using a lifecycle policy.<br />D. Use Amazon <a href="#S3">S3</a> with cross&#8211;origin
		resource sharing (CORS) enabled. After 30 days, transition the data to Amazon <a href="#S3">S3</a> Glacier Deep
		Archive using a lifecycle policy.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon <a href="#S3">S3</a> with
		cross&#8211;Region replication enabled. After 30 days, transition the data to Amazon <a href="#S3">S3</a>
		Glacier using lifecycle policy.<br /></div><a href="#S3">S3(44)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a><a id=IAM>
		<h2>IAM</h2>
	</a> - 5 Questions <br><a href="#IAM">IAM(5)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 34<br />A company allows its developers to attach
		existing <a href="#IAM">IAM</a> policies to existing <a href="#IAM">IAM</a> roles to enable faster
		experimentation and agility. However, the security operations team is concerned that the developers could attach
		the existing administrator policy, which would allow the developers to circumvent any other security
		policies.<br /><br />How should a solutions architect address this issue?<br /><br />A. Create an Amazon SNS
		topic to send an alert every time a developer creates a new policy.<br />B. Use service control policies to
		disable <a href="#IAM">IAM</a> activity across all account in the organizational unit.<br />C. Prevent the
		developers from attaching any policies and assign all <a href="#IAM">IAM</a> duties to the security operations
		team.<br />D. Set an <a href="#IAM">IAM</a> permissions boundary on the developer <a href="#IAM">IAM</a> role
		that explicitly denies attaching the administrator policy.<br /><br /><b>Correct Answer:</b><br />D. Set an <a
			href="#IAM">IAM</a> permissions boundary on the developer <a href="#IAM">IAM</a> role that explicitly denies
		attaching the administrator policy.<br /><br />Answer Description:<br />The permissions boundary for an <a
			href="#IAM">IAM</a> entity (user or role) sets the maximum permissions that the entity can have. This can
		change the effective permissions for that user or role. The effective permissions for an entity are the
		permissions that are granted by all the policies that affect the user or role. Within an account, the
		permissions for an entity can be affected by identity&#8211;based policies, resource&#8211;based policies,
		permissions boundaries, Organizations SCPs, or session policies.<br /><br />Therefore, the solutions architect
		can set an <a href="#IAM">IAM</a> permissions boundary on the developer <a href="#IAM">IAM</a> role that
		explicitly denies attaching the administrator policy.<br /><br />CORRECT: &quot;Set an <a href="#IAM">IAM</a>
		permissions boundary on the developer <a href="#IAM">IAM</a> role that explicitly denies attaching the
		administrator policy&quot; is the correct answer.<br /><br />INCORRECT: &quot;Create an Amazon SNS topic to send
		an alert every time a developer creates a new policy&quot; is incorrect as this would mean investigating every
		incident which is not an efficient solution.<br /><br />INCORRECT: &quot;Use service control policies to disable
		<a href="#IAM">IAM</a> activity across all accounts in the organizational unit&quot; is incorrect as this would
		prevent the developers from being able to work with <a href="#IAM">IAM</a> completely.<br /><br />INCORRECT:
		&quot;Prevent the developers from attaching any policies and assign all <a href="#IAM">IAM</a> duties to the
		security operations team&quot; is incorrect as this is not necessary. The requirement is to allow developers to
		work with policies, the solution needs to find a secure way of achieving this.<br /><br />References:<br />AWS
		Identity and Access Management > User Guide > Permissions boundaries for <a href="#IAM">IAM</a> entities<br />
	</div><a href="#IAM">IAM(5)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 70<br />A company has enabled AWS CloudTrail logs
		to deliver log files to an Amazon <a href="#S3">S3</a> bucket for each of its developer accounts. The company
		has created a central AWS account for streamlining management and audit reviews. An internal auditor needs to
		access the CloudTrail logs, yet access needs to be restricted for all developer account users. The solution must
		be secure and optimized.<br /><br />How should a solutions architect meet these requirements?<br /><br />A.
		Configure an AWS Lambda function in each developer account to copy the log files to the central account. Create
		an <a href="#IAM">IAM</a> role in the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy
		providing read only permissions to the bucket.<br />B. Configure CloudTrail from each developer account to
		deliver the log files to an <a href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a>
		user in the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing full permissions
		to the bucket.<br />C. Configure CloudTrail from each developer account to deliver the log files to an <a
			href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a> role in the central
		account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing read only permissions to the
		bucket.<br />D. Configure an AWS Lambda function in the central account to copy the log files from the <a
			href="#S3">S3</a> bucket in each developer account. Create an <a href="#IAM">IAM</a> user in the central
		account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing full permissions to the
		bucket.<br /><br /><b>Correct Answer:</b><br />C. Configure CloudTrail from each developer account to deliver
		the log files to an <a href="#S3">S3</a> bucket in the central account. Create an <a href="#IAM">IAM</a> role in
		the central account for the auditor. Attach an <a href="#IAM">IAM</a> policy providing read only permissions to
		the bucket.<br /><br /> Go to dashboard</div><a href="#IAM">IAM(5)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 91<br />A company has established a new AWS
		account. The account is newly provisioned and no changed have been made to the default settings. The company is
		concerned about the security of the AWS account root user.<br /><br />What should be done to secure the root
		user?<br /><br />A. Create <a href="#IAM">IAM</a> users for daily administrative tasks. Disable the root
		user.<br />B. Create <a href="#IAM">IAM</a> users for daily administrative tasks. Enable multi&#8211;factor
		authentication on the root user.<br />C. Generate an access key for the root user. Use the access key for daily
		administration tasks instead of the AWS Management Console.<br />D. Provide the root user credentials to the
		most senior solutions architect. Have the solutions architect use the root user for daily administration
		tasks.<br /><br /><b>Correct Answer:</b><br />B. Create <a href="#IAM">IAM</a> users for daily administrative
		tasks. Enable multi&#8211;factor authentication on the root user.<br /></div><a href="#IAM">IAM(5)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 135<br />An application running on an Amazon <a
			href="#EC2">EC2</a> instance needs to access an Amazon DynamoDB table. Both the <a href="#EC2">EC2</a>
		instance and the DynamoDB table are in the same AWS account. A solutions architect must configure the necessary
		permissions.<br /><br />Which solution will allow least privilege access to the DynamoDB table from the <a
			href="#EC2">EC2</a> instance?<br /><br />A. Create an <a href="#IAM">IAM</a> role with the appropriate
		policy to allow access to the DynamoDB table. Create an instance profile to assign this <a href="#IAM">IAM</a>
		role to the <a href="#EC2">EC2</a> instance.<br />B. Create an <a href="#IAM">IAM</a> role with the appropriate
		policy to allow access to the DynamoDB table. Add the <a href="#EC2">EC2</a> instance to the trust relationship
		policy document to allow it to assume the role.<br />C. Create an <a href="#IAM">IAM</a> user with the
		appropriate policy to allow access to the DynamoDB table. Store the credentials in an Amazon <a
			href="#S3">S3</a> bucket and read them from within the application code directly.<br />D. Create an <a
			href="#IAM">IAM</a> user with the appropriate policy to allow access to the DynamoDB table. Ensure that the
		application stores the <a href="#IAM">IAM</a> credentials securely on local storage and uses them to make the
		DynamoDB calls.<br /><br /><b>Correct Answer:</b><br />A. Create an <a href="#IAM">IAM</a> role with the
		appropriate policy to allow access to the DynamoDB table. Create an instance profile to assign this <a
			href="#IAM">IAM</a> role to the <a href="#EC2">EC2</a> instance.<br /></div><a href="#IAM">IAM(5)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 152<br />A marketing company is storing CSV files
		in an Amazon <a href="#S3">S3</a> bucket for statistical analysis. An application on an Amazon <a
			href="#EC2">EC2</a> instance needs permission to efficiently process the CSV data stored in the <a
			href="#S3">S3</a> bucket.<br /><br />Which action will MOST securely grant the <a href="#EC2">EC2</a>
		instance access to the <a href="#S3">S3</a> bucket?<br /><br />A. Attach a resource&#8211;based policy to the <a
			href="#S3">S3</a> bucket.<br />B. Create an <a href="#IAM">IAM</a> user for the application with specific
		permissions to the <a href="#S3">S3</a> bucket.<br />C. Associate an <a href="#IAM">IAM</a> role with least
		privilege permissions to the <a href="#EC2">EC2</a> instance profile.<br />D. Store AWS credentials directly on
		the <a href="#EC2">EC2</a> instance for applications on the instance to use for API calls.<br /><br /><b>Correct
			Answer:</b><br />C. Associate an <a href="#IAM">IAM</a> role with least privilege permissions to the <a
			href="#EC2">EC2</a> instance profile.<br /><br />Answer Description:<br />Keyword: Privilege Permission + <a
			href="#IAM">IAM</a> Role<br /><br />AWS Identity and Access Management (<a href="#IAM">IAM</a>) enables you
		to manage access to AWS services and resources securely. Using <a href="#IAM">IAM</a>, you can create and manage
		AWS users and groups, and use permissions to allow and deny their access to AWS resources.<br /><br /><a
			href="#IAM">IAM</a> is a feature of your AWS account offered at no additional charge. You will be charged
		only for use of other AWS services by your users.<br /><br /><a href="#IAM">IAM</a> roles for Amazon <a
			href="#EC2">EC2</a><br />Applications must sign their API requests with AWS credentials. Therefore, if you
		are an application developer, you need a strategy for managing credentials for your applications that run on <a
			href="#EC2">EC2</a> instances. For example, you can securely distribute your AWS credentials to the
		instances, enabling the applications on those instances to use your credentials to sign requests, while
		protecting your credentials from other users. However, it&aposs challenging to securely distribute credentials
		to each instance, especially those that AWS creates on your behalf, such as Spot Instances or instances in Auto
		Scaling groups. You must also be able to update the credentials on each instance when you rotate your AWS
		credentials.<br /><br />We designed <a href="#IAM">IAM</a> roles so that your applications can securely make API
		requests from your instances, without requiring you to manage the security credentials that the applications
		use.<br /><br />Instead of creating and distributing your AWS credentials, you can delegate permission to make
		API requests using <a href="#IAM">IAM</a> roles as follows:<br /><br />Create an <a href="#IAM">IAM</a>
		role.<br /><br />Define which accounts or AWS services can assume the role.<br /><br />Define which API actions
		and resources the application can use after assuming the role. Specify the role when you launch your instance,
		or attach the role to an existing instance. Have the application retrieve a set of temporary credentials and use
		them.<br /><br />For example, you can use <a href="#IAM">IAM</a> roles to grant permissions to applications
		running on your instances that need to use a bucket in Amazon <a href="#S3">S3</a>. You can specify permissions
		for <a href="#IAM">IAM</a> roles by creating a policy in JSON format. These are similar to the policies that you
		create for <a href="#IAM">IAM</a> users. If you change a role, the change is propagated to all
		instances.<br /><br />When creating <a href="#IAM">IAM</a> roles, associate least privilege <a
			href="#IAM">IAM</a> policies that restrict access to the specific API calls the application
		requires.<br /><br />References:<br /><br />AWS Identity and Access Management (<a href="#IAM">IAM</a>)
		FAQs<br />Amazon Elastic Compute Cloud > User Guide for Linux Instances > <a href="#IAM">IAM</a> roles for
		Amazon <a href="#EC2">EC2</a><br /><br /></div><a href="#IAM">IAM(5)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a><a id=EC2>
		<h2>EC2</h2>
	</a> - 29 Questions <br><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 3<br />A company captures clickstream data from
		multiple websites and analyzes it using batch processing. The data is loaded nightly into Amazon Redshift and is
		consumed by business analysts. The company wants to move towards near&#8211;real&#8211;time data processing for
		timely insights. The solution should process the streaming data with minimal effort and operational
		overhead.<br /><br />Which combination of AWS services are MOST cost&#8211;effective for this solution? (Choose
		two.)<br /><br />A. Amazon <a href="#EC2">EC2</a><br />B. AWS Lambda<br />C. Amazon Kinesis Data Streams<br />D.
		Amazon Kinesis Data Firehose<br />E. Amazon Kinesis Data Analytics<br /><br /><b>Correct Answer:</b><br />A.
		Amazon <a href="#EC2">EC2</a><br />D. Amazon Kinesis Data Firehose<br /><br />Answer Description:<br />Kinesis
		Data Streams and Kinesis Client Library (KCL) &#8212; Data from the data source can be continuously captured and
		streamed in near real&#8211;time using Kinesis Data Streams. With the Kinesis Client Library (KCL), you can
		build your own application that can preprocess the streaming data as they arrive and emit the data for
		generating incremental views and downstream analysis. Kinesis Data Analytics &#8212; This service provides the
		easiest way to process the data that is streaming through Kinesis Data Stream or Kinesis Data Firehose using
		SQL. This enables customers to gain actionable insight in near real&#8211;time from the incremental stream
		before storing it in Amazon <a href="#S3">S3</a>.<br /><br />Lambda architecture building blocks on
		AWS<br /><br />References:<br /><br />Evolve from batch to real&#8211;time analytics</div><a
		href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 4<br />A company&aposs application runs on Amazon
		<a href="#EC2">EC2</a> instances behind an Application Load Balancer (ALB). The instances run in an Amazon <a
			href="#EC2">EC2</a> Auto Scaling group across multiple Availability Zones. On the first day of every month
		at midnight, the application becomes much slower when the month&#8211;end financial calculation batch executes.
		This causes the CPU utilization of the <a href="#EC2">EC2</a> instances to immediately peak to 100%, which
		disrupts the application.<br /><br />What should a solutions architect recommend to ensure the application is
		able to handle the workload and avoid downtime?<br /><br />A. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution in front of the ALB.<br />B. Configure an <a
			href="#EC2">EC2</a> Auto Scaling simple scaling policy based on CPU utilization.<br />C. Configure an <a
			href="#EC2">EC2</a> Auto Scaling scheduled scaling policy based on the monthly schedule.<br />D. Configure
		Amazon ElastiCache to remove some of the workload from the <a href="#EC2">EC2</a>
		instances.<br /><br /><b>Correct Answer:</b><br />C. Configure an <a href="#EC2">EC2</a> Auto Scaling scheduled
		scaling policy based on the monthly schedule.<br /><br />Answer Description:<br />Scheduled Scaling for Amazon
		<a href="#EC2">EC2</a> Auto Scaling<br />Scheduled scaling allows you to set your own scaling schedule. For
		example, let&aposs say that every week the traffic to your web application starts to increase on Wednesday,
		remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the
		predictable traffic patterns of your web application. Scaling actions are performed automatically as a function
		of time and date.<br /><br />Scheduled scaling allows you to set your own scaling schedule. In this case the
		scaling action can be scheduled to occur just prior to the time that the reports will be run each month. Scaling
		actions are performed automatically as a function of time and date. This will ensure that there are enough <a
			href="#EC2">EC2</a> instances to serve the demand and prevent the application from slowing
		down.<br /><br />CORRECT: &quot;Configure an <a href="#EC2">EC2</a> Auto Scaling scheduled scaling policy based
		on the monthly schedule&quot; is the correct answer.<br /><br />INCORRECT: &quot;Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution in front of the ALB&quot; is incorrect as this would be more
		suitable for providing access to global users by caching content.<br /><br />INCORRECT: &quot;Configure an <a
			href="#EC2">EC2</a> Auto Scaling simple scaling policy based on CPU utilization&quot; is incorrect as this
		would not prevent the slow&#8211;down from occurring as there would be a delay between when the CPU hits 100%
		and the metric being reported and additional instances being launched.<br /><br />INCORRECT: &quot;Configure
		Amazon ElastiCache to remove some of the workload from the <a href="#EC2">EC2</a> instances&quot; is incorrect
		as ElastiCache is a database cache, it cannot replace the compute functions of an <a href="#EC2">EC2</a>
		instance.<br /><br />References:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling > User Guide > Scheduled
		scaling for Amazon <a href="#EC2">EC2</a> Auto Scaling</div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 8<br />A solutions architect is deploying a
		distributed database on multiple Amazon <a href="#EC2">EC2</a> instances. The database stores all data on
		multiple instances so it can withstand the loss of an instance. The database requires block storage with latency
		and throughput to support several million transactions per second per server.<br /><br />Which storage solution
		should the solutions architect use?<br /><br />A. Amazon EBS<br />B. Amazon <a href="#EC2">EC2</a> instance
		store<br />C. Amazon EFS<br />D. Amazon <a href="#S3">S3</a><br /><br /><b>Correct Answer:</b><br />B. Amazon <a
			href="#EC2">EC2</a> instance store<br /><br />Answer Description:<br />It is block storage made for high
		throughput and low latency.<br /><br />References:<br /><br />Amazon Elastic Compute Cloud > User Guide for
		Linux Instances > Amazon <a href="#EC2">EC2</a> instance store</div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 10<br />A start&#8211;up company has a web
		application based in the us&#8211;east&#8211;1 Region with multiple Amazon <a href="#EC2">EC2</a> instances
		running behind an Application Load Balancer across multiple Availability Zones. As the company&aposs user base
		grows in the us&#8211;west&#8211;1 Region, it needs a solution with low latency and high
		availability.<br /><br />What should a solutions architect do to accomplish this?<br /><br />A. Provision <a
			href="#EC2">EC2</a> instances in us&#8211;west&#8211;1. Switch the Application Load Balancer to a Network
		Load Balancer to achieve cross&#8211;Region load balancing.<br />B. Provision <a href="#EC2">EC2</a> instances
		and an Application Load Balancer in us&#8211;west&#8211;1. Make the load balancer distribute the traffic based
		on the location of the request.<br />C. Provision <a href="#EC2">EC2</a> instances and configure an Application
		Load Balancer in us&#8211;west&#8211;1. Create an accelerator in AWS Global Accelerator that uses an endpoint
		group that includes the load balancer endpoints in both Regions.<br />D. Provision <a href="#EC2">EC2</a>
		instances and configure an Application Load Balancer in us&#8211;west&#8211;1. Configure Amazon Route 53 with a
		weighted routing policy. Create alias records in Route 53 that point to the Application Load
		Balancer.<br /><br /><b>Correct Answer:</b><br />C. Provision <a href="#EC2">EC2</a> instances and configure an
		Application Load Balancer in us&#8211;west&#8211;1. Create an accelerator in AWS Global Accelerator that uses an
		endpoint group that includes the load balancer endpoints in both Regions.<br /><br />Answer
		Description:<br />Register endpoints for endpoint groups: You register one or more regional resources, such as
		Application Load Balancers, Network Load Balancers, <a href="#EC2">EC2</a> Instances, or Elastic IP addresses,
		in each endpoint group. Then you can set weights to choose how much traffic is routed to each
		endpoint.<br /><br />Endpoints in AWS Global Accelerator: Endpoints in AWS Global Accelerator can be Network
		Load Balancers, Application Load Balancers, Amazon <a href="#EC2">EC2</a> instances, or Elastic IP addresses. A
		static IP address serves as a single point of contact for clients, and Global Accelerator then distributes
		incoming traffic across healthy endpoints. Global Accelerator directs traffic to endpoints by using the port (or
		port range) that you specify for the listener that the endpoint group for the endpoint belongs
		to.<br /><br />Each endpoint group can have multiple endpoints. You can add each endpoint to multiple endpoint
		groups, but the endpoint groups must be associated with different listeners.<br /><br />Global Accelerator
		continually monitors the health of all endpoints that are included in an endpoint group. It routes traffic only
		to the active endpoints that are healthy. If Global Accelerator doesn&apost have any healthy endpoints to route
		traffic to, it routes traffic to all endpoints.<br /><br />ELB provides load balancing within one Region, AWS
		Global Accelerator provides traffic management across multiple Regions […] AWS Global Accelerator complements
		ELB by extending these capabilities beyond a single AWS Region, allowing you to provision a global interface for
		your applications in any number of Regions. If you have workloads that cater to a global client base, we
		recommend that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by
		clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to
		manage your resources.<br /><br />References:<br /><br />AWS Global Accelerator FAQs<br /></div><a
		href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 18<br />A gaming company has multiple Amazon <a
			href="#EC2">EC2</a> instances in a single Availability Zone for its multiplayer game that communicates with
		users on Layer 4. The chief technology officer (CTO) wants to make the architecture highly available and
		cost&#8211;effective.<br />What should a solutions architect do to meet these requirements? (Choose
		two.)?<br /><br />A. Increase the number of <a href="#EC2">EC2</a> instances.<br />B. Decrease the number of <a
			href="#EC2">EC2</a> instances.<br />C. Configure a Network Load Balancer in front of the <a
			href="#EC2">EC2</a> instances.<br />D. Configure an Application Load Balancer in front of the <a
			href="#EC2">EC2</a> instances.<br />E. Configure an Auto Scaling group to add or remove instances in
		multiple Availability Zones automatically.<br /><br /><b>Correct Answer:</b><br />C. Configure a Network Load
		Balancer in front of the <a href="#EC2">EC2</a> instances.<br />E. Configure an Auto Scaling group to add or
		remove instances in multiple Availability Zones automatically.<br /><br />Answer Description:<br />Network Load
		Balancer overview: A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection
		(OSI) model. It can handle millions of requests per second. After the load balancer receives a connection
		request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to
		the selected target on the port specified in the listener configuration.<br /><br />When you enable an
		Availability Zone for the load balancer, Elastic Load Balancing creates a load balancer node in the Availability
		Zone. By default, each load balancer node distributes traffic across the registered targets in its Availability
		Zone only. If you enable cross&#8211;zone load balancing, each load balancer node distributes traffic across the
		registered targets in all enabled Availability Zones. For more information, see Availability
		Zones.<br /><br />If you enable multiple Availability Zones for your load balancer and ensure that each target
		group has at least one target in each enabled Availability Zone, this increases the fault tolerance of your
		applications. For example, if one or more target groups does not have a healthy target in an Availability Zone,
		we remove the IP address for the corresponding subnet from DNS, but the load balancer nodes in the other
		Availability Zones are still available to route traffic. If a client doesn&apost honor the
		time&#8211;to&#8211;live (TTL) and sends requests to the IP address after it is removed from DNS, the requests
		fail.<br /><br />For TCP traffic, the load balancer selects a target using a flow hash algorithm based on the
		protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number. The
		TCP connections from a client have different source ports and sequence numbers, and can be routed to different
		targets. Each individual TCP connection is routed to a single target for the life of the
		connection.<br /><br />For UDP traffic, the load balancer selects a target using a flow hash algorithm based on
		the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the
		same source and destination, so it is consistently routed to a single target throughout its lifetime. Different
		UDP flows have different source IP addresses and ports, so they can be routed to different
		targets.<br /><br />An Auto Scaling group contains a collection of Amazon <a href="#EC2">EC2</a> instances that
		are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group
		also enables you to use Amazon <a href="#EC2">EC2</a> Auto Scaling features such as health check replacements
		and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling
		are the core functionality of the Amazon <a href="#EC2">EC2</a> Auto Scaling service.<br /><br />The size of an
		Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its
		size to meet demand, either manually or by using automatic scaling.<br /><br />An Auto Scaling group starts by
		launching enough instances to meet its desired capacity. It maintains this number of instances by performing
		periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed
		number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group
		terminates the unhealthy instance and launches another instance to replace it.<br /><br />The solutions
		architect must enable high availability for the architecture and ensure it is cost&#8211; effective. To enable
		high availability an Amazon <a href="#EC2">EC2</a> Auto Scaling group should be created to add and remove
		instances across multiple availability zones.<br /><br />In order to distribute the traffic to the instances the
		architecture should use a Network Load Balancer which operates at Layer 4. This architecture will also be
		cost&#8211;effective as the Auto Scaling group will ensure the right number of instances are running based on
		demand.<br /><br />CORRECT: &quot;Configure a Network Load Balancer in front of the <a href="#EC2">EC2</a>
		instances&quot; is a correct answer.<br /><br />CORRECT: &quot;Configure an Auto Scaling group to add or remove
		instances in multiple Availability Zones automatically&quot; is also a correct answer.<br /><br />INCORRECT:
		&quot;Increase the number of instances and use smaller <a href="#EC2">EC2</a> instance types&quot; is incorrect
		as this is not the most cost&#8211;effective option. Auto Scaling should be used to maintain the right number of
		active instances.<br /><br />INCORRECT: &quot;Configure an Auto Scaling group to add or remove instances in the
		Availability Zone automatically&quot; is incorrect as this is not highly available as it&aposs a single
		AZ.<br /><br />INCORRECT: &quot;Configure an Application Load Balancer in front of the <a href="#EC2">EC2</a>
		instances&quot; is incorrect as an ALB operates at Layer 7 rather than Layer
		4.<br /><br />References:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling > User Guide > Elastic Load
		Balancing and Amazon <a href="#EC2">EC2</a> Auto Scaling<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 24<br />A company is performing an AWS
		Well&#8211;Architected Framework review of an existing workload deployed on AWS. The review identified a
		public&#8211;facing website running on the same Amazon <a href="#EC2">EC2</a> instance as a Microsoft Active
		Directory domain controller that was installed recently to support other AWS services. A solutions architect
		needs to recommend a new design that would improve the security of the architecture and minimize the
		administrative demand on IT staff.<br /><br />What should the solutions architect recommend?<br /><br />A. Use
		AWS Directory Service to create a managed Active Directory. Uninstall Active Directory on the current <a
			href="#EC2">EC2</a> instance.<br />B. Create another <a href="#EC2">EC2</a> instance in the same subnet and
		reinstall Active Directory on it. Uninstall Active Directory.<br />C. Use AWS Directory Service to create an
		Active Directory connector. Proxy Active Directory requests to the Active domain controller running on the
		current <a href="#EC2">EC2</a> instance.<br />D. Enable AWS Single Sign&#8211;On (AWS SSO) with Security
		Assertion Markup Language (SAML) 2.0 federation with the current Active Directory controller. Modify the <a
			href="#EC2">EC2</a> instance&aposs security group to deny public access to Active
		Directory.<br /><br /><b>Correct Answer:</b><br />A. Use AWS Directory Service to create a managed Active
		Directory. Uninstall Active Directory on the current <a href="#EC2">EC2</a> instance.<br /><br />Answer
		Description:<br />AWS Managed Microsoft AD: AWS Directory Service lets you run Microsoft Active Directory (AD)
		as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed
		Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is
		created as a highly available pair of domain controllers connected to your virtual private cloud (<a
			href="#VPC">VPC</a>). The domain controllers run in different Availability Zones in a region of your choice.
		Host monitoring and recovery, data replication, snapshots, and software updates are automatically configured and
		managed for you.<br /><br />Migrate AD to AWS Managed AD and keep the webserver alone. Reduce risk = remove AD
		from that <a href="#EC2">EC2</a>. Minimize admin = remove AD from any <a href="#EC2">EC2</a><br /><br />&#8211;>
		use AWS Directory Service<br /><br />Active Directory connector is only for ON&#8211;PREM AD. The one they have
		exists in the cloud already.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 40<br />An Amazon <a href="#EC2">EC2</a>
		administrator created the following policy associated with an <a href="#IAM">IAM</a> group containing several
		users:<br /><br />An Amazon <a href="#EC2">EC2</a> administrator created the following policy associated with an
		<a href="#IAM">IAM</a> group containing several users.<br /><br />What is the effect of this
		policy?<br /><br />A. Users can terminate an <a href="#EC2">EC2</a> instance in any AWS Region except
		us&#8211;east&#8211;1.<br />B. Users can terminate an <a href="#EC2">EC2</a> instance with the IP address
		10.100.100.1 in the us&#8211;east&#8211;1 Region.<br />C. Users can terminate an <a href="#EC2">EC2</a> instance
		in the us&#8211;east&#8211;1 Region when the user&aposs source IP is 10.100.100.254.<br />D. Users cannot
		terminate an <a href="#EC2">EC2</a> instance in the us&#8211;east&#8211;1 Region when the user&aposs source IP
		is 10.100.100.254.<br /><br /><b>Correct Answer:</b><br />C. Users can terminate an <a href="#EC2">EC2</a>
		instance in the us&#8211;east&#8211;1 Region when the user&aposs source IP is 10.100.100.254.<br /><br />Answer
		Description:<br />What the policy means:<br />1. <a href="#All">All</a>ow termination of any instance if
		user&aposs source IP address is 100.100.254.<br />2. Deny termination of instances that are not in the
		us&#8211;east&#8211;1 Combining this two, you get:<br />&quot;<a href="#All">All</a>ow instance termination in
		the us&#8211;east&#8211;1 region if the user&aposs source IP address is 10.100.100.254. Deny termination
		operation on other regions.&quot;<br /><br /><br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 49<br />A company recently deployed a new auditing
		system to centralize information about operating system versions, patching, and installed software for Amazon <a
			href="#EC2">EC2</a> instances. A solutions architect must ensure all instances provisioned through <a
			href="#EC2">EC2</a> Auto Scaling groups successfully send reports to the auditing system as soon as they are
		launched and terminated.<br /><br />Which solution achieves these goals MOST efficiently?<br /><br />A. Use a
		scheduled AWS Lambda function and execute a script remotely on all <a href="#EC2">EC2</a> instances to send data
		to the audit system.<br />B. Use <a href="#EC2">EC2</a> Auto Scaling lifecycle hooks to execute a custom script
		to send data to the audit system when instances are launched and terminated.<br />C. Use an <a
			href="#EC2">EC2</a> Auto Scaling launch configuration to execute a custom script through user data to send
		data to the audit system when instances are launched and terminated.<br />D. Execute a custom script on the
		instance operating system to send data to the audit system. Configure the script to be executed by the <a
			href="#EC2">EC2</a> Auto Scaling group when the instance starts and is terminated.<br /><br /><b>Correct
			Answer:</b><br />B. Use <a href="#EC2">EC2</a> Auto Scaling lifecycle hooks to execute a custom script to
		send data to the audit system when instances are launched and terminated.<br /></div><a href="#EC2">EC2(29)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 57<br />A solutions architect has created two <a
			href="#IAM">IAM</a> policies: Policy1 and Policy2. Both policies are attached to an <a href="#IAM">IAM</a>
		group.<br /><br />A solutions architect has created two <a href="#IAM">IAM</a> policies: Policy1 and Policy2.
		Both policies are attached to an <a href="#IAM">IAM</a> group.<br /><br />A cloud engineer is added as an <a
			href="#IAM">IAM</a> user to the <a href="#IAM">IAM</a> group. Which action will the cloud engineer be able
		to perform?<br /><br />A. Deleting <a href="#IAM">IAM</a> users<br />B. Deleting directories<br />C. Deleting
		Amazon <a href="#EC2">EC2</a> instances<br />D. Deleting logs from Amazon CloudWatch Logs<br /><br /><b>Correct
			Answer:</b><br />C. Deleting Amazon <a href="#EC2">EC2</a> instances<br /></div><a href="#EC2">EC2(29)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 66<br />A company has two applications it wants to
		migrate to AWS. Both applications process a large set of files by accessing the same files at the same time.
		Both applications need to read the files with low latency.<br /><br />Which architecture should a solutions
		architect recommend for this situation?<br /><br />A. Configure two AWS Lambda functions to run the
		applications. Create an Amazon <a href="#EC2">EC2</a> instance with an instance store volume to store the
		data.<br />B. Configure two AWS Lambda functions to run the applications. Create an Amazon <a
			href="#EC2">EC2</a> instance with an Amazon Elastic Block Store (Amazon EBS) volume to store the
		data.<br />C. Configure one memory optimized Amazon <a href="#EC2">EC2</a> instance to run both applications
		simultaneously. Create an Amazon Elastic Block Store (Amazon EBS) volume with Provisioned IOPS to store the
		data.<br />D. Configure two Amazon <a href="#EC2">EC2</a> instances to run both applications. Configure Amazon
		Elastic File System (Amazon EFS) with General Purpose performance mode and Bursting Throughput mode to store the
		data.<br /><br /><b>Correct Answer:</b><br />D. Configure two Amazon <a href="#EC2">EC2</a> instances to run
		both applications. Configure Amazon Elastic File System (Amazon EFS) with General Purpose performance mode and
		Bursting Throughput mode to store the data.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 78<br />A three&#8211;tier web application
		processes orders from customers. The web tier consists of Amazon <a href="#EC2">EC2</a> instances behind an
		Application Load Balancer, a middle tier of three <a href="#EC2">EC2</a> instances decoupled from the web tier
		using Amazon SQS, and an Amazon DynamoDB backend. At peak times, customers who submit orders using the site have
		to wait much longer than normal to receive confirmations due to lengthy processing times. A solutions architect
		needs to reduce these processing times.<br /><br />Which action will be MOST effective in accomplishing
		this?<br /><br />A. Replace the SQS queue with Amazon Kinesis Data Firehose.<br />B. Use Amazon ElastiCache for
		Redis in front of the DynamoDB backend tier.<br />C. Add an Amazon <a href="#CloudFront">CloudFront</a>
		distribution to cache the responses for the web tier.<br />D. Use Amazon <a href="#EC2">EC2</a> Auto Scaling to
		scale out the middle tier instances based on the SQS queue depth.<br /><br /><b>Correct Answer:</b><br />D. Use
		Amazon <a href="#EC2">EC2</a> Auto Scaling to scale out the middle tier instances based on the SQS queue
		depth.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 99<br />A company is planning to build a new web
		application on AWS. The company expects predictable traffic most of the year and very high traffic on occasion.
		The web application needs to be highly available and fault tolerant with minimal latency.<br /><br />What should
		a solutions architect recommend to meet these requirements?<br /><br />A. Use an Amazon Route 53 routing policy
		to distribute requests to two AWS Regions, each with one Amazon <a href="#EC2">EC2</a> instance.<br />B. Use
		Amazon <a href="#EC2">EC2</a> instances in an Auto Scaling group with an Application Load Balancer across
		multiple Availability Zones.<br />C. Use Amazon <a href="#EC2">EC2</a> instances in a cluster placement group
		with an Application Load Balancer across multiple Availability Zones.<br />D. Use Amazon <a href="#EC2">EC2</a>
		instances in a cluster placement group and include the cluster placement group within a new Auto Scaling
		group.<br /><br /><b>Correct Answer:</b><br />B. Use Amazon <a href="#EC2">EC2</a> instances in an Auto Scaling
		group with an Application Load Balancer across multiple Availability Zones.<br /></div><a
		href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 102<br />A company is running a three&#8211;tier
		web application to process credit card payments. The front&#8211;end user interface consists of static webpages.
		The application tier can have long&#8211;running processes. The database tier uses MySQL.<br /><br />The
		application is currently running on a single, general&#8211;purpose large Amazon <a href="#EC2">EC2</a>
		instance. A solutions architect needs to decouple the services to make the web application highly
		available.<br /><br />Which solution would provide the HIGHEST availability?<br /><br />A. Move static assets to
		Amazon <a href="#CloudFront">CloudFront</a>. Leave the application in <a href="#EC2">EC2</a> in an Auto Scaling
		group. Move the database to Amazon RDS to deploy Multi&#8211;AZ.<br />B. Move static assets and the application
		into a medium <a href="#EC2">EC2</a> instance. Leave the database on the large instance. Place both instances in
		an Auto Scaling group.<br />C. Move static assets to Amazon <a href="#S3">S3</a>, Move the application to AWS
		Lambda with the concurrency limit set. Move the database to Amazon DynamoDB with on&#8211;demand
		enabled.<br />D. Move static assets to Amazon <a href="#S3">S3</a>. Move the application to Amazon Elastic
		Container Service (Amazon <a href="#ECS">ECS</a>) containers with Auto Scaling enabled. Move the database to
		Amazon RDS to deploy Multi&#8211;AZ.<br /><br /><b>Correct Answer:</b><br />B. Move static assets and the
		application into a medium <a href="#EC2">EC2</a> instance. Leave the database on the large instance. Place both
		instances in an Auto Scaling group.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 106<br />A company is hosting multiple websites
		for several lines of business under its registered parent domain.<br /><br />Users accessing these websites will
		be routed to appropriate backend Amazon <a href="#EC2">EC2</a> instances based on the subdomain. The websites
		host static webpages, images, and server&#8211;side scripts like PHP and JavaScript. Some of the websites
		experience peak access during the first two hours of business with constant usage throughout the rest of the
		day. A solutions architect needs to design a solution that will automatically adjust capacity to these traffic
		patterns while keeping costs low.<br /><br />Which combination of AWS services or features will meet these
		requirements? (Choose two.)<br /><br />A. AWS Batch<br />B. Network Load Balancer<br />C. Application Load
		Balancer<br />D. Amazon <a href="#EC2">EC2</a> Auto Scaling<br />E. Amazon <a href="#S3">S3</a> website
		hosting<br /><br /><b>Correct Answer:</b><br />C. Application Load Balancer<br />D. Amazon <a
			href="#EC2">EC2</a> Auto Scaling<br /><br />References:<br /><br />Amazon Simple Storage Service > User
		Guide > Hosting a static website using Amazon <a href="#S3">S3</a></div><a href="#EC2">EC2(29)</a> <a href="#"
		<i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 109<br />A company hosts its website on AWS. To
		address the highly variable demand, the company has implemented Amazon <a href="#EC2">EC2</a> Auto
		Scaling.<br /><br />Management is concerned that the company is over&#8211;provisioning its infrastructure,
		especially at the front end of the three&#8211;tier application. A solutions architect needs to ensure costs are
		optimized without impacting performance.<br /><br />What should the solutions architect do to accomplish
		this?<br /><br />A. Use Auto Scaling with Reserved Instances.<br />B. Use Auto Scaling with a scheduled scaling
		policy.<br />C. Use Auto Scaling with the suspend&#8211;resume feature.<br />D. Use Auto Scaling with a target
		tracking scaling policy.<br /><br /><b>Correct Answer:</b><br />D. Use Auto Scaling with a target tracking
		scaling policy.<br /><br />References:<br /><br />Amazon <a href="#EC2">EC2</a> Auto Scaling > User Guid >
		Target tracking scaling policies for Amazon <a href="#EC2">EC2</a> Auto Scaling</div><a href="#EC2">EC2(29)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 121<br />A company currently has 250 TB of backup
		files stored in Amazon <a href="#S3">S3</a> in a vendor&aposs proprietary format.<br /><br />Using a
		Linux&#8211;based software application provided by the vendor, the company wants to retrieve files from Amazon
		<a href="#S3">S3</a>, transform the files to an industry&#8211;standard format, and re&#8211;upload them to
		Amazon <a href="#S3">S3</a>. The company wants to minimize the data transfer charges associated with this
		conversation.<br /><br />What should a solutions architect do to accomplish this?<br /><br />A. Install the
		conversion software as an Amazon <a href="#S3">S3</a> batch operation so the data is transformed without leaving
		Amazon <a href="#S3">S3</a>.<br />B. Install the conversion software onto an on&#8211;premises virtual machine.
		Perform the transformation and reupload the files to Amazon <a href="#S3">S3</a> from the virtual
		machine.<br />C. Use AWS Snowball Edge devices to export the data and install the conversion software onto the
		devices. Perform the data transformation and re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the
		Snowball Edge devices.<br />D. Launch an Amazon <a href="#EC2">EC2</a> instance in the same Region as Amazon <a
			href="#S3">S3</a> and install the conversion software onto the instance. Perform the transformation and
		re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the <a href="#EC2">EC2</a>
		instance.<br /><br /><b>Correct Answer:</b><br />D. Launch an Amazon <a href="#EC2">EC2</a> instance in the same
		Region as Amazon <a href="#S3">S3</a> and install the conversion software onto the instance. Perform the
		transformation and re&#8211;upload the files to Amazon <a href="#S3">S3</a> from the <a href="#EC2">EC2</a>
		instance.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 125<br />A solutions architect is creating an
		application that will handle batch processing of large amounts of data.<br /><br />The input data will be held
		in Amazon <a href="#S3">S3</a> and the output data will be stored in a different <a href="#S3">S3</a> bucket.
		For processing, the application will transfer the data over the network between multiple Amazon <a
			href="#EC2">EC2</a> instances.<br /><br />What should the solutions architect do to reduce the overall data
		transfer costs?<br /><br />A. Place all the <a href="#EC2">EC2</a> instances in an Auto Scaling group.<br />B.
		Place all the <a href="#EC2">EC2</a> instances in the same AWS Region.<br />C. Place all the <a
			href="#EC2">EC2</a> instances in the same Availability Zone.<br />D. Place all the <a href="#EC2">EC2</a>
		instances in private subnets in multiple Availability Zones.<br /><br /><b>Correct Answer:</b><br />C. Place all
		the <a href="#EC2">EC2</a> instances in the same Availability Zone.<br /><br />Answer Description:<br />The
		transfer is between <a href="#EC2">EC2</a> instances and not just between <a href="#S3">S3</a> and <a
			href="#EC2">EC2</a>.<br /><br />Also, be aware of inter&#8211;Availability Zones data transfer charges
		between Amazon <a href="#EC2">EC2</a> instances, even within the same region. If possible, the instances in a
		development or test environment that need to communicate with each other should be co&#8211;located within the
		same Availability Zone to avoid data transfer charges. (This doesn&apost apply to production workloads which
		will most likely need to span multiple Availability Zones for high
		availability.)<br /><br />References:<br /><br />AWS Management & Governance Blog > Using AWS Cost Explorer to
		analyze data transfer costs<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 135<br />An application running on an Amazon <a
			href="#EC2">EC2</a> instance needs to access an Amazon DynamoDB table. Both the <a href="#EC2">EC2</a>
		instance and the DynamoDB table are in the same AWS account. A solutions architect must configure the necessary
		permissions.<br /><br />Which solution will allow least privilege access to the DynamoDB table from the <a
			href="#EC2">EC2</a> instance?<br /><br />A. Create an <a href="#IAM">IAM</a> role with the appropriate
		policy to allow access to the DynamoDB table. Create an instance profile to assign this <a href="#IAM">IAM</a>
		role to the <a href="#EC2">EC2</a> instance.<br />B. Create an <a href="#IAM">IAM</a> role with the appropriate
		policy to allow access to the DynamoDB table. Add the <a href="#EC2">EC2</a> instance to the trust relationship
		policy document to allow it to assume the role.<br />C. Create an <a href="#IAM">IAM</a> user with the
		appropriate policy to allow access to the DynamoDB table. Store the credentials in an Amazon <a
			href="#S3">S3</a> bucket and read them from within the application code directly.<br />D. Create an <a
			href="#IAM">IAM</a> user with the appropriate policy to allow access to the DynamoDB table. Ensure that the
		application stores the <a href="#IAM">IAM</a> credentials securely on local storage and uses them to make the
		DynamoDB calls.<br /><br /><b>Correct Answer:</b><br />A. Create an <a href="#IAM">IAM</a> role with the
		appropriate policy to allow access to the DynamoDB table. Create an instance profile to assign this <a
			href="#IAM">IAM</a> role to the <a href="#EC2">EC2</a> instance.<br /></div><a href="#EC2">EC2(29)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 139<br />A company receives inconsistent service
		from its data center provider because the company is headquartered in an area affected by natural disasters. The
		company is not ready to fully migrate to the AWS Cloud, but it wants a failure environment on AWS in case the
		on&#8211;premises data center fails.<br /><br />The company runs web servers that connect to external vendors.
		The data available on AWS and on&#8211;premises must be uniform.<br /><br />Which solution should a solutions
		architect recommend that has the LEAST amount of downtime?<br /><br />A. Configure an Amazon Route 53 failover
		record. Run application servers on Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer
		in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon <a
			href="#S3">S3</a>.<br />B. Configure an Amazon Route 53 failover record. Execute an AWS CloudFormation
		template from a script to create Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer.
		Set up AWS Storage Gateway with stored volumes to back up data to Amazon <a href="#S3">S3</a>.<br />C. Configure
		an Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a <a href="#VPC">VPC</a> and
		the data center. Run application servers on Amazon <a href="#EC2">EC2</a> in an Auto Scaling group. Run an AWS
		Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer.<br />D.
		Configure an Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation
		template to launch two Amazon <a href="#EC2">EC2</a> instances. Set up AWS Storage Gateway with stored volumes
		to back up data to Amazon <a href="#S3">S3</a>. Set up an AWS Direct Connect connection between a <a
			href="#VPC">VPC</a> and the data center.<br /><br /><b>Correct Answer:</b><br />A. Configure an Amazon Route
		53 failover record. Run application servers on Amazon <a href="#EC2">EC2</a> instances behind an Application
		Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon
		<a href="#S3">S3</a>.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 142<br />A company runs a high performance
		computing (HPC) workload on AWS. The workload required low latency network performance and high network
		throughput with tightly coupled node&#8211;to&#8211;node communication. The Amazon <a href="#EC2">EC2</a>
		instances are properly sized for compute and storage capacity, and are launched using default
		options.<br /><br />What should a solutions architect propose to improve the performance of the
		workload?<br /><br />A. Choose a cluster placement group while launching Amazon <a href="#EC2">EC2</a>
		instances.<br />B. Choose dedicated instance tenancy while launching Amazon <a href="#EC2">EC2</a>
		instances.<br />C. Choose an Elastic Inference accelerator while launching Amazon <a href="#EC2">EC2</a>
		instances.<br />D. Choose the required capacity reservation while launching Amazon <a href="#EC2">EC2</a>
		instances.<br /><br /><b>Correct Answer:</b><br />A. Choose a cluster placement group while launching Amazon <a
			href="#EC2">EC2</a> instances.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 143<br />A solutions architect is designing a high
		performance computing (HPC) workload on Amazon <a href="#EC2">EC2</a>. The <a href="#EC2">EC2</a> instances need
		to communicate to each other frequently and require network performance with low latency and high
		throughput.<br /><br />Which <a href="#EC2">EC2</a> configuration meets these requirements?<br /><br />A. Launch
		the <a href="#EC2">EC2</a> instances in a cluster placement group in one Availability Zone.<br />B. Launch the
		<a href="#EC2">EC2</a> instances in a spread placement group in one Availability Zone.<br />C. Launch the <a
			href="#EC2">EC2</a> instances in an Auto Scaling group in two Regions and peer the <a
			href="#VPC">VPC</a>s.<br />D. Launch the <a href="#EC2">EC2</a> instances in an Auto Scaling group spanning
		multiple Availability Zones.<br /><br /><b>Correct Answer:</b><br />A. Launch the <a href="#EC2">EC2</a>
		instances in a cluster placement group in one Availability Zone.<br /><br />Answer Description:<br />When you
		launch a new <a href="#EC2">EC2</a> instance, the <a href="#EC2">EC2</a> service attempts to place the instance
		in such a way that all of your instances are spread out across underlying hardware to minimize correlated
		failures. You can use placement groups to influence the placement of a group of interdependent instances to meet
		the needs of your workload.<br /><br />Depending on the type of workload, you can create a placement group using
		one of the following placement strategies:<br /><br />Cluster • packs instances close together inside an
		Availability Zone. This strategy enables workloads to achieve the low&#8211;latency network performance
		necessary for tightly&#8211;coupled node&#8211;to&#8211;node communication that is typical of HPC
		applications.<br /><br />Partition • spreads your instances across logical partitions such that groups of
		instances in one partition do not share the underlying hardware with groups of instances in different
		partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop,
		Cassandra, and Kafka.<br /><br />Spread • strictly places a small group of instances across distinct underlying
		hardware to reduce correlated failures.<br /><br />For this scenario, a cluster placement group should be used
		as this is the best option for providing low&#8211;latency network performance for a HPC
		application.<br /><br />CORRECT: &quot;Launch the <a href="#EC2">EC2</a> instances in a cluster placement group
		in one Availability Zone&quot; is the correct answer.<br /><br />INCORRECT: &quot;Launch the <a
			href="#EC2">EC2</a> instances in a spread placement group in one Availability Zone&quot; is incorrect as the
		spread placement group is used to spread instances across distinct underlying hardware.<br /><br />INCORRECT:
		&quot;Launch the <a href="#EC2">EC2</a> instances in an Auto Scaling group in two Regions. Place a Network Load
		Balancer in front of the instances&quot; is incorrect as this does not achieve the stated requirement to provide
		low&#8211;latency, high throughput network performance between instances. Also, you cannot use an ELB across
		Regions.<br /><br />INCORRECT: &quot;Launch the <a href="#EC2">EC2</a> instances in an Auto Scaling group
		spanning multiple Availability Zones&quot; is incorrect as this does not reduce network latency or improve
		performance.<br /><br />References:<br /><br />Amazon Elastic Compute Cloud > User Guide for Linux Instances >
		Placement groups</div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 144<br />A company&aposs application is running on
		Amazon <a href="#EC2">EC2</a> instances in a single Region. In the event of a disaster, a solutions architect
		needs to ensure that the resources can also be deployed to a second Region.<br /><br />Which combination of
		actions should the solutions architect take to accomplish this? (Choose two.)<br /><br />A. Detach a volume on
		an <a href="#EC2">EC2</a> instance and copy it to Amazon <a href="#S3">S3</a>.<br />B. Launch a new <a
			href="#EC2">EC2</a> instance from an Amazon Machine Image (AMI) in a new Region.<br />C. Launch a new <a
			href="#EC2">EC2</a> instance in a new Region and copy a volume from Amazon <a href="#S3">S3</a> to the new
		instance.<br />D. Copy an Amazon Machine Image (AMI) of an <a href="#EC2">EC2</a> instance and specify a
		different Region for the destination.<br />E. Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon
		<a href="#S3">S3</a> and launch an <a href="#EC2">EC2</a> instance in the destination Region using that EBS
		volume.<br /><br /><b>Correct Answer:</b><br />B. Launch a new <a href="#EC2">EC2</a> instance from an Amazon
		Machine Image (AMI) in a new Region.<br />D. Copy an Amazon Machine Image (AMI) of an <a href="#EC2">EC2</a>
		instance and specify a different Region for the destination.<br /><br />Answer Description:<br />Cross Region <a
			href="#EC2">EC2</a> AMI Copy<br />We know that you want to build applications that span AWS Regions and
		we&aposre working to provide you with the services and features needed to do so. We started out by launching the
		EBS Snapshot Copy feature late last year. This feature gave you the ability to copy a snapshot from Region to
		Region with just a couple of clicks. In addition, last month we made a significant reduction (26% to 83%) in the
		cost of transferring data between AWS Regions, making it less expensive to operate in more than one AWS
		region.<br /><br />Today we are introducing a new feature: Amazon Machine Image (AMI) Copy. AMI Copy enables you
		to easily copy your Amazon Machine Images between AWS Regions. AMI Copy helps enable several key scenarios
		including:<br /><br />Simple and Consistent Multi&#8211;Region Deployment &#8212; You can copy an AMI from one
		region to another, enabling you to easily launch consistent instances based on the same AMI into different
		regions.<br /><br />Scalability &#8212; You can more easily design and build world&#8211;scale applications that
		meet the needs of your users, regardless of their location.<br /><br />Performance &#8212; You can increase
		performance by distributing your application and locating critical components of your application in closer
		proximity to your users. You can also take advantage of region specific features such as instance types or other
		AWS services.<br /><br />Even Higher Availability &#8212; You can design and deploy applications across AWS
		regions, to increase availability. Once the new AMI is in an Available state the copy is
		complete.<br /><br />Once the new AMI is in an Available state the copy is complete.<br /></div><a
		href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 151<br />A company has a two&#8211;tier
		application architecture that runs in public and private subnets. Amazon <a href="#EC2">EC2</a> instances
		running the web application are in the public subnet and a database runs on the private subnet.<br /><br />The
		web application instances and the database are running in a single Availability Zone (AZ).<br /><br />Which
		combination of steps should a solutions architect take to provide high availability for this architecture?
		(Choose two.)<br /><br />A. Create new public and private subnets in the same AZ for high availability.<br />B.
		Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group and Application Load Balancer spanning multiple
		AZs.<br />C. Add the existing web application instances to an Auto Scaling group behind an Application Load
		Balancer.<br />D. Create new public and private subnets in a new AZ. Create a database using Amazon <a
			href="#EC2">EC2</a> in one AZ.<br />E. Create new public and private subnets in the same <a
			href="#VPC">VPC</a>, each in a new AZ. Migrate the database to an Amazon RDS multi&#8211;AZ
		deployment.<br /><br /><b>Correct Answer:</b><br />B. Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group
		and Application Load Balancer spanning multiple AZs.<br />E. Create new public and private subnets in the same
		<a href="#VPC">VPC</a>, each in a new AZ. Migrate the database to an Amazon RDS multi&#8211;AZ
		deployment.<br /><br />Answer Description:<br /><br />You would the <a href="#EC2">EC2</a> instances to have
		high availability by placing them in multiple AZs.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 152<br />A marketing company is storing CSV files
		in an Amazon <a href="#S3">S3</a> bucket for statistical analysis. An application on an Amazon <a
			href="#EC2">EC2</a> instance needs permission to efficiently process the CSV data stored in the <a
			href="#S3">S3</a> bucket.<br /><br />Which action will MOST securely grant the <a href="#EC2">EC2</a>
		instance access to the <a href="#S3">S3</a> bucket?<br /><br />A. Attach a resource&#8211;based policy to the <a
			href="#S3">S3</a> bucket.<br />B. Create an <a href="#IAM">IAM</a> user for the application with specific
		permissions to the <a href="#S3">S3</a> bucket.<br />C. Associate an <a href="#IAM">IAM</a> role with least
		privilege permissions to the <a href="#EC2">EC2</a> instance profile.<br />D. Store AWS credentials directly on
		the <a href="#EC2">EC2</a> instance for applications on the instance to use for API calls.<br /><br /><b>Correct
			Answer:</b><br />C. Associate an <a href="#IAM">IAM</a> role with least privilege permissions to the <a
			href="#EC2">EC2</a> instance profile.<br /><br />Answer Description:<br />Keyword: Privilege Permission + <a
			href="#IAM">IAM</a> Role<br /><br />AWS Identity and Access Management (<a href="#IAM">IAM</a>) enables you
		to manage access to AWS services and resources securely. Using <a href="#IAM">IAM</a>, you can create and manage
		AWS users and groups, and use permissions to allow and deny their access to AWS resources.<br /><br /><a
			href="#IAM">IAM</a> is a feature of your AWS account offered at no additional charge. You will be charged
		only for use of other AWS services by your users.<br /><br /><a href="#IAM">IAM</a> roles for Amazon <a
			href="#EC2">EC2</a><br />Applications must sign their API requests with AWS credentials. Therefore, if you
		are an application developer, you need a strategy for managing credentials for your applications that run on <a
			href="#EC2">EC2</a> instances. For example, you can securely distribute your AWS credentials to the
		instances, enabling the applications on those instances to use your credentials to sign requests, while
		protecting your credentials from other users. However, it&aposs challenging to securely distribute credentials
		to each instance, especially those that AWS creates on your behalf, such as Spot Instances or instances in Auto
		Scaling groups. You must also be able to update the credentials on each instance when you rotate your AWS
		credentials.<br /><br />We designed <a href="#IAM">IAM</a> roles so that your applications can securely make API
		requests from your instances, without requiring you to manage the security credentials that the applications
		use.<br /><br />Instead of creating and distributing your AWS credentials, you can delegate permission to make
		API requests using <a href="#IAM">IAM</a> roles as follows:<br /><br />Create an <a href="#IAM">IAM</a>
		role.<br /><br />Define which accounts or AWS services can assume the role.<br /><br />Define which API actions
		and resources the application can use after assuming the role. Specify the role when you launch your instance,
		or attach the role to an existing instance. Have the application retrieve a set of temporary credentials and use
		them.<br /><br />For example, you can use <a href="#IAM">IAM</a> roles to grant permissions to applications
		running on your instances that need to use a bucket in Amazon <a href="#S3">S3</a>. You can specify permissions
		for <a href="#IAM">IAM</a> roles by creating a policy in JSON format. These are similar to the policies that you
		create for <a href="#IAM">IAM</a> users. If you change a role, the change is propagated to all
		instances.<br /><br />When creating <a href="#IAM">IAM</a> roles, associate least privilege <a
			href="#IAM">IAM</a> policies that restrict access to the specific API calls the application
		requires.<br /><br />References:<br /><br />AWS Identity and Access Management (<a href="#IAM">IAM</a>)
		FAQs<br />Amazon Elastic Compute Cloud > User Guide for Linux Instances > <a href="#IAM">IAM</a> roles for
		Amazon <a href="#EC2">EC2</a><br /><br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 161<br />A solutions architect is moving the
		static content from a public website hosted on Amazon <a href="#EC2">EC2</a> instances to an Amazon <a
			href="#S3">S3</a> bucket. An Amazon <a href="#CloudFront">CloudFront</a> distribution will be used to
		deliver the static assets. The security group used by the <a href="#EC2">EC2</a> instances restricts access to a
		limited set of IP ranges. Access to the static content should be similarly restricted.<br /><br />Which
		combination of steps will meet these requirements? (Choose two.)<br /><br />A. Create an origin access identity
		(OAI) and associate it with the distribution. Change the permissions in the bucket policy so that only the OAI
		can read the objects.<br />B. Create an AWS WAF web ACL that includes the same IP restrictions that exist in the
		<a href="#EC2">EC2</a> security group. Associate this new web ACL with the <a href="#CloudFront">CloudFront</a>
		distribution.<br />C. Create a new security group that includes the same IP restrictions that exist in the
		current <a href="#EC2">EC2</a> security group. Associate this new security group with the <a
			href="#CloudFront">CloudFront</a> distribution.<br />D. Create a new security group that includes the same
		IP restrictions that exist in the current <a href="#EC2">EC2</a> security group. Associate this new security
		group with the <a href="#S3">S3</a> bucket hosting the static content.<br />E. Create a new <a
			href="#IAM">IAM</a> role and associate the role with the distribution. Change the permissions either on the
		<a href="#S3">S3</a> bucket or on the files within the <a href="#S3">S3</a> bucket so that only the newly
		created <a href="#IAM">IAM</a> role has read and download permissions.<br /><br /><b>Correct Answer:</b><br />A.
		Create an origin access identity (OAI) and associate it with the distribution. Change the permissions in the
		bucket policy so that only the OAI can read the objects.<br />B. Create an AWS WAF web ACL that includes the
		same IP restrictions that exist in the <a href="#EC2">EC2</a> security group. Associate this new web ACL with
		the <a href="#CloudFront">CloudFront</a> distribution.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 164<br />A company relies on an application that
		needs at least 4 Amazon <a href="#EC2">EC2</a> instances during regular traffic and must scale up to 12 <a
			href="#EC2">EC2</a> instances during peak loads. The application is critical to the business and must be
		highly available.<br /><br />Which solution will meet these requirements?<br /><br />A. Deploy the <a
			href="#EC2">EC2</a> instances in an Auto Scaling group. Set the minimum to 4 and the maximum to 12, with 2
		in Availability Zone A and 2 in Availability Zone B.<br />B. Deploy the <a href="#EC2">EC2</a> instances in an
		Auto Scaling group. Set the minimum to 4 and the maximum to 12, with all 4 in Availability Zone A.<br />C.
		Deploy the <a href="#EC2">EC2</a> instances in an Auto Scaling group. Set the minimum to 8 and the maximum to
		12, with 4 in Availability Zone A and 4 in Availability Zone B.<br />D. Deploy the <a href="#EC2">EC2</a>
		instances in an Auto Scaling group. Set the minimum to 8 and the maximum to 12, with all 8 in Availability Zone
		A.<br /><br /><b>Correct Answer:</b><br />C. Deploy the <a href="#EC2">EC2</a> instances in an Auto Scaling
		group. Set the minimum to 8 and the maximum to 12, with 4 in Availability Zone A and 4 in Availability Zone
		B.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 165<br />A solutions architect must design a
		solution for a persistent database that is being migrated from on&#8211;premises to AWS. The database requires
		64,000 IOPS according to the database administrator. If possible, the database administrator wants to use a
		single Amazon Elastic Block Store (Amazon EBS) volume to host the database instance.<br /><br />Which solution
		effectively meets the database administrator&aposs criteria?<br /><br />A. Use an instance from the I3 I/O
		optimized family and leverage local ephemeral storage to achieve the IOPS requirement.<br />B. Create an
		Nitro&#8211;based Amazon <a href="#EC2">EC2</a> instance with an Amazon EBS Provisioned IOPS SSD (io1) volume
		attached. Configure the volume to have 64,000 IOPS.<br />C. Create and map an Amazon Elastic File System (Amazon
		EFS) volume to the database instance and use the volume to achieve the required IOPS for the database.<br />D.
		Provision two volumes and assign 32,000 IOPS to each. Create a logical volume at the operating system level that
		aggregates both volumes to achieve the IOPS requirements.<br /><br /><b>Correct Answer:</b><br />B. Create an
		Nitro&#8211;based Amazon <a href="#EC2">EC2</a> instance with an Amazon EBS Provisioned IOPS SSD (io1) volume
		attached. Configure the volume to have 64,000 IOPS.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 169<br />A monolithic application was recently
		migrated to AWS and is now running on a single Amazon <a href="#EC2">EC2</a> instance. Due to application
		limitations, it is not possible to use automatic scaling to scale out the application. The chief technology
		officer (CTO) wants an automated solution to restore the <a href="#EC2">EC2</a> instance in the unlikely event
		the underlying hardware fails.<br /><br />What would allow for automatic recovery of the <a href="#EC2">EC2</a>
		instance as quickly as possible?<br /><br />A. Configure an Amazon CloudWatch alarm that triggers the recovery
		of the <a href="#EC2">EC2</a> instance if it becomes impaired.<br />B. Configure an Amazon CloudWatch alarm to
		trigger an SNS message that alerts the CTO when the <a href="#EC2">EC2</a> instance is impaired.<br />C.
		Configure AWS CloudTrail to monitor the health of the <a href="#EC2">EC2</a> instance, and if it becomes
		impaired, trigger instance recovery.<br />D. Configure an Amazon EventBridge event to trigger an AWS Lambda
		function once an hour that checks the health of the <a href="#EC2">EC2</a> instance and triggers instance
		recovery if the <a href="#EC2">EC2</a> instance is unhealthy.<br /><br /><b>Correct Answer:</b><br />A.
		Configure an Amazon CloudWatch alarm that triggers the recovery of the <a href="#EC2">EC2</a> instance if it
		becomes impaired.<br /><br />References:<br /><br />Amazon Elastic Compute Cloud > User Guide for Linux
		Instances > Recover your instance</div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 194<br />A company runs multiple Amazon <a
			href="#EC2">EC2</a> Linux instances in a <a href="#VPC">VPC</a> with applications that use a hierarchical
		directory structure. The applications need to rapidly and concurrently read and write to shared
		storage.<br /><br />How can this be achieved?<br /><br />A. Create an Amazon EFS file system and mount it from
		each <a href="#EC2">EC2</a> instance.<br />B. Create an Amazon <a href="#S3">S3</a> bucket and permit access
		from all the <a href="#EC2">EC2</a> instances in the <a href="#VPC">VPC</a>.<br />C. Create a file system on an
		Amazon EBS Provisioned IOPS SSD (io1) volume. Attach the volume to all the <a href="#EC2">EC2</a>
		instances.<br />D. Create file systems on Amazon EBS volumes attached to each <a href="#EC2">EC2</a> instance.
		Synchronize the Amazon EBS volumes across the different <a href="#EC2">EC2</a> instances.<br /><br /><b>Correct
			Answer:</b><br />A. Create an Amazon EFS file system and mount it from each <a href="#EC2">EC2</a>
		instance.<br /></div><a href="#EC2">EC2(29)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a><a id=VPC>
		<h2>VPC</h2>
	</a> - 8 Questions <br><a href="#VPC">VPC(8)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 47<br />An application runs on Amazon <a
			href="#EC2">EC2</a> instances in private subnets. The application needs to access an Amazon DynamoDB table.
		What is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS
		network?<br /><br />A. Use a <a href="#VPC">VPC</a> endpoint for DynamoDB.<br />B. Use a NAT gateway in a public
		subnet.<br />C. Use a NAT instance in a private subnet.<br />D. Use the internet gateway attached to the <a
			href="#VPC">VPC</a>.<br /><br /><b>Correct Answer:</b><br />A. Use a <a href="#VPC">VPC</a> endpoint for
		DynamoDB.<br /><br />Answer Description:<br />An Interface endpoint uses AWS PrivateLink and is an elastic
		network interface (ENI) with a private IP address that serves as an entry point for traffic destined to a
		supported service.<br /><br />Using PrivateLink you can connect your <a href="#VPC">VPC</a> to supported AWS
		services, services hosted by other AWS accounts (<a href="#VPC">VPC</a> endpoint services), and supported AWS
		Marketplace partner services.<br /><br />AWS PrivateLink access over Inter&#8211;Region <a href="#VPC">VPC</a>
		Peering:<br /><br />Applications in an AWS <a href="#VPC">VPC</a> can securely access AWS PrivateLink endpoints
		across AWS Regions using Inter&#8211;Region <a href="#VPC">VPC</a> Peering.<br /><br />AWS PrivateLink allows
		you to privately access services hosted on AWS in a highly available and scalable manner, without using public
		IPs, and without requiring the traffic to traverse the Internet.<br /><br />Customers can privately connect to a
		service even if the service endpoint resides in a different AWS Region.<br /><br />Traffic using
		Inter&#8211;Region <a href="#VPC">VPC</a> Peering stays on the global AWS backbone and never traverses the
		public Internet.<br /><br />A gateway endpoint is a gateway that is a target for a specified route in your route
		table, used for traffic destined to a supported AWS service.<br /><br />An interface <a href="#VPC">VPC</a>
		endpoint (interface endpoint) enables you to connect to services powered by AWS
		PrivateLink.<br /><br />References:<br />Amazon DynamoDB > Developer Guide > What Is Amazon DynamoDB?<br />
	</div><a href="#VPC">VPC(8)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 56<br />A company&aposs web application is running
		on Amazon <a href="#EC2">EC2</a> instances behind an Application Load Balancer.<br /><br />The company recently
		changed its policy, which now requires the application to be accessed from one specific country
		only.<br /><br />Which configuration will meet this requirement?<br /><br />A. Configure the security group for
		the <a href="#EC2">EC2</a> instances.<br />B. Configure the security group on the Application Load
		Balancer.<br />C. Configure AWS WAF on the Application Load Balancer in a <a href="#VPC">VPC</a>.<br />D.
		Configure the network ACL for the subnet that contains the <a href="#EC2">EC2</a>
		instances.<br /><br /><b>Correct Answer:</b><br />C. Configure AWS WAF on the Application Load Balancer in a <a
			href="#VPC">VPC</a>.<br /><br />References:<br /><br />AWS Security Blog > How to use AWS WAF to filter
		incoming traffic from embargoed countries</div><a href="#VPC">VPC(8)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 64<br />An application running on an Amazon <a
			href="#EC2">EC2</a> instance in <a href="#VPC">VPC</a>&#8211;A needs to access files in another <a
			href="#EC2">EC2</a> instance in <a href="#VPC">VPC</a>&#8211;B. Both are in separate. AWS accounts. The
		network administrator needs to design a solution to enable secure access to <a href="#EC2">EC2</a> instance in
		<a href="#VPC">VPC</a>&#8211;B from <a href="#VPC">VPC</a>&#8211;A. The connectivity should not have a single
		point of failure or bandwidth concerns.<br /><br />Which solution will meet these requirements?<br /><br />A.
		Set up a <a href="#VPC">VPC</a> peering connection between <a href="#VPC">VPC</a>&#8211;A and <a
			href="#VPC">VPC</a>&#8211;B.<br />B. Set up <a href="#VPC">VPC</a> gateway endpoints for the <a
			href="#EC2">EC2</a> instance running in <a href="#VPC">VPC</a>&#8211;B.<br />C. Attach a virtual private
		gateway to <a href="#VPC">VPC</a>&#8211;B and enable routing from <a href="#VPC">VPC</a>&#8211;A.<br />D. Create
		a private virtual interface (VIF) for the <a href="#EC2">EC2</a> instance running in <a
			href="#VPC">VPC</a>&#8211;B and add appropriate routes from <a
			href="#VPC">VPC</a>&#8211;B.<br /><br /><b>Correct Answer:</b><br />A. Set up a <a href="#VPC">VPC</a>
		peering connection between <a href="#VPC">VPC</a>&#8211;A and <a href="#VPC">VPC</a>&#8211;B.<br /><br />Answer
		Description:<br />A <a href="#VPC">VPC</a> peering connection is a networking connection between two <a
			href="#VPC">VPC</a>s that enables you to route traffic between them using private IPv4 addresses or IPv6
		addresses. Instances in either <a href="#VPC">VPC</a> can communicate with each other as if they are within the
		same network. You can create a <a href="#VPC">VPC</a> peering connection between your own <a
			href="#VPC">VPC</a>s, or with a <a href="#VPC">VPC</a> in another AWS account.<br /><br />The traffic
		remains in the private IP space. <a href="#All">All</a> inter&#8211;region traffic is encrypted with no single
		point of failure, or bandwidth bottleneck.<br /><br />References:<br />Amazon Virtual Private Cloud > <a
			href="#VPC">VPC</a> Peering > What is <a href="#VPC">VPC</a> peering?<br /></div><a href="#VPC">VPC(8)</a>
	<a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 65<br />A company&aposs application hosted on
		Amazon <a href="#EC2">EC2</a> instances needs to access an Amazon <a href="#S3">S3</a> bucket. Due to data
		sensitivity, traffic cannot traverse the internet.<br /><br />How should a solutions architect configure
		access?<br /><br />A. Create a private hosted zone using Amazon Route 53.<br />B. Configure a <a
			href="#VPC">VPC</a> gateway endpoint for Amazon <a href="#S3">S3</a> in the <a href="#VPC">VPC</a>.<br />C.
		Configure AWS PrivateLink between the <a href="#EC2">EC2</a> instance and the <a href="#S3">S3</a>
		bucket.<br />D. Set up a site&#8211;to&#8211;site VPN connection between the <a href="#VPC">VPC</a> and the <a
			href="#S3">S3</a> bucket.<br /><br /><b>Correct Answer:</b><br />B. Configure a <a href="#VPC">VPC</a>
		gateway endpoint for Amazon <a href="#S3">S3</a> in the <a href="#VPC">VPC</a>.<br /></div><a
		href="#VPC">VPC(8)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 75<br />An application is running on Amazon <a
			href="#EC2">EC2</a> instances. Sensitive information required for the application is stored in an Amazon <a
			href="#S3">S3</a> bucket. The bucket needs to be protected from internet access while only allowing services
		within the <a href="#VPC">VPC</a> access to the bucket.<br /><br />Which combination of actions should solutions
		archived take to accomplish this? (Choose two.)<br /><br />A. Create a <a href="#VPC">VPC</a> endpoint for
		Amazon <a href="#S3">S3</a>.<br />B. Enable server access logging on the bucket.<br />C. Apply a bucket policy
		to restrict access to the <a href="#S3">S3</a> endpoint.<br />D. Add an <a href="#S3">S3</a> ACL to the bucket
		that has sensitive information.<br />E. Restrict users using the <a href="#IAM">IAM</a> policy to use the
		specific bucket.<br /><br /><b>Correct Answer:</b><br />A. Create a <a href="#VPC">VPC</a> endpoint for Amazon
		<a href="#S3">S3</a>.<br />C. Apply a bucket policy to restrict access to the <a href="#S3">S3</a>
		endpoint.<br /><br />Answer Description:<br />ACL is a property at object level not at bucket level. Also by
		just adding ACL you cant let the services in <a href="#VPC">VPC</a> allow access to the bucket.<br /></div><a
		href="#VPC">VPC(8)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 94<br />A company is using a <a
			href="#VPC">VPC</a> peering strategy to connect its <a href="#VPC">VPC</a>s in a single Region to allow for
		cross communication.<br /><br />A recent increase in account creations and <a href="#VPC">VPC</a>s has made it
		difficult to maintain the <a href="#VPC">VPC</a> peering strategy, and the company expects to grow to hundreds
		of <a href="#VPC">VPC</a>s. There are also new requests to create site&#8211;to&#8211;site VPNs with some of the
		<a href="#VPC">VPC</a>s. A solutions architect has been tasked with creating a centrally managed networking
		setup for multiple accounts, <a href="#VPC">VPC</a>s, and VPNs.<br /><br />Which networking solution meets these
		requirements?<br /><br />A. Configure shared <a href="#VPC">VPC</a>s and VPNs and share to each other.<br />B.
		Configure a hub&#8211;and&#8211;spoke <a href="#VPC">VPC</a> and route all traffic through <a
			href="#VPC">VPC</a> peering.<br />C. Configure an AWS Direct Connect connection between all <a
			href="#VPC">VPC</a>s and VPNs.<br />D. Configure a transit gateway with AWS Transit Gateway and connect all
		<a href="#VPC">VPC</a>s and VPNs.<br /><br /><b>Correct Answer:</b><br />D. Configure a transit gateway with AWS
		Transit Gateway and connect all <a href="#VPC">VPC</a>s and VPNs.<br /></div><a href="#VPC">VPC(8)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 150<br />A company has deployed an API in a <a
			href="#VPC">VPC</a> behind an internet&#8211;facing Application Load Balancer (ALB). An application that
		consumes the API as a client is deployed in a second account in private subnets behind a NAT gateway. When
		requests to the client application increase, the NAT gateway costs are higher than expected. A solutions
		architect has configured the ALB to be internal.<br /><br />Which combination of architectural changes will
		reduce the NAT gateway costs? (Choose two.)<br /><br />A. Configure a <a href="#VPC">VPC</a> peering connection
		between the two <a href="#VPC">VPC</a>s. Access the API using the private address.<br />B. Configure an AWS
		Direct Connect connection between the two <a href="#VPC">VPC</a>s. Access the API using the private
		address.<br />C. Configure a ClassicLink connection for the API into the client <a href="#VPC">VPC</a>. Access
		the API using the ClassicLink address.<br />D. Configure a PrivateLink connection for the API into the client <a
			href="#VPC">VPC</a>. Access the API using the PrivateLink address.<br />E. Configure an AWS Resource Access
		Manager connection between the two accounts. Access the API using the private address.<br /><br /><b>Correct
			Answer:</b><br />A. Configure a <a href="#VPC">VPC</a> peering connection between the two <a
			href="#VPC">VPC</a>s. Access the API using the private address.<br />D. Configure a PrivateLink connection
		for the API into the client <a href="#VPC">VPC</a>. Access the API using the PrivateLink
		address.<br /><br />Answer Description:<br />PrivateLink makes it easy to connect services across different
		accounts and <a href="#VPC">VPC</a>s to significantly simplify the network architecture. There is no API listed
		in shareable resources for RAM.<br /><br />References:<br /><br />AWS Resource Access Manager > User Guide >
		Shareable AWS resources<br /></div><a href="#VPC">VPC(8)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 151<br />A company has a two&#8211;tier
		application architecture that runs in public and private subnets. Amazon <a href="#EC2">EC2</a> instances
		running the web application are in the public subnet and a database runs on the private subnet.<br /><br />The
		web application instances and the database are running in a single Availability Zone (AZ).<br /><br />Which
		combination of steps should a solutions architect take to provide high availability for this architecture?
		(Choose two.)<br /><br />A. Create new public and private subnets in the same AZ for high availability.<br />B.
		Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group and Application Load Balancer spanning multiple
		AZs.<br />C. Add the existing web application instances to an Auto Scaling group behind an Application Load
		Balancer.<br />D. Create new public and private subnets in a new AZ. Create a database using Amazon <a
			href="#EC2">EC2</a> in one AZ.<br />E. Create new public and private subnets in the same <a
			href="#VPC">VPC</a>, each in a new AZ. Migrate the database to an Amazon RDS multi&#8211;AZ
		deployment.<br /><br /><b>Correct Answer:</b><br />B. Create an Amazon <a href="#EC2">EC2</a> Auto Scaling group
		and Application Load Balancer spanning multiple AZs.<br />E. Create new public and private subnets in the same
		<a href="#VPC">VPC</a>, each in a new AZ. Migrate the database to an Amazon RDS multi&#8211;AZ
		deployment.<br /><br />Answer Description:<br /><br />You would the <a href="#EC2">EC2</a> instances to have
		high availability by placing them in multiple AZs.<br /></div><a href="#VPC">VPC(8)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a><a id=CloudFront>
		<h2>CloudFront</h2>
	</a> - 15 Questions <br><a href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 5<br />A company runs a multi&#8211;tier web
		application that hosts news content. The application runs on Amazon <a href="#EC2">EC2</a> instances behind an
		Application Load Balancer. The instances run in an <a href="#EC2">EC2</a> Auto Scaling group across multiple
		Availability Zones and use an Amazon Aurora database. A solutions architect needs to make the application more
		resilient to periodic increases in request rates.<br /><br />Which architecture should the solutions architect
		implement? (Choose two.)<br /><br />A. Add AWS Shield.<br />B. Add Aurora Replica.<br />C. Add AWS Direct
		Connect.<br />D. Add AWS Global Accelerator.<br />E. Add an Amazon <a href="#CloudFront">CloudFront</a>
		distribution in front of the Application Load Balancer.<br /><br /><b>Correct Answer:</b><br />B. Add Aurora
		Replica.<br />E. Add an Amazon <a href="#CloudFront">CloudFront</a> distribution in front of the Application
		Load Balancer.<br /><br />Answer Description:<br />AWS Global Accelerator: Acceleration for
		latency&#8211;sensitive applications. Many applications, especially in areas such as gaming, media, mobile apps,
		and financials, require very low latency for a great user experience. To improve the user experience, Global
		Accelerator directs user traffic to the application endpoint that is nearest to the client, which reduces
		internet latency and jitter.<br />Global Accelerator routes traffic to the closest edge location by using
		Anycast, and then routes it to the closest regional endpoint over the AWS global network. Global Accelerator
		quickly reacts to changes in network performance to improve your users&apos application
		performance.<br /><br />Amazon <a href="#CloudFront">CloudFront</a>: Amazon <a href="#CloudFront">CloudFront</a>
		is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to
		customers globally with low latency, high transfer speeds, all within a developer&#8211;friendly
		environment.<br /><br />The architecture is already highly resilient but the may be subject to performance
		degradation if there are sudden increases in request rates. To resolve this situation Amazon Aurora Read
		Replicas can be used to serve read traffic which offloads requests from the main database. On the frontend an
		Amazon <a href="#CloudFront">CloudFront</a> distribution can be placed in front of the ALB and this will cache
		content for better performance and also offloads requests from the backend.<br /><br />CORRECT: &quot;Add Amazon
		Aurora Replicas&quot; is the correct answer.<br /><br />CORRECT: &quot;Add an Amazon <a
			href="#CloudFront">CloudFront</a> distribution in front of the ALB&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Add and Amazon WAF in front of the ALB&quot; is incorrect. A web application
		firewall protects applications from malicious attacks. It does not improve performance.<br /><br />INCORRECT:
		&quot;Add an Amazon Transit Gateway to the Availability Zones&quot; is incorrect as this is used to connect
		on&#8211;premises networks to <a href="#VPC">VPC</a>s.<br /><br />INCORRECT: &quot;Add an Amazon Global
		Accelerator endpoint&quot; is incorrect as this service is used for directing users to different instances of
		the application in different regions based on latency.<br /><br />References:<br /><br />Amazon Aurora > User
		Guide for Aurora > Replication with Amazon Aurora<br />Amazon <a href="#CloudFront">CloudFront</a> > Developer
		Guide > What is Amazon <a href="#CloudFront">CloudFront</a>?</div><a href="#CloudFront">CloudFront(15)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 9<br />Organizers for a global event want to put
		daily reports online as static HTML pages. The pages are expected to generate millions of views from users
		around the world. The files are stored in an Amazon <a href="#S3">S3</a> bucket. A solutions architect has been
		asked to design an efficient and effective solution.<br /><br />Which action should the solutions architect take
		to accomplish this?<br /><br />A. Generate presigned URLs for the files.<br />B. Use cross&#8211;Region
		replication to all Regions.<br />C. Use the geoproximity feature of Amazon Route 53.<br />D. Use Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as its origin.<br /><br /><b>Correct
			Answer:</b><br />D. Use Amazon <a href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as
		its origin.<br /><br />Answer Description:<br />Using Amazon <a href="#S3">S3</a> Origins, MediaPackage
		Channels, and Custom Origins for Web Distributions<br /><br />Using Amazon <a href="#S3">S3</a> Buckets for Your
		Origin<br />When you use Amazon <a href="#S3">S3</a> as an origin for your distribution, you place any objects
		that you want <a href="#CloudFront">CloudFront</a> to deliver in an Amazon <a href="#S3">S3</a> bucket. You can
		use any method that is supported by Amazon <a href="#S3">S3</a> to get your objects into Amazon <a
			href="#S3">S3</a>, for example, the Amazon <a href="#S3">S3</a> console or API, or a third&#8211;party tool.
		You can create a hierarchy in your bucket to store the objects, just as you would with any other Amazon <a
			href="#S3">S3</a> bucket.<br /><br />Using an existing Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change the bucket in any way; you can still use
		it as you normally would to store and access Amazon <a href="#S3">S3</a> objects at the standard Amazon <a
			href="#S3">S3</a> price. You incur regular Amazon <a href="#S3">S3</a> charges for storing the objects in
		the bucket.<br /><br />Using Amazon <a href="#S3">S3</a> Buckets Configured as Website Endpoints for Your
		Origin<br />You can set up an Amazon <a href="#S3">S3</a> bucket that is configured as a website endpoint as
		custom origin with <a href="#CloudFront">CloudFront</a>.<br /><br />When you configure your <a
			href="#CloudFront">CloudFront</a> distribution, for the origin, enter the Amazon <a href="#S3">S3</a> static
		website hosting endpoint for your bucket. This value appears in the Amazon <a href="#S3">S3</a> console, on the
		Properties tab, in the Static website hosting pane. For example:
		http://bucket&#8211;name.s3&#8211;website&#8211;region.amazonaws.com<br /><br />For more information about
		specifying Amazon <a href="#S3">S3</a> static website endpoints, see Website endpoints in the Amazon Simple
		Storage Service Developer Guide.<br /><br />When you specify the bucket name in this format as your origin, you
		can use Amazon <a href="#S3">S3</a> redirects and Amazon <a href="#S3">S3</a> custom error documents. For more
		information about Amazon <a href="#S3">S3</a> features, see the Amazon <a href="#S3">S3</a>
		documentation.<br /><br />Using an Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change it in any way. You can still use it as
		you normally would and you incur regular Amazon <a href="#S3">S3</a> charges.<br /><br />Amazon <a
			href="#CloudFront">CloudFront</a> can be used to cache the files in edge locations around the world and this
		will improve the performance of the webpages.<br /><br />To serve a static website hosted on Amazon <a
			href="#S3">S3</a>, you can deploy a <a href="#CloudFront">CloudFront</a> distribution using one of these
		configurations:<br /><br />Using a REST API endpoint as the origin with access restricted by an origin access
		identity (OAI) Using a website endpoint as the origin with anonymous (public) access allowed<br /><br />Using a
		website endpoint as the origin with access restricted by a Referer header CORRECT: &quot;Use Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as its origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Generate presigned URLs for the files&quot; is incorrect as this is used to
		restrict access which is not a requirement.<br /><br />INCORRECT: &quot;Use cross&#8211;Region replication to
		all Regions&quot; is incorrect as this does not provide a mechanism for directing users to the closest copy of
		the static webpages.<br /><br />INCORRECT: &quot;Use the geoproximity feature of Amazon Route 53&quot; is
		incorrect as this does not include a solution for having multiple copies of the data in different geographic
		locations.<br /><br />References:<br /><br />How do I use <a href="#CloudFront">CloudFront</a> to serve a static
		website hosted on Amazon <a href="#S3">S3</a>?</div><a href="#CloudFront">CloudFront(15)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 11<br />A solutions architect is designing a
		solution to access a catalog of images and provide users with the ability to submit requests to customize
		images. Image customization parameters will be in any request sent to an AWS API Gateway API. The customized
		image will be generated on demand, and users will receive a link they can click to view or download their
		customized image. The solution must be highly available for viewing and customizing images.<br /><br />What is
		the MOST cost&#8211;effective solution to meet these requirements?<br /><br />A. Use Amazon <a
			href="#EC2">EC2</a> instances to manipulate the original image into the requested customization. Store the
		original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an Elastic Load Balancer in front of
		the <a href="#EC2">EC2</a> instances.<br />B. Use AWS Lambda to manipulate the original image to the requested
		customization. Store the original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the origin.<br />C.
		Use AWS Lambda to manipulate the original image to the requested customization. Store the original images in
		Amazon <a href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in
		front of the Amazon <a href="#EC2">EC2</a> instances.<br />D. Use Amazon <a href="#EC2">EC2</a> instances to
		manipulate the original image into the requested customization. Store the original images in Amazon <a
			href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the
		origin.<br /><br /><b>Correct Answer:</b><br />B. Use AWS Lambda to manipulate the original image to the
		requested customization. Store the original and manipulated images in Amazon <a href="#S3">S3</a>. Configure an
		Amazon <a href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the
		origin.<br /><br />Answer Description:<br />AWS Lambda is a compute service that lets you run code without
		provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from
		a few requests per day to thousands per second. You pay only for the compute time you consume &#8212; there is
		no charge when your code is not running. With AWS Lambda, you can run code for virtually any type of application
		or backend service &#8212; all with zero administration. AWS Lambda runs your code on a high&#8211;availability
		compute infrastructure and performs all of the administration of the compute resources, including server and
		operating system maintenance, capacity provisioning and automatic scaling, code monitoring, and
		logging.<br /><br /><a href="#All">All</a> you need to do is supply your code in one of the languages that AWS
		Lambda supports.<br /><br />Storing your static content with <a href="#S3">S3</a> provides a lot of advantages.
		But to help optimize your application&aposs performance and security while effectively managing cost, we
		recommend that you also set up Amazon <a href="#CloudFront">CloudFront</a> to work with your <a
			href="#S3">S3</a> bucket to serve and protect the content. <a href="#CloudFront">CloudFront</a> is a content
		delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the
		world, securely and at scale. By design, delivering data out of <a href="#CloudFront">CloudFront</a> can be more
		cost effective than delivering it from <a href="#S3">S3</a> directly to your users.<br /><br /><a
			href="#CloudFront">CloudFront</a> serves content through a worldwide network of data centers called Edge
		Locations. Using edge servers to cache and serve content improves performance by providing content closer to
		where viewers are located. <a href="#CloudFront">CloudFront</a> has edge servers in locations all around the
		world.<br /><br /><a href="#All">All</a> solutions presented are highly available. The key requirement that must
		be satisfied is that the solution should be cost&#8211;effective and you must choose the most
		cost&#8211;effective option.<br /><br />Therefore, it&aposs best to eliminate services such as Amazon <a
			href="#EC2">EC2</a> and ELB as these require ongoing costs even when they&aposre not used. Instead, a fully
		serverless solution should be used. AWS Lambda, Amazon <a href="#S3">S3</a> and <a
			href="#CloudFront">CloudFront</a> are the best services to use for these requirements.<br /><br />CORRECT:
		&quot;Use AWS Lambda to manipulate the original images to the requested customization. Store the original and
		manipulated images in Amazon <a href="#S3">S3</a>. Configure an Amazon <a href="#CloudFront">CloudFront</a>
		distribution with the <a href="#S3">S3</a> bucket as the origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Use Amazon <a href="#EC2">EC2</a> instances to manipulate the original
		images into the requested customization. Store the original and manipulated images in Amazon <a
			href="#S3">S3</a>. Configure an Elastic Load Balancer in front of the <a href="#EC2">EC2</a> instances&quot;
		is incorrect. This is not the most cost&#8211;effective option as the ELB and <a href="#EC2">EC2</a> instances
		will incur costs even when not used.<br /><br />INCORRECT: &quot;Use AWS Lambda to manipulate the original
		images to the requested customization. Store the original images in Amazon <a href="#S3">S3</a> and the
		manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in front of the Amazon <a
			href="#EC2">EC2</a> instances&quot; is incorrect. This is not the most cost&#8211;effective option as the
		ELB will incur costs even when not used. Also, Amazon DynamoDB will incur RCU/WCUs when running and is not the
		best choice for storing images.<br /><br />INCORRECT: &quot;Use Amazon <a href="#EC2">EC2</a> instances to
		manipulate the original images into the requested customization. Store the original images in Amazon <a
			href="#S3">S3</a> and the manipulated images in Amazon DynamoDB. Configure an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with the <a href="#S3">S3</a> bucket as the origin&quot; is
		incorrect. This is not the most cost&#8211;effective option as the <a href="#EC2">EC2</a> instances will incur
		costs even when not used.<br /><br />References:<br /><br />Serverless on AWS</div><a
		href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 41<br />A solutions architect is optimizing a
		website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be
		available on demand. The event is expected to attract a global online audience.<br /><br />Which service will
		improve the performance of both the real&#8211;time and on&#8211;demand streaming?<br /><br />A. Amazon <a
			href="#CloudFront">CloudFront</a><br />B. AWS Global Accelerator<br />C. Amazon Route <a
			href="#S3">S3</a><br />D. Amazon <a href="#S3">S3</a> Transfer Acceleration<br /><br /><b>Correct
			Answer:</b><br />A. Amazon <a href="#CloudFront">CloudFront</a><br /></div><a
		href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 86<br />A company is running a two&#8211;tier
		eCommerce website using services. The current architect uses a public facing Elastic Load Balancer that sends
		traffic to Amazon <a href="#EC2">EC2</a> instances in a private subnet. The static content is hosted on <a
			href="#EC2">EC2</a> instances, and the dynamic content is retrieved from a MYSQL database. The application
		is running in the United States. The company recently started selling to users in Europe and Australia. A
		solutions architect needs to design solution so their international users have an improved browsing
		experience.<br /><br />Which solution is MOST cost&#8211;effective?<br /><br />A. Host the entire website on
		Amazon <a href="#S3">S3</a>.<br />B. Use Amazon <a href="#CloudFront">CloudFront</a> and Amazon <a
			href="#S3">S3</a> to host static images.<br />C. Increase the number of public load balancers and <a
			href="#EC2">EC2</a> instances.<br />D. Deploy the two&#8211;tier website in AWS Regions in Europe and
		Australia.<br /><br /><b>Correct Answer:</b><br />B. Use Amazon <a href="#CloudFront">CloudFront</a> and Amazon
		<a href="#S3">S3</a> to host static images.<br /></div><a href="#CloudFront">CloudFront(15)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 87<br />A company&aposs website provides users
		with downloadable historical performance reports. The website needs a solution that will scale to meet the
		company&aposs website demands globally. The solution should be cost effective, limit the provisioning of
		infrastructure resources, and provide the fastest possible response time.<br /><br />Which combination should a
		solutions architect recommend to meet these requirements?<br /><br />A. Amazon <a
			href="#CloudFront">CloudFront</a> and Amazon <a href="#S3">S3</a><br />B. AWS Lambda and Amazon
		DynamoDB<br />C. Application Load Balancer with Amazon <a href="#EC2">EC2</a> Auto Scaling<br />D. Amazon Route
		53 with internal Application Load Balancers<br /><br /><b>Correct Answer:</b><br />A. Amazon <a
			href="#CloudFront">CloudFront</a> and Amazon <a href="#S3">S3</a><br /></div><a
		href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 105<br />A company&aposs dynamic website is hosted
		using on&#8211;premises servers in the United States. The company is launching its product in Europe, and it
		wants to optimize site loading times for new European users. The site&aposs backend must remain in the United
		States. The product is being launched in a few days, and an immediate solution is needed.<br /><br />What should
		the solutions architect recommend?<br /><br />A. Launch an Amazon <a href="#EC2">EC2</a> instance in
		us&#8211;east&#8211;1 and migrate the site to it.<br />B. Move the website to Amazon <a href="#S3">S3</a>. Use
		cross&#8211;Region replication between Regions.<br />C. Use Amazon <a href="#CloudFront">CloudFront</a> with a
		custom origin pointing to the on&#8211;premises servers.<br />D. Use an Amazon Route 53 geo&#8211;proximity
		routing policy pointing to on&#8211;premises servers.<br /><br /><b>Correct Answer:</b><br />C. Use Amazon <a
			href="#CloudFront">CloudFront</a> with a custom origin pointing to the on&#8211;premises servers.<br />
	</div><a href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 128<br />A company is hosting an election
		reporting website on AWS for users around the world. The website uses Amazon <a href="#EC2">EC2</a> instances
		for the web and application tiers in an Auto Scaling group with Application Load Balancers. The database tier
		uses an Amazon RDS for MySQL database. The website is updated with election results once an hour and has
		historically observed hundreds of users accessing the reports.<br /><br />The company is expecting a significant
		increase in demand because of upcoming elections in different countries. A solutions architect must improve the
		website&aposs ability to handle additional demand while minimizing the need for additional <a
			href="#EC2">EC2</a> instances.<br /><br />Which solution will meet these requirements?<br /><br />A. Launch
		an Amazon ElastiCache cluster to cache common database queries.<br />B. Launch an Amazon <a
			href="#CloudFront">CloudFront</a> web distribution to cache commonly requested website content.<br />C.
		Enable disk&#8211;based caching on the <a href="#EC2">EC2</a> instances to cache commonly requested website
		content.<br />D. Deploy a reverse proxy into the design using an <a href="#EC2">EC2</a> instance with caching
		enabled for commonly requested website content.<br /><br /><b>Correct Answer:</b><br />B. Launch an Amazon <a
			href="#CloudFront">CloudFront</a> web distribution to cache commonly requested website content.<br /></div>
	<a href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 149<br />A company hosts a static website
		on&#8211;premises and wants to migrate the website to AWS. The website should load as quickly as possible for
		users around the world. The company also wants the most cost&#8211;effective solution.<br /><br />What should a
		solutions architect do to accomplish this?<br /><br />A. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Replicate the <a
			href="#S3">S3</a> bucket to multiple AWS Regions.<br />B. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin.<br />C. Copy the
		website content to an Amazon EBS&#8211;backed Amazon <a href="#EC2">EC2</a> instance running Apache HTTP Server.
		Configure Amazon Route 53 geolocation routing policies to select the closest origin.<br />D. Copy the website
		content to multiple Amazon EBS&#8211;backed Amazon <a href="#EC2">EC2</a> instances running Apache HTTP Server
		in multiple AWS Regions. Configure Amazon <a href="#CloudFront">CloudFront</a> geolocation routing policies to
		select the closest origin.<br /><br /><b>Correct Answer:</b><br />B. Copy the website content to an Amazon <a
			href="#S3">S3</a> bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin.<br /><br />Answer
		Description:<br />What Is Amazon <a href="#CloudFront">CloudFront</a>?<br />Amazon <a
			href="#CloudFront">CloudFront</a> is a web service that speeds up distribution of your static and dynamic
		web content, such as .html, .css, .js, and image files, to your users. <a href="#CloudFront">CloudFront</a>
		delivers your content through a worldwide network of data centers called edge locations. When a user requests
		content that you&aposre serving with <a href="#CloudFront">CloudFront</a>, the user is routed to the edge
		location that provides the lowest latency (time delay), so that content is delivered with the best possible
		performance.<br /><br />Using Amazon <a href="#S3">S3</a> Buckets for Your Origin<br />When you use Amazon <a
			href="#S3">S3</a> as an origin for your distribution, you place any objects that you want <a
			href="#CloudFront">CloudFront</a> to deliver in an Amazon <a href="#S3">S3</a> bucket. You can use any
		method that is supported by Amazon <a href="#S3">S3</a> to get your objects into Amazon <a href="#S3">S3</a>,
		for example, the Amazon <a href="#S3">S3</a> console or API, or a third&#8211;party tool. You can create a
		hierarchy in your bucket to store the objects, just as you would with any other Amazon <a href="#S3">S3</a>
		bucket.<br /><br />Using an existing Amazon <a href="#S3">S3</a> bucket as your <a
			href="#CloudFront">CloudFront</a> origin server doesn&apost change the bucket in any way; you can still use
		it as you normally would to store and access Amazon <a href="#S3">S3</a> objects at the standard Amazon <a
			href="#S3">S3</a> price. You incur regular Amazon <a href="#S3">S3</a> charges for storing the objects in
		the bucket.<br /><br />The most cost&#8211;effective option is to migrate the website to an Amazon <a
			href="#S3">S3</a> bucket and configure that bucket for static website hosting. To enable good performance
		for global users the solutions architect should then configure a <a href="#CloudFront">CloudFront</a>
		distribution with the <a href="#S3">S3</a> bucket as the origin. This will cache the static content around the
		world closer to users.<br /><br />CORRECT: &quot;Copy the website content to an Amazon <a href="#S3">S3</a>
		bucket. Configure the bucket to serve static webpage content. Configure Amazon <a
			href="#CloudFront">CloudFront</a> with the <a href="#S3">S3</a> bucket as the origin&quot; is the correct
		answer.<br /><br />INCORRECT: &quot;Copy the website content to an Amazon <a href="#S3">S3</a> bucket. Configure
		the bucket to serve static webpage content. Replicate the <a href="#S3">S3</a> bucket to multiple AWS
		Regions&quot; is incorrect as there is no solution here for directing users to the closest region. This could be
		a more cost&#8211;effective (though less elegant) solution if AWS Route 53 latency records are
		created.<br /><br />INCORRECT: &quot;Copy the website content to an Amazon <a href="#EC2">EC2</a> instance.
		Configure Amazon Route 53 geolocation routing policies to select the closest origin&quot; is incorrect as using
		Amazon <a href="#EC2">EC2</a> instances is less cost&#8211;effective compared to hosting the website on <a
			href="#S3">S3</a>. Also, geolocation routing does not achieve anything with only a single
		record.<br /><br />INCORRECT: &quot;Copy the website content to multiple Amazon <a href="#EC2">EC2</a> instances
		in multiple AWS Regions. Configure AWS Route 53 geolocation routing policies to select the closest region&quot;
		is incorrect as using Amazon <a href="#EC2">EC2</a> instances is less cost&#8211;effective compared to hosting
		the website on <a href="#S3">S3</a>.<br /><br />References:<br /><br />How do I use <a
			href="#CloudFront">CloudFront</a> to serve a static website hosted on Amazon <a href="#S3">S3</a>?</div><a
		href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 159<br />A Solutions Architect must design a web
		application that will be hosted on AWS, allowing users to purchase access to premium, shared content that is
		stored in an <a href="#S3">S3</a> bucket. Upon payment, content will be available for download for 14 days
		before the user is denied access.<br /><br />Which of the following would be the LEAST complicated
		implementation?<br /><br />A. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an origin
		access identity (OAI). Configure the distribution with an Amazon <a href="#S3">S3</a> origin to provide access
		to the file through signed URLs. Design a Lambda function to remove data that is older than 14 days.<br />B. Use
		an <a href="#S3">S3</a> bucket and provide direct access to the file. Design the application to track purchases
		in a DynamoDB table. Configure a Lambda function to remove data that is older than 14 days based on a query to
		Amazon DynamoDB.<br />C. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an OAI. Configure
		the distribution with an Amazon <a href="#S3">S3</a> origin to provide access to the file through signed URLs.
		Design the application to set an expiration of 14 days for the URL.<br />D. Use an Amazon <a
			href="#CloudFront">CloudFront</a> distribution with an OAI. Configure the distribution with an Amazon <a
			href="#S3">S3</a> origin to provide access to the file through signed URLs. Design the application to set an
		expiration of 60 minutes for the URL and recreate the URL as necessary.<br /><br /><b>Correct
			Answer:</b><br />C. Use an Amazon <a href="#CloudFront">CloudFront</a> distribution with an OAI. Configure
		the distribution with an Amazon <a href="#S3">S3</a> origin to provide access to the file through signed URLs.
		Design the application to set an expiration of 14 days for the URL.<br /></div><a
		href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 161<br />A solutions architect is moving the
		static content from a public website hosted on Amazon <a href="#EC2">EC2</a> instances to an Amazon <a
			href="#S3">S3</a> bucket. An Amazon <a href="#CloudFront">CloudFront</a> distribution will be used to
		deliver the static assets. The security group used by the <a href="#EC2">EC2</a> instances restricts access to a
		limited set of IP ranges. Access to the static content should be similarly restricted.<br /><br />Which
		combination of steps will meet these requirements? (Choose two.)<br /><br />A. Create an origin access identity
		(OAI) and associate it with the distribution. Change the permissions in the bucket policy so that only the OAI
		can read the objects.<br />B. Create an AWS WAF web ACL that includes the same IP restrictions that exist in the
		<a href="#EC2">EC2</a> security group. Associate this new web ACL with the <a href="#CloudFront">CloudFront</a>
		distribution.<br />C. Create a new security group that includes the same IP restrictions that exist in the
		current <a href="#EC2">EC2</a> security group. Associate this new security group with the <a
			href="#CloudFront">CloudFront</a> distribution.<br />D. Create a new security group that includes the same
		IP restrictions that exist in the current <a href="#EC2">EC2</a> security group. Associate this new security
		group with the <a href="#S3">S3</a> bucket hosting the static content.<br />E. Create a new <a
			href="#IAM">IAM</a> role and associate the role with the distribution. Change the permissions either on the
		<a href="#S3">S3</a> bucket or on the files within the <a href="#S3">S3</a> bucket so that only the newly
		created <a href="#IAM">IAM</a> role has read and download permissions.<br /><br /><b>Correct Answer:</b><br />A.
		Create an origin access identity (OAI) and associate it with the distribution. Change the permissions in the
		bucket policy so that only the OAI can read the objects.<br />B. Create an AWS WAF web ACL that includes the
		same IP restrictions that exist in the <a href="#EC2">EC2</a> security group. Associate this new web ACL with
		the <a href="#CloudFront">CloudFront</a> distribution.<br /></div><a href="#CloudFront">CloudFront(15)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 162<br />A company recently launched its website
		to serve content to its global user base. The company wants to store and accelerate the delivery of static
		content to its users by leveraging Amazon <a href="#CloudFront">CloudFront</a> with an Amazon <a
			href="#EC2">EC2</a> instance attached as its origin.<br /><br />How should a solutions architect optimize
		high availability for the application?<br /><br />A. Use Lambda@Edge for <a
			href="#CloudFront">CloudFront</a>.<br />B. Use Amazon <a href="#S3">S3</a> Transfer Acceleration for <a
			href="#CloudFront">CloudFront</a>.<br />C. Configure another <a href="#EC2">EC2</a> instance in a different
		Availability Zone as part of the origin group.<br />D. Configure another <a href="#EC2">EC2</a> instance as part
		of the origin server cluster in the same Availability Zone.<br /><br /><b>Correct Answer:</b><br />A. Use
		Lambda@Edge for <a href="#CloudFront">CloudFront</a>.<br /></div><a href="#CloudFront">CloudFront(15)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 167<br />A solutions architect needs to design a
		low&#8211;latency solution for a static single&#8211;page application accessed by users utilizing a custom
		domain name. The solution must be serverless, encrypted in transit, and cost&#8211;effective.<br /><br />Which
		combination of AWS services and features should the solutions architect use? (Choose two.)<br /><br />A. Amazon
		<a href="#S3">S3</a><br />B. Amazon <a href="#EC2">EC2</a><br />C. AWS Fargate<br />D. Amazon <a
			href="#CloudFront">CloudFront</a><br />E. Elastic Load Balancer<br /><br /><b>Correct Answer:</b><br />A.
		Amazon <a href="#S3">S3</a><br />D. Amazon <a href="#CloudFront">CloudFront</a><br /></div><a
		href="#CloudFront">CloudFront(15)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 179<br />A company serves content to its
		subscribers across the world using an application running on AWS. The application has several Amazon <a
			href="#EC2">EC2</a> instances in a private subnet behind an Application Load Balancer (ALB). Due to a recent
		change in copyright restrictions, the chief information officer (CIO) wants to block access for certain
		countries.<br /><br />Which action will meet these requirements?<br /><br />A. Modify the ALB security group to
		deny incoming traffic from blocked countries.<br />B. Modify the security group for <a href="#EC2">EC2</a>
		instances to deny incoming traffic from blocked countries.<br />C. Use Amazon <a
			href="#CloudFront">CloudFront</a> to serve the application and deny access to blocked countries.<br />D. Use
		ALB listener rules to return access denied responses to incoming traffic from blocked
		countries.<br /><br /><b>Correct Answer:</b><br />C. Use Amazon <a href="#CloudFront">CloudFront</a> to serve
		the application and deny access to blocked countries.<br /><br />Answer Description:<br />&quot;block access for
		certain countries.&quot; You can use geo restriction, also known as geo blocking, to prevent users in specific
		geographic locations from accessing content that you&aposre distributing through a <a
			href="#CloudFront">CloudFront</a> web distribution.<br /><br />When a user requests your content, <a
			href="#CloudFront">CloudFront</a> typically serves the requested content regardless of where the user is
		located. If you need to prevent users in specific countries from accessing your content, you can use the <a
			href="#CloudFront">CloudFront</a> geo restriction feature to do one of the following:<br /><br /><a
			href="#All">All</a>ow your users to access your content only if they&aposre in one of the countries on a
		whitelist of approved countries.<br /><br />Prevent your users from accessing your content if they&aposre in one
		of the countries on a blacklist of banned countries.<br /><br />For example, if a request comes from a country
		where, for copyright reasons, you are not authorized to distribute your content, you can use <a
			href="#CloudFront">CloudFront</a> geo restriction to block the request. This is the easiest and most
		effective way to implement a geographic restriction for the delivery of content.<br /><br />CORRECT: &quot;Use
		Amazon <a href="#CloudFront">CloudFront</a> to serve the application and deny access to blocked countries&quot;
		is the correct answer.<br /><br />INCORRECT: &quot;Use a Network ACL to block the IP address ranges associated
		with the specific countries&quot; is incorrect as this would be extremely difficult to
		manage.<br /><br />INCORRECT: &quot;Modify the ALB security group to deny incoming traffic from blocked
		countries&quot; is incorrect as security groups cannot block traffic by country.<br /><br />INCORRECT:
		&quot;Modify the security group for <a href="#EC2">EC2</a> instances to deny incoming traffic from blocked
		countries&quot; is incorrect as security groups cannot block traffic by
		country.<br /><br />References:<br /><br />Amazon <a href="#CloudFront">CloudFront</a> > Developer Guide >
		Restricting the geographic distribution of your content</div><a href="#CloudFront">CloudFront(15)</a> <a
		href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 187<br />A company hosts its product information
		webpages on AWS. The existing solution uses multiple Amazon C2 instances behind an Application Load Balancer in
		an Auto Scaling group. The website also uses a custom DNS name and communicates with HTTPS only using a
		dedicated SSL certificate. The company is planning a new product launch and wants to be sure that users from
		around the world have the best possible experience on the new website.<br /><br />What should a solutions
		architect do to meet these requirements?<br /><br />A. Redesign the application to use Amazon <a
			href="#CloudFront">CloudFront</a>.<br />B. Redesign the application to use AWS Elastic Beanstalk.<br />C.
		Redesign the application to use a Network Load Balancer.<br />D. Redesign the application to use Amazon <a
			href="#S3">S3</a> static website hosting.<br /><br /><b>Correct Answer:</b><br />A. Redesign the application
		to use Amazon <a href="#CloudFront">CloudFront</a>.<br /><br />Answer Description:<br />What Is Amazon <a
			href="#CloudFront">CloudFront</a>?<br />Amazon <a href="#CloudFront">CloudFront</a> is a web service that
		speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to
		your users. <a href="#CloudFront">CloudFront</a> delivers your content through a worldwide network of data
		centers called edge locations. When a user requests content that you&aposre serving with <a
			href="#CloudFront">CloudFront</a>, the user is routed to the edge location that provides the lowest latency
		(time delay), so that content is delivered with the best possible performance.<br /><br />If the content is
		already in the edge location with the lowest latency, <a href="#CloudFront">CloudFront</a> delivers it
		immediately.<br /><br />If the content is not in that edge location, <a href="#CloudFront">CloudFront</a>
		retrieves it from an origin that you&aposve defined &#8212; such as an Amazon <a href="#S3">S3</a> bucket, a
		MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for
		the definitive version of your content.<br /><br />As an example, suppose that you&aposre serving an image from
		a traditional web server, not from <a href="#CloudFront">CloudFront</a>. For example, you might serve an image,
		sunsetphoto.png, using the URL http://example.com/sunsetphoto.png.<br /><br />Your users can easily navigate to
		this URL and see the image. But they probably don&apost know that their request was routed from one network to
		another &#8212; through the complex collection of interconnected networks that comprise the internet &#8212;
		until the image was found.<br /><br /><a href="#CloudFront">CloudFront</a> speeds up the distribution of your
		content by routing each user request through the AWS backbone network to the edge location that can best serve
		your content. Typically, this is a <a href="#CloudFront">CloudFront</a> edge server that provides the fastest
		delivery to the viewer. Using the AWS network dramatically reduces the number of networks that your users&apos
		requests must pass through, which improves performance. Users get lower latency &#8212; the time it takes to
		load the first byte of the file &#8212; and higher data transfer rates.<br /><br />You also get increased
		reliability and availability because copies of your files (also known as objects) are now held (or cached) in
		multiple edge locations around the world.<br /></div><a href="#CloudFront">CloudFront(15)</a> <a href="#" <i
		class="bi bi-house">Home</i>
		<hr>
	</a><a id=ECS>
		<h2>ECS</h2>
	</a> - 1 Questions <br><a href="#ECS">ECS(1)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
	<div class="shadow p-3 mb-5 bg-white rounded"><br />Exam Question 134<br />A company&aposs
		near&#8211;real&#8211;time streaming application is running on AWS. As the data is ingested, a job runs on the
		data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of
		incoming data. A solutions architect needs to design a scalable and serverless solution to enhance
		performance.<br /><br />Which combination of steps should the solutions architect take? (Choose
		two.)<br /><br />A. Use Amazon Kinesis Data Firehose to ingest the data.<br />B. Use AWS Lambda with AWS Step
		Functions to process the data.<br />C. Use AWS Database Migration Service (AWS DMS) to ingest the data.<br />D.
		Use Amazon <a href="#EC2">EC2</a> instances in an Auto Scaling group to process the data.<br />E. Use AWS
		Fargate with Amazon Elastic Container Service (Amazon <a href="#ECS">ECS</a>) to process the
		data.<br /><br /><b>Correct Answer:</b><br />A. Use Amazon Kinesis Data Firehose to ingest the data.<br />E. Use
		AWS Fargate with Amazon Elastic Container Service (Amazon <a href="#ECS">ECS</a>) to process the data.<br />
	</div><a href="#ECS">ECS(1)</a> <a href="#" <i class="bi bi-house">Home</i>
		<hr>
	</a>
</body>

</html>